Namespace(acc_type='class', augment=True, batch_size=64, bottleneck=True, cachemode=True, cardinality=8, criterion='crossentropy', cuda=True, data_dir='../data', dataset='cifar100', decayinterval=None, decaylevel=None, droprate=0, epochs=300, evaluate=False, expandConfig=None, expandSize=2, from_modelzoo=False, growth=48, layers=40, learningratescheduler='cifarschedular', logdir='../logs/densenetbc_cifar100_40_48', lr=0.1, manualSeed=123, maxlr=0.1, minlr=0.0005, model_def='densenet_cifar', momentum=0.9, name='densenetbc_cifar100_40_48', nclasses=100, nesterov=True, ngpus=1, optimType='sgd', pretrained=False, pretrained_file='', printfreq=200, reduce=0.5, resume='', start_epoch=0, store='', tenCrop=False, tensorboard=True, testOnly=False, verbose=False, weightDecay=0.0001, weight_init=False, widen_factor=4, workers=2)
DenseNet3 (
  (conv1): Conv2d(3, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): DenseBlock (
    (layer): Sequential (
      (0): BottleneckBlock (
        (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock (
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(144, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock (
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock (
        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(240, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock (
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock (
        (bn1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(336, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans1): TransitionBlock (
    (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True)
    (relu): ReLU (inplace)
    (conv1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block2): DenseBlock (
    (layer): Sequential (
      (0): BottleneckBlock (
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock (
        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(240, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock (
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock (
        (bn1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(336, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock (
        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock (
        (bn1): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(432, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans2): TransitionBlock (
    (bn1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True)
    (relu): ReLU (inplace)
    (conv1): Conv2d(480, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block3): DenseBlock (
    (layer): Sequential (
      (0): BottleneckBlock (
        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(240, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock (
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock (
        (bn1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(336, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock (
        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock (
        (bn1): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(432, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock (
        (bn1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True)
  (relu): ReLU (inplace)
  (fc): Linear (528 -> 100)
)
Files already downloaded and verified
Starting epoch number: 0 Learning rate: 0.1
Train: [0]	Time 84.638	Data 0.298	Loss 3.760	Prec@1 12.3380	Prec@5 34.8180	
Val: [0]	Time 4.962	Data 0.105	Loss 3.578	Prec@1 16.5900	Prec@5 43.7900	
Best Prec@1: [16.590]	
Starting epoch number: 1 Learning rate: 0.1
Train: [1]	Time 83.963	Data 0.244	Loss 2.850	Prec@1 27.6220	Prec@5 59.1840	
Val: [1]	Time 5.060	Data 0.110	Loss 2.607	Prec@1 34.4000	Prec@5 66.0800	
Best Prec@1: [34.400]	
Starting epoch number: 2 Learning rate: 0.1
Train: [2]	Time 84.781	Data 0.257	Loss 2.254	Prec@1 39.7900	Prec@5 72.8800	
Val: [2]	Time 5.136	Data 0.121	Loss 2.284	Prec@1 40.3800	Prec@5 73.6700	
Best Prec@1: [40.380]	
Starting epoch number: 3 Learning rate: 0.1
Train: [3]	Time 85.097	Data 0.277	Loss 1.903	Prec@1 47.7680	Prec@5 79.7380	
Val: [3]	Time 5.105	Data 0.104	Loss 2.064	Prec@1 45.2800	Prec@5 76.8800	
Best Prec@1: [45.280]	
Starting epoch number: 4 Learning rate: 0.1
Train: [4]	Time 85.227	Data 0.243	Loss 1.687	Prec@1 52.8680	Prec@5 83.5580	
Val: [4]	Time 5.122	Data 0.123	Loss 1.845	Prec@1 50.3900	Prec@5 81.3500	
Best Prec@1: [50.390]	
Starting epoch number: 5 Learning rate: 0.1
Train: [5]	Time 85.269	Data 0.243	Loss 1.528	Prec@1 56.9060	Prec@5 86.0500	
Val: [5]	Time 5.094	Data 0.114	Loss 1.832	Prec@1 52.5800	Prec@5 82.1000	
Best Prec@1: [52.580]	
Starting epoch number: 6 Learning rate: 0.1
Train: [6]	Time 85.302	Data 0.221	Loss 1.408	Prec@1 59.7220	Prec@5 87.9120	
Val: [6]	Time 5.075	Data 0.124	Loss 1.571	Prec@1 56.9100	Prec@5 85.6300	
Best Prec@1: [56.910]	
Starting epoch number: 7 Learning rate: 0.1
Train: [7]	Time 85.485	Data 0.297	Loss 1.316	Prec@1 62.1640	Prec@5 89.2180	
Val: [7]	Time 5.127	Data 0.142	Loss 1.567	Prec@1 57.7100	Prec@5 85.4400	
Best Prec@1: [57.710]	
Starting epoch number: 8 Learning rate: 0.1
Train: [8]	Time 85.579	Data 0.368	Loss 1.244	Prec@1 63.7820	Prec@5 90.2320	
Val: [8]	Time 5.088	Data 0.137	Loss 1.531	Prec@1 57.6500	Prec@5 86.3100	
Best Prec@1: [57.710]	
Starting epoch number: 9 Learning rate: 0.1
Train: [9]	Time 85.645	Data 0.356	Loss 1.173	Prec@1 66.0760	Prec@5 91.0220	
Val: [9]	Time 5.095	Data 0.136	Loss 1.483	Prec@1 60.6800	Prec@5 87.4500	
Best Prec@1: [60.680]	
Starting epoch number: 10 Learning rate: 0.1
Train: [10]	Time 85.658	Data 0.351	Loss 1.130	Prec@1 66.8620	Prec@5 91.8120	
Val: [10]	Time 5.146	Data 0.156	Loss 1.664	Prec@1 57.0200	Prec@5 85.4900	
Best Prec@1: [60.680]	
Starting epoch number: 11 Learning rate: 0.1
Train: [11]	Time 85.507	Data 0.397	Loss 1.081	Prec@1 68.2280	Prec@5 92.2080	
Val: [11]	Time 5.112	Data 0.113	Loss 1.472	Prec@1 61.6100	Prec@5 87.9100	
Best Prec@1: [61.610]	
Starting epoch number: 12 Learning rate: 0.1
Train: [12]	Time 85.375	Data 0.358	Loss 1.040	Prec@1 69.3680	Prec@5 92.7140	
Val: [12]	Time 5.119	Data 0.114	Loss 1.516	Prec@1 59.6100	Prec@5 86.9700	
Best Prec@1: [61.610]	
Starting epoch number: 13 Learning rate: 0.1
Train: [13]	Time 85.201	Data 0.348	Loss 1.012	Prec@1 70.1200	Prec@5 93.1740	
Val: [13]	Time 5.103	Data 0.103	Loss 1.497	Prec@1 60.9900	Prec@5 87.9400	
Best Prec@1: [61.610]	
Starting epoch number: 14 Learning rate: 0.1
Train: [14]	Time 85.278	Data 0.375	Loss 0.970	Prec@1 71.1400	Prec@5 93.6900	
Val: [14]	Time 5.083	Data 0.120	Loss 1.667	Prec@1 58.6900	Prec@5 86.1600	
Best Prec@1: [61.610]	
Starting epoch number: 15 Learning rate: 0.1
Train: [15]	Time 85.327	Data 0.377	Loss 0.945	Prec@1 71.9780	Prec@5 93.9920	
Val: [15]	Time 5.089	Data 0.117	Loss 1.366	Prec@1 63.2900	Prec@5 88.6700	
Best Prec@1: [63.290]	
Starting epoch number: 16 Learning rate: 0.1
Train: [16]	Time 85.380	Data 0.347	Loss 0.928	Prec@1 72.1780	Prec@5 94.3100	
Val: [16]	Time 5.139	Data 0.130	Loss 1.380	Prec@1 63.1800	Prec@5 88.8900	
Best Prec@1: [63.290]	
Starting epoch number: 17 Learning rate: 0.1
Train: [17]	Time 85.169	Data 0.220	Loss 0.907	Prec@1 72.8800	Prec@5 94.3880	
Val: [17]	Time 5.120	Data 0.114	Loss 1.337	Prec@1 64.4600	Prec@5 89.6800	
Best Prec@1: [64.460]	
Starting epoch number: 18 Learning rate: 0.1
Train: [18]	Time 85.143	Data 0.232	Loss 0.883	Prec@1 73.5260	Prec@5 94.7180	
Val: [18]	Time 5.119	Data 0.111	Loss 1.428	Prec@1 61.7700	Prec@5 89.0600	
Best Prec@1: [64.460]	
Starting epoch number: 19 Learning rate: 0.1
Train: [19]	Time 85.194	Data 0.249	Loss 0.864	Prec@1 73.9440	Prec@5 94.9820	
Val: [19]	Time 5.122	Data 0.111	Loss 1.411	Prec@1 63.8600	Prec@5 88.7600	
Best Prec@1: [64.460]	
Starting epoch number: 20 Learning rate: 0.1
Train: [20]	Time 85.195	Data 0.242	Loss 0.850	Prec@1 74.4980	Prec@5 95.2280	
Val: [20]	Time 5.091	Data 0.096	Loss 1.478	Prec@1 63.1200	Prec@5 88.2400	
Best Prec@1: [64.460]	
Starting epoch number: 21 Learning rate: 0.1
Train: [21]	Time 85.207	Data 0.243	Loss 0.831	Prec@1 74.6560	Prec@5 95.2960	
Val: [21]	Time 5.104	Data 0.102	Loss 1.384	Prec@1 64.3000	Prec@5 88.9500	
Best Prec@1: [64.460]	
Starting epoch number: 22 Learning rate: 0.1
Train: [22]	Time 85.149	Data 0.230	Loss 0.811	Prec@1 75.4880	Prec@5 95.5160	
Val: [22]	Time 5.117	Data 0.118	Loss 1.510	Prec@1 62.1000	Prec@5 88.3700	
Best Prec@1: [64.460]	
Starting epoch number: 23 Learning rate: 0.1
Train: [23]	Time 85.240	Data 0.240	Loss 0.799	Prec@1 75.7000	Prec@5 95.7380	
Val: [23]	Time 5.083	Data 0.094	Loss 1.395	Prec@1 63.9000	Prec@5 89.5700	
Best Prec@1: [64.460]	
Starting epoch number: 24 Learning rate: 0.1
Train: [24]	Time 85.213	Data 0.224	Loss 0.790	Prec@1 76.0340	Prec@5 95.7800	
Val: [24]	Time 5.119	Data 0.110	Loss 1.574	Prec@1 61.9500	Prec@5 88.3500	
Best Prec@1: [64.460]	
Starting epoch number: 25 Learning rate: 0.1
Train: [25]	Time 85.253	Data 0.255	Loss 0.774	Prec@1 76.3260	Prec@5 95.9540	
Val: [25]	Time 5.100	Data 0.126	Loss 1.475	Prec@1 61.8300	Prec@5 88.6400	
Best Prec@1: [64.460]	
Starting epoch number: 26 Learning rate: 0.1
Train: [26]	Time 85.068	Data 0.221	Loss 0.763	Prec@1 76.8500	Prec@5 95.9680	
Val: [26]	Time 5.114	Data 0.110	Loss 1.504	Prec@1 62.5000	Prec@5 88.2700	
Best Prec@1: [64.460]	
Starting epoch number: 27 Learning rate: 0.1
Train: [27]	Time 84.961	Data 0.218	Loss 0.750	Prec@1 77.1340	Prec@5 96.1400	
Val: [27]	Time 5.085	Data 0.126	Loss 1.356	Prec@1 64.8800	Prec@5 89.5300	
Best Prec@1: [64.880]	
Starting epoch number: 28 Learning rate: 0.1
Train: [28]	Time 84.932	Data 0.237	Loss 0.744	Prec@1 77.0360	Prec@5 96.2140	
Val: [28]	Time 5.082	Data 0.094	Loss 1.387	Prec@1 64.4300	Prec@5 89.7900	
Best Prec@1: [64.880]	
Starting epoch number: 29 Learning rate: 0.1
Train: [29]	Time 84.857	Data 0.240	Loss 0.730	Prec@1 77.5340	Prec@5 96.4260	
Val: [29]	Time 5.078	Data 0.106	Loss 1.486	Prec@1 63.6700	Prec@5 88.9300	
Best Prec@1: [64.880]	
Starting epoch number: 30 Learning rate: 0.1
Train: [30]	Time 84.946	Data 0.239	Loss 0.716	Prec@1 78.0040	Prec@5 96.4060	
Val: [30]	Time 5.122	Data 0.131	Loss 1.353	Prec@1 64.7600	Prec@5 89.6100	
Best Prec@1: [64.880]	
Starting epoch number: 31 Learning rate: 0.1
Train: [31]	Time 84.969	Data 0.233	Loss 0.719	Prec@1 77.9260	Prec@5 96.4600	
Val: [31]	Time 5.105	Data 0.123	Loss 1.276	Prec@1 66.6800	Prec@5 90.7700	
Best Prec@1: [66.680]	
Starting epoch number: 32 Learning rate: 0.1
Train: [32]	Time 85.029	Data 0.226	Loss 0.705	Prec@1 78.3900	Prec@5 96.6600	
Val: [32]	Time 5.082	Data 0.099	Loss 1.333	Prec@1 66.0300	Prec@5 89.5100	
Best Prec@1: [66.680]	
Starting epoch number: 33 Learning rate: 0.1
Train: [33]	Time 85.089	Data 0.247	Loss 0.697	Prec@1 78.5520	Prec@5 96.6600	
Val: [33]	Time 5.103	Data 0.116	Loss 1.371	Prec@1 65.2200	Prec@5 90.2600	
Best Prec@1: [66.680]	
Starting epoch number: 34 Learning rate: 0.1
Train: [34]	Time 85.056	Data 0.234	Loss 0.696	Prec@1 78.4540	Prec@5 96.6120	
Val: [34]	Time 5.138	Data 0.129	Loss 1.668	Prec@1 59.7700	Prec@5 86.6900	
Best Prec@1: [66.680]	
Starting epoch number: 35 Learning rate: 0.1
Train: [35]	Time 85.077	Data 0.224	Loss 0.676	Prec@1 79.0880	Prec@5 96.9300	
Val: [35]	Time 5.108	Data 0.115	Loss 1.322	Prec@1 65.2400	Prec@5 90.5000	
Best Prec@1: [66.680]	
Starting epoch number: 36 Learning rate: 0.1
Train: [36]	Time 85.120	Data 0.234	Loss 0.675	Prec@1 79.0200	Prec@5 96.9720	
Val: [36]	Time 5.096	Data 0.117	Loss 1.460	Prec@1 64.9500	Prec@5 89.3500	
Best Prec@1: [66.680]	
Starting epoch number: 37 Learning rate: 0.1
Train: [37]	Time 85.144	Data 0.242	Loss 0.671	Prec@1 79.2700	Prec@5 96.9860	
Val: [37]	Time 5.121	Data 0.096	Loss 1.348	Prec@1 65.6700	Prec@5 90.0700	
Best Prec@1: [66.680]	
Starting epoch number: 38 Learning rate: 0.1
Train: [38]	Time 85.154	Data 0.217	Loss 0.672	Prec@1 79.1160	Prec@5 96.9960	
Val: [38]	Time 5.113	Data 0.110	Loss 1.281	Prec@1 66.3000	Prec@5 90.7200	
Best Prec@1: [66.680]	
Starting epoch number: 39 Learning rate: 0.1
Train: [39]	Time 85.207	Data 0.255	Loss 0.651	Prec@1 79.8600	Prec@5 97.0260	
Val: [39]	Time 5.090	Data 0.099	Loss 1.469	Prec@1 64.4400	Prec@5 89.3700	
Best Prec@1: [66.680]	
Starting epoch number: 40 Learning rate: 0.1
Train: [40]	Time 85.261	Data 0.254	Loss 0.655	Prec@1 79.7620	Prec@5 97.1180	
Val: [40]	Time 5.100	Data 0.094	Loss 1.354	Prec@1 65.0600	Prec@5 89.7300	
Best Prec@1: [66.680]	
Starting epoch number: 41 Learning rate: 0.1
Train: [41]	Time 85.235	Data 0.237	Loss 0.639	Prec@1 80.3700	Prec@5 97.2540	
Val: [41]	Time 5.121	Data 0.110	Loss 1.467	Prec@1 64.7800	Prec@5 89.4900	
Best Prec@1: [66.680]	
Starting epoch number: 42 Learning rate: 0.1
Train: [42]	Time 85.267	Data 0.246	Loss 0.641	Prec@1 79.9920	Prec@5 97.2500	
Val: [42]	Time 5.068	Data 0.113	Loss 1.308	Prec@1 66.8100	Prec@5 90.3800	
Best Prec@1: [66.810]	
Starting epoch number: 43 Learning rate: 0.1
Train: [43]	Time 85.283	Data 0.233	Loss 0.632	Prec@1 80.2780	Prec@5 97.3040	
Val: [43]	Time 5.100	Data 0.109	Loss 1.292	Prec@1 67.1200	Prec@5 90.8300	
Best Prec@1: [67.120]	
Starting epoch number: 44 Learning rate: 0.1
Train: [44]	Time 85.208	Data 0.219	Loss 0.631	Prec@1 80.1880	Prec@5 97.4160	
Val: [44]	Time 5.111	Data 0.103	Loss 1.428	Prec@1 64.8600	Prec@5 89.8200	
Best Prec@1: [67.120]	
Starting epoch number: 45 Learning rate: 0.1
Train: [45]	Time 85.244	Data 0.225	Loss 0.626	Prec@1 80.5820	Prec@5 97.3060	
Val: [45]	Time 5.066	Data 0.124	Loss 1.342	Prec@1 66.5200	Prec@5 90.6300	
Best Prec@1: [67.120]	
Starting epoch number: 46 Learning rate: 0.1
Train: [46]	Time 85.193	Data 0.249	Loss 0.624	Prec@1 80.5940	Prec@5 97.3600	
Val: [46]	Time 5.100	Data 0.118	Loss 1.412	Prec@1 65.4100	Prec@5 90.0900	
Best Prec@1: [67.120]	
Starting epoch number: 47 Learning rate: 0.1
Train: [47]	Time 85.088	Data 0.257	Loss 0.619	Prec@1 80.7220	Prec@5 97.3880	
Val: [47]	Time 5.104	Data 0.111	Loss 1.371	Prec@1 65.4600	Prec@5 89.8500	
Best Prec@1: [67.120]	
Starting epoch number: 48 Learning rate: 0.1
Train: [48]	Time 84.898	Data 0.246	Loss 0.616	Prec@1 80.7320	Prec@5 97.4540	
Val: [48]	Time 5.104	Data 0.112	Loss 1.279	Prec@1 66.6700	Prec@5 90.6100	
Best Prec@1: [67.120]	
Starting epoch number: 49 Learning rate: 0.1
Train: [49]	Time 84.865	Data 0.233	Loss 0.617	Prec@1 80.7460	Prec@5 97.4600	
Val: [49]	Time 5.090	Data 0.096	Loss 1.390	Prec@1 66.1500	Prec@5 90.1300	
Best Prec@1: [67.120]	
Starting epoch number: 50 Learning rate: 0.1
Train: [50]	Time 84.924	Data 0.241	Loss 0.606	Prec@1 81.2020	Prec@5 97.5200	
Val: [50]	Time 5.087	Data 0.097	Loss 1.445	Prec@1 65.1500	Prec@5 89.0500	
Best Prec@1: [67.120]	
Starting epoch number: 51 Learning rate: 0.1
Train: [51]	Time 84.972	Data 0.241	Loss 0.613	Prec@1 80.9680	Prec@5 97.4500	
Val: [51]	Time 5.115	Data 0.119	Loss 1.412	Prec@1 65.7700	Prec@5 89.7400	
Best Prec@1: [67.120]	
Starting epoch number: 52 Learning rate: 0.1
Train: [52]	Time 85.014	Data 0.234	Loss 0.601	Prec@1 81.1980	Prec@5 97.5940	
Val: [52]	Time 5.048	Data 0.105	Loss 1.462	Prec@1 65.4700	Prec@5 89.3300	
Best Prec@1: [67.120]	
Starting epoch number: 53 Learning rate: 0.1
Train: [53]	Time 85.067	Data 0.230	Loss 0.598	Prec@1 81.3980	Prec@5 97.5500	
Val: [53]	Time 5.062	Data 0.093	Loss 1.425	Prec@1 65.0100	Prec@5 89.4800	
Best Prec@1: [67.120]	
Starting epoch number: 54 Learning rate: 0.1
Train: [54]	Time 85.128	Data 0.256	Loss 0.601	Prec@1 81.3980	Prec@5 97.5140	
Val: [54]	Time 5.120	Data 0.108	Loss 1.478	Prec@1 65.0800	Prec@5 89.5000	
Best Prec@1: [67.120]	
Starting epoch number: 55 Learning rate: 0.1
Train: [55]	Time 85.202	Data 0.316	Loss 0.584	Prec@1 81.5960	Prec@5 97.7200	
Val: [55]	Time 5.126	Data 0.108	Loss 1.459	Prec@1 65.4800	Prec@5 89.9000	
Best Prec@1: [67.120]	
Starting epoch number: 56 Learning rate: 0.1
Train: [56]	Time 85.232	Data 0.288	Loss 0.596	Prec@1 81.3640	Prec@5 97.6520	
Val: [56]	Time 5.114	Data 0.116	Loss 1.498	Prec@1 64.6100	Prec@5 89.6100	
Best Prec@1: [67.120]	
Starting epoch number: 57 Learning rate: 0.1
Train: [57]	Time 85.294	Data 0.327	Loss 0.587	Prec@1 81.7440	Prec@5 97.7860	
Val: [57]	Time 5.119	Data 0.106	Loss 1.477	Prec@1 65.0700	Prec@5 89.2100	
Best Prec@1: [67.120]	
Starting epoch number: 58 Learning rate: 0.1
Train: [58]	Time 85.274	Data 0.305	Loss 0.576	Prec@1 81.9400	Prec@5 97.7440	
Val: [58]	Time 5.160	Data 0.153	Loss 1.448	Prec@1 65.1100	Prec@5 89.4600	
Best Prec@1: [67.120]	
Starting epoch number: 59 Learning rate: 0.1
Train: [59]	Time 85.220	Data 0.310	Loss 0.578	Prec@1 82.0760	Prec@5 97.7300	
Val: [59]	Time 5.092	Data 0.124	Loss 1.422	Prec@1 65.5500	Prec@5 89.7900	
Best Prec@1: [67.120]	
Starting epoch number: 60 Learning rate: 0.1
Train: [60]	Time 85.105	Data 0.334	Loss 0.576	Prec@1 82.0460	Prec@5 97.8100	
Val: [60]	Time 5.117	Data 0.123	Loss 1.492	Prec@1 64.4900	Prec@5 89.3800	
Best Prec@1: [67.120]	
Starting epoch number: 61 Learning rate: 0.1
Train: [61]	Time 85.086	Data 0.344	Loss 0.574	Prec@1 82.0500	Prec@5 97.8140	
Val: [61]	Time 5.098	Data 0.107	Loss 1.504	Prec@1 65.4100	Prec@5 89.6300	
Best Prec@1: [67.120]	
Starting epoch number: 62 Learning rate: 0.1
Train: [62]	Time 84.924	Data 0.305	Loss 0.579	Prec@1 81.8120	Prec@5 97.7380	
Val: [62]	Time 5.108	Data 0.117	Loss 1.301	Prec@1 68.2700	Prec@5 90.7200	
Best Prec@1: [68.270]	
Starting epoch number: 63 Learning rate: 0.1
Train: [63]	Time 84.942	Data 0.308	Loss 0.568	Prec@1 82.1480	Prec@5 97.8160	
Val: [63]	Time 5.118	Data 0.130	Loss 1.443	Prec@1 64.8000	Prec@5 89.5500	
Best Prec@1: [68.270]	
Starting epoch number: 64 Learning rate: 0.1
Train: [64]	Time 85.027	Data 0.342	Loss 0.574	Prec@1 81.9740	Prec@5 97.8260	
Val: [64]	Time 5.118	Data 0.116	Loss 1.485	Prec@1 64.2000	Prec@5 89.3300	
Best Prec@1: [68.270]	
Starting epoch number: 65 Learning rate: 0.1
Train: [65]	Time 85.066	Data 0.313	Loss 0.572	Prec@1 81.9980	Prec@5 97.8860	
Val: [65]	Time 5.109	Data 0.110	Loss 1.443	Prec@1 65.9100	Prec@5 89.8700	
Best Prec@1: [68.270]	
Starting epoch number: 66 Learning rate: 0.1
Train: [66]	Time 85.168	Data 0.338	Loss 0.566	Prec@1 82.3820	Prec@5 97.8640	
Val: [66]	Time 5.097	Data 0.110	Loss 1.378	Prec@1 65.7900	Prec@5 89.9400	
Best Prec@1: [68.270]	
Starting epoch number: 67 Learning rate: 0.1
Train: [67]	Time 85.219	Data 0.328	Loss 0.560	Prec@1 82.4420	Prec@5 97.8120	
Val: [67]	Time 5.089	Data 0.104	Loss 1.469	Prec@1 65.2000	Prec@5 89.9300	
Best Prec@1: [68.270]	
Starting epoch number: 68 Learning rate: 0.1
Train: [68]	Time 85.290	Data 0.364	Loss 0.563	Prec@1 82.4040	Prec@5 97.9160	
Val: [68]	Time 5.083	Data 0.115	Loss 1.503	Prec@1 64.5300	Prec@5 89.3600	
Best Prec@1: [68.270]	
Starting epoch number: 69 Learning rate: 0.1
Train: [69]	Time 85.136	Data 0.327	Loss 0.550	Prec@1 82.7080	Prec@5 98.0000	
Val: [69]	Time 5.094	Data 0.107	Loss 1.378	Prec@1 65.6400	Prec@5 89.6100	
Best Prec@1: [68.270]	
Starting epoch number: 70 Learning rate: 0.1
Train: [70]	Time 85.048	Data 0.308	Loss 0.557	Prec@1 82.3760	Prec@5 97.9040	
Val: [70]	Time 5.075	Data 0.099	Loss 1.346	Prec@1 66.4900	Prec@5 90.3900	
Best Prec@1: [68.270]	
Starting epoch number: 71 Learning rate: 0.1
Train: [71]	Time 84.968	Data 0.342	Loss 0.559	Prec@1 82.1380	Prec@5 98.0020	
Val: [71]	Time 5.111	Data 0.141	Loss 1.373	Prec@1 66.2800	Prec@5 89.9500	
Best Prec@1: [68.270]	
Starting epoch number: 72 Learning rate: 0.1
Train: [72]	Time 84.901	Data 0.329	Loss 0.552	Prec@1 82.6980	Prec@5 97.9580	
Val: [72]	Time 5.111	Data 0.130	Loss 1.348	Prec@1 67.0600	Prec@5 90.1700	
Best Prec@1: [68.270]	
Starting epoch number: 73 Learning rate: 0.1
Train: [73]	Time 84.984	Data 0.347	Loss 0.545	Prec@1 82.7300	Prec@5 98.1180	
Val: [73]	Time 5.076	Data 0.110	Loss 1.473	Prec@1 64.8700	Prec@5 89.1300	
Best Prec@1: [68.270]	
Starting epoch number: 74 Learning rate: 0.1
Train: [74]	Time 85.062	Data 0.343	Loss 0.547	Prec@1 82.8640	Prec@5 98.0440	
Val: [74]	Time 5.099	Data 0.109	Loss 1.344	Prec@1 66.6300	Prec@5 91.0100	
Best Prec@1: [68.270]	
Starting epoch number: 75 Learning rate: 0.1
Train: [75]	Time 85.108	Data 0.335	Loss 0.550	Prec@1 82.6200	Prec@5 98.0620	
Val: [75]	Time 5.135	Data 0.133	Loss 1.485	Prec@1 64.2400	Prec@5 88.0300	
Best Prec@1: [68.270]	
Starting epoch number: 76 Learning rate: 0.1
Train: [76]	Time 85.256	Data 0.407	Loss 0.541	Prec@1 82.8940	Prec@5 98.0600	
Val: [76]	Time 5.159	Data 0.138	Loss 1.409	Prec@1 66.3100	Prec@5 89.7500	
Best Prec@1: [68.270]	
Starting epoch number: 77 Learning rate: 0.1
Train: [77]	Time 85.537	Data 0.572	Loss 0.542	Prec@1 82.9440	Prec@5 98.0360	
Val: [77]	Time 5.198	Data 0.171	Loss 1.441	Prec@1 65.8300	Prec@5 89.9500	
Best Prec@1: [68.270]	
Starting epoch number: 78 Learning rate: 0.1
Train: [78]	Time 85.485	Data 0.544	Loss 0.543	Prec@1 82.8120	Prec@5 98.0060	
Val: [78]	Time 5.174	Data 0.141	Loss 1.549	Prec@1 64.2800	Prec@5 89.0400	
Best Prec@1: [68.270]	
Starting epoch number: 79 Learning rate: 0.1
Train: [79]	Time 85.419	Data 0.463	Loss 0.534	Prec@1 83.2320	Prec@5 98.1240	
Val: [79]	Time 5.164	Data 0.146	Loss 1.324	Prec@1 66.8200	Prec@5 90.5400	
Best Prec@1: [68.270]	
Starting epoch number: 80 Learning rate: 0.1
Train: [80]	Time 85.471	Data 0.551	Loss 0.543	Prec@1 82.9380	Prec@5 98.1100	
Val: [80]	Time 5.178	Data 0.160	Loss 1.410	Prec@1 66.3000	Prec@5 90.1300	
Best Prec@1: [68.270]	
Starting epoch number: 81 Learning rate: 0.1
Train: [81]	Time 85.305	Data 0.456	Loss 0.533	Prec@1 83.1240	Prec@5 98.1600	
Val: [81]	Time 5.168	Data 0.139	Loss 1.483	Prec@1 65.1200	Prec@5 89.9000	
Best Prec@1: [68.270]	
Starting epoch number: 82 Learning rate: 0.1
Train: [82]	Time 85.262	Data 0.498	Loss 0.540	Prec@1 82.9940	Prec@5 98.0080	
Val: [82]	Time 5.164	Data 0.155	Loss 1.434	Prec@1 66.0100	Prec@5 89.5600	
Best Prec@1: [68.270]	
Starting epoch number: 83 Learning rate: 0.1
Train: [83]	Time 85.161	Data 0.501	Loss 0.535	Prec@1 83.0960	Prec@5 98.1180	
Val: [83]	Time 5.154	Data 0.164	Loss 1.517	Prec@1 65.4700	Prec@5 89.3800	
Best Prec@1: [68.270]	
Starting epoch number: 84 Learning rate: 0.1
Train: [84]	Time 85.203	Data 0.497	Loss 0.541	Prec@1 83.0800	Prec@5 98.1000	
Val: [84]	Time 5.149	Data 0.159	Loss 1.363	Prec@1 66.4900	Prec@5 90.0700	
Best Prec@1: [68.270]	
Starting epoch number: 85 Learning rate: 0.1
Train: [85]	Time 85.331	Data 0.515	Loss 0.528	Prec@1 83.3700	Prec@5 98.2300	
Val: [85]	Time 5.161	Data 0.152	Loss 1.514	Prec@1 65.4600	Prec@5 89.2500	
Best Prec@1: [68.270]	
Starting epoch number: 86 Learning rate: 0.1
Train: [86]	Time 85.387	Data 0.585	Loss 0.533	Prec@1 83.1080	Prec@5 98.1940	
Val: [86]	Time 5.221	Data 0.163	Loss 1.506	Prec@1 65.5300	Prec@5 89.1600	
Best Prec@1: [68.270]	
Starting epoch number: 87 Learning rate: 0.1
Train: [87]	Time 85.317	Data 0.455	Loss 0.532	Prec@1 83.3320	Prec@5 98.0900	
Val: [87]	Time 5.118	Data 0.138	Loss 1.490	Prec@1 65.6000	Prec@5 89.6300	
Best Prec@1: [68.270]	
Starting epoch number: 88 Learning rate: 0.1
Train: [88]	Time 85.426	Data 0.506	Loss 0.523	Prec@1 83.5760	Prec@5 98.2080	
Val: [88]	Time 5.097	Data 0.096	Loss 1.492	Prec@1 64.2400	Prec@5 89.4300	
Best Prec@1: [68.270]	
Starting epoch number: 89 Learning rate: 0.1
Train: [89]	Time 84.982	Data 0.229	Loss 0.527	Prec@1 83.3780	Prec@5 98.0560	
Val: [89]	Time 5.090	Data 0.114	Loss 1.464	Prec@1 65.9400	Prec@5 90.0600	
Best Prec@1: [68.270]	
Starting epoch number: 90 Learning rate: 0.1
Train: [90]	Time 84.955	Data 0.256	Loss 0.533	Prec@1 83.1840	Prec@5 98.0800	
Val: [90]	Time 5.107	Data 0.114	Loss 1.412	Prec@1 66.1400	Prec@5 89.7800	
Best Prec@1: [68.270]	
Starting epoch number: 91 Learning rate: 0.1
Train: [91]	Time 84.871	Data 0.254	Loss 0.526	Prec@1 83.3680	Prec@5 98.2120	
Val: [91]	Time 5.092	Data 0.125	Loss 1.517	Prec@1 64.8000	Prec@5 88.7800	
Best Prec@1: [68.270]	
Starting epoch number: 92 Learning rate: 0.1
Train: [92]	Time 84.752	Data 0.226	Loss 0.521	Prec@1 83.4440	Prec@5 98.1680	
Val: [92]	Time 5.079	Data 0.115	Loss 1.410	Prec@1 65.2200	Prec@5 89.4000	
Best Prec@1: [68.270]	
Starting epoch number: 93 Learning rate: 0.1
Train: [93]	Time 84.833	Data 0.254	Loss 0.520	Prec@1 83.4220	Prec@5 98.2860	
Val: [93]	Time 5.079	Data 0.094	Loss 1.324	Prec@1 67.1000	Prec@5 90.9400	
Best Prec@1: [68.270]	
Starting epoch number: 94 Learning rate: 0.1
Train: [94]	Time 84.868	Data 0.235	Loss 0.523	Prec@1 83.6200	Prec@5 98.2000	
Val: [94]	Time 5.091	Data 0.096	Loss 1.406	Prec@1 66.5000	Prec@5 90.2700	
Best Prec@1: [68.270]	
Starting epoch number: 95 Learning rate: 0.1
Train: [95]	Time 84.947	Data 0.257	Loss 0.518	Prec@1 83.5700	Prec@5 98.3080	
Val: [95]	Time 5.093	Data 0.111	Loss 1.442	Prec@1 65.7500	Prec@5 89.9200	
Best Prec@1: [68.270]	
Starting epoch number: 96 Learning rate: 0.1
Train: [96]	Time 84.980	Data 0.243	Loss 0.525	Prec@1 83.4260	Prec@5 98.1320	
Val: [96]	Time 5.095	Data 0.115	Loss 1.357	Prec@1 66.1700	Prec@5 90.1800	
Best Prec@1: [68.270]	
Starting epoch number: 97 Learning rate: 0.1
Train: [97]	Time 85.042	Data 0.241	Loss 0.521	Prec@1 83.5900	Prec@5 98.2900	
Val: [97]	Time 5.107	Data 0.118	Loss 1.443	Prec@1 66.4100	Prec@5 90.0400	
Best Prec@1: [68.270]	
Starting epoch number: 98 Learning rate: 0.1
Train: [98]	Time 85.053	Data 0.229	Loss 0.522	Prec@1 83.7400	Prec@5 98.2640	
Val: [98]	Time 5.122	Data 0.122	Loss 1.735	Prec@1 62.3700	Prec@5 88.2100	
Best Prec@1: [68.270]	
Starting epoch number: 99 Learning rate: 0.1
Train: [99]	Time 85.117	Data 0.240	Loss 0.513	Prec@1 83.7680	Prec@5 98.4080	
Val: [99]	Time 5.123	Data 0.121	Loss 1.400	Prec@1 66.6600	Prec@5 90.5800	
Best Prec@1: [68.270]	
Starting epoch number: 100 Learning rate: 0.1
Train: [100]	Time 85.181	Data 0.237	Loss 0.518	Prec@1 83.7140	Prec@5 98.2700	
Val: [100]	Time 5.111	Data 0.126	Loss 1.452	Prec@1 65.7300	Prec@5 89.4500	
Best Prec@1: [68.270]	
Starting epoch number: 101 Learning rate: 0.1
Train: [101]	Time 85.090	Data 0.238	Loss 0.517	Prec@1 83.5660	Prec@5 98.2400	
Val: [101]	Time 5.123	Data 0.118	Loss 1.334	Prec@1 67.4800	Prec@5 90.3100	
Best Prec@1: [68.270]	
Starting epoch number: 102 Learning rate: 0.1
Train: [102]	Time 84.984	Data 0.223	Loss 0.512	Prec@1 83.7940	Prec@5 98.2920	
Val: [102]	Time 5.108	Data 0.117	Loss 1.462	Prec@1 65.8000	Prec@5 89.2200	
Best Prec@1: [68.270]	
Starting epoch number: 103 Learning rate: 0.1
Train: [103]	Time 84.907	Data 0.241	Loss 0.513	Prec@1 83.9360	Prec@5 98.2640	
Val: [103]	Time 5.105	Data 0.124	Loss 1.512	Prec@1 65.1100	Prec@5 89.2000	
Best Prec@1: [68.270]	
Starting epoch number: 104 Learning rate: 0.1
Train: [104]	Time 84.919	Data 0.248	Loss 0.508	Prec@1 83.9220	Prec@5 98.3320	
Val: [104]	Time 5.083	Data 0.119	Loss 1.543	Prec@1 64.8400	Prec@5 89.4600	
Best Prec@1: [68.270]	
Starting epoch number: 105 Learning rate: 0.1
Train: [105]	Time 84.881	Data 0.252	Loss 0.514	Prec@1 83.7520	Prec@5 98.1920	
Val: [105]	Time 5.070	Data 0.095	Loss 1.299	Prec@1 68.2100	Prec@5 90.7500	
Best Prec@1: [68.270]	
Starting epoch number: 106 Learning rate: 0.1
Train: [106]	Time 84.945	Data 0.244	Loss 0.511	Prec@1 83.8340	Prec@5 98.3500	
Val: [106]	Time 5.144	Data 0.133	Loss 1.514	Prec@1 65.9100	Prec@5 89.9000	
Best Prec@1: [68.270]	
Starting epoch number: 107 Learning rate: 0.1
Train: [107]	Time 85.000	Data 0.241	Loss 0.504	Prec@1 84.0260	Prec@5 98.3840	
Val: [107]	Time 5.111	Data 0.135	Loss 1.411	Prec@1 67.0500	Prec@5 90.1900	
Best Prec@1: [68.270]	
Starting epoch number: 108 Learning rate: 0.1
Train: [108]	Time 85.055	Data 0.241	Loss 0.515	Prec@1 83.8340	Prec@5 98.2160	
Val: [108]	Time 5.119	Data 0.120	Loss 1.447	Prec@1 65.6800	Prec@5 90.0300	
Best Prec@1: [68.270]	
Starting epoch number: 109 Learning rate: 0.1
Train: [109]	Time 85.070	Data 0.226	Loss 0.504	Prec@1 83.8800	Prec@5 98.2740	
Val: [109]	Time 5.084	Data 0.101	Loss 1.495	Prec@1 65.9100	Prec@5 89.7800	
Best Prec@1: [68.270]	
Starting epoch number: 110 Learning rate: 0.1
Train: [110]	Time 85.107	Data 0.242	Loss 0.512	Prec@1 83.8940	Prec@5 98.2700	
Val: [110]	Time 5.109	Data 0.126	Loss 1.511	Prec@1 64.7600	Prec@5 89.0200	
Best Prec@1: [68.270]	
Starting epoch number: 111 Learning rate: 0.1
Train: [111]	Time 85.005	Data 0.225	Loss 0.504	Prec@1 83.9760	Prec@5 98.2940	
Val: [111]	Time 5.056	Data 0.139	Loss 1.360	Prec@1 67.2800	Prec@5 90.3500	
Best Prec@1: [68.270]	
Starting epoch number: 112 Learning rate: 0.1
Train: [112]	Time 85.022	Data 0.242	Loss 0.506	Prec@1 84.1560	Prec@5 98.2740	
Val: [112]	Time 5.044	Data 0.100	Loss 1.380	Prec@1 66.8200	Prec@5 90.5300	
Best Prec@1: [68.270]	
Starting epoch number: 113 Learning rate: 0.1
Train: [113]	Time 84.842	Data 0.225	Loss 0.494	Prec@1 84.3440	Prec@5 98.4100	
Val: [113]	Time 5.120	Data 0.140	Loss 1.377	Prec@1 66.7700	Prec@5 90.5000	
Best Prec@1: [68.270]	
Starting epoch number: 114 Learning rate: 0.1
Train: [114]	Time 84.765	Data 0.237	Loss 0.504	Prec@1 83.9660	Prec@5 98.3620	
Val: [114]	Time 5.102	Data 0.114	Loss 1.385	Prec@1 66.3300	Prec@5 90.5300	
Best Prec@1: [68.270]	
Starting epoch number: 115 Learning rate: 0.1
Train: [115]	Time 84.849	Data 0.237	Loss 0.505	Prec@1 84.0540	Prec@5 98.3120	
Val: [115]	Time 5.085	Data 0.111	Loss 1.348	Prec@1 67.2500	Prec@5 90.2800	
Best Prec@1: [68.270]	
Starting epoch number: 116 Learning rate: 0.1
Train: [116]	Time 84.884	Data 0.218	Loss 0.503	Prec@1 84.0500	Prec@5 98.2900	
Val: [116]	Time 5.087	Data 0.116	Loss 1.290	Prec@1 68.9000	Prec@5 91.5000	
Best Prec@1: [68.900]	
Starting epoch number: 117 Learning rate: 0.1
Train: [117]	Time 85.018	Data 0.274	Loss 0.499	Prec@1 84.1760	Prec@5 98.4140	
Val: [117]	Time 5.087	Data 0.119	Loss 1.397	Prec@1 66.2300	Prec@5 90.1500	
Best Prec@1: [68.900]	
Starting epoch number: 118 Learning rate: 0.1
Train: [118]	Time 85.282	Data 0.410	Loss 0.501	Prec@1 84.1920	Prec@5 98.3600	
Val: [118]	Time 5.100	Data 0.126	Loss 1.342	Prec@1 67.5500	Prec@5 90.7500	
Best Prec@1: [68.900]	
Starting epoch number: 119 Learning rate: 0.1
Train: [119]	Time 85.396	Data 0.452	Loss 0.497	Prec@1 84.2560	Prec@5 98.3120	
Val: [119]	Time 5.145	Data 0.144	Loss 1.518	Prec@1 65.2700	Prec@5 89.3900	
Best Prec@1: [68.900]	
Starting epoch number: 120 Learning rate: 0.1
Train: [120]	Time 85.371	Data 0.460	Loss 0.509	Prec@1 83.9760	Prec@5 98.3640	
Val: [120]	Time 5.154	Data 0.163	Loss 1.272	Prec@1 68.8400	Prec@5 90.9800	
Best Prec@1: [68.900]	
Starting epoch number: 121 Learning rate: 0.1
Train: [121]	Time 85.300	Data 0.479	Loss 0.494	Prec@1 84.3960	Prec@5 98.4280	
Val: [121]	Time 5.128	Data 0.157	Loss 1.374	Prec@1 67.1300	Prec@5 90.5300	
Best Prec@1: [68.900]	
Starting epoch number: 122 Learning rate: 0.1
Train: [122]	Time 85.225	Data 0.482	Loss 0.500	Prec@1 84.2740	Prec@5 98.4000	
Val: [122]	Time 5.134	Data 0.168	Loss 1.344	Prec@1 67.5400	Prec@5 91.0000	
Best Prec@1: [68.900]	
Starting epoch number: 123 Learning rate: 0.1
Train: [123]	Time 85.163	Data 0.519	Loss 0.486	Prec@1 84.8040	Prec@5 98.4200	
Val: [123]	Time 5.097	Data 0.122	Loss 1.459	Prec@1 66.0800	Prec@5 90.0400	
Best Prec@1: [68.900]	
Starting epoch number: 124 Learning rate: 0.1
Train: [124]	Time 85.032	Data 0.466	Loss 0.505	Prec@1 83.9960	Prec@5 98.4280	
Val: [124]	Time 5.147	Data 0.151	Loss 1.408	Prec@1 66.9800	Prec@5 90.4600	
Best Prec@1: [68.900]	
Starting epoch number: 125 Learning rate: 0.1
Train: [125]	Time 85.133	Data 0.494	Loss 0.492	Prec@1 84.4540	Prec@5 98.4500	
Val: [125]	Time 5.118	Data 0.146	Loss 1.393	Prec@1 67.9900	Prec@5 90.4300	
Best Prec@1: [68.900]	
Starting epoch number: 126 Learning rate: 0.1
Train: [126]	Time 85.279	Data 0.543	Loss 0.495	Prec@1 84.2180	Prec@5 98.3820	
Val: [126]	Time 5.156	Data 0.160	Loss 1.477	Prec@1 65.9700	Prec@5 89.4400	
Best Prec@1: [68.900]	
Starting epoch number: 127 Learning rate: 0.1
Train: [127]	Time 85.218	Data 0.453	Loss 0.496	Prec@1 84.3900	Prec@5 98.5440	
Val: [127]	Time 5.153	Data 0.155	Loss 1.722	Prec@1 61.6600	Prec@5 86.6100	
Best Prec@1: [68.900]	
Starting epoch number: 128 Learning rate: 0.1
Train: [128]	Time 85.382	Data 0.533	Loss 0.494	Prec@1 84.3800	Prec@5 98.3380	
Val: [128]	Time 5.159	Data 0.151	Loss 1.339	Prec@1 67.6700	Prec@5 90.1000	
Best Prec@1: [68.900]	
Starting epoch number: 129 Learning rate: 0.1
Train: [129]	Time 85.471	Data 0.546	Loss 0.488	Prec@1 84.3860	Prec@5 98.4400	
Val: [129]	Time 5.171	Data 0.165	Loss 1.474	Prec@1 66.6200	Prec@5 89.6700	
Best Prec@1: [68.900]	
Starting epoch number: 130 Learning rate: 0.1
Train: [130]	Time 85.360	Data 0.425	Loss 0.493	Prec@1 84.2540	Prec@5 98.4640	
Val: [130]	Time 5.130	Data 0.137	Loss 1.370	Prec@1 66.3800	Prec@5 90.3900	
Best Prec@1: [68.900]	
Starting epoch number: 131 Learning rate: 0.1
Train: [131]	Time 85.305	Data 0.387	Loss 0.500	Prec@1 84.2160	Prec@5 98.4600	
Val: [131]	Time 5.134	Data 0.122	Loss 1.346	Prec@1 67.3600	Prec@5 91.1800	
Best Prec@1: [68.900]	
Starting epoch number: 132 Learning rate: 0.1
Train: [132]	Time 85.117	Data 0.247	Loss 0.493	Prec@1 84.2560	Prec@5 98.4120	
Val: [132]	Time 5.131	Data 0.108	Loss 1.430	Prec@1 66.8600	Prec@5 90.0400	
Best Prec@1: [68.900]	
Starting epoch number: 133 Learning rate: 0.1
Train: [133]	Time 85.075	Data 0.247	Loss 0.486	Prec@1 84.5620	Prec@5 98.4800	
Val: [133]	Time 5.114	Data 0.115	Loss 1.390	Prec@1 67.7400	Prec@5 89.7600	
Best Prec@1: [68.900]	
Starting epoch number: 134 Learning rate: 0.1
Train: [134]	Time 84.963	Data 0.250	Loss 0.487	Prec@1 84.4320	Prec@5 98.4460	
Val: [134]	Time 5.106	Data 0.126	Loss 1.398	Prec@1 66.9000	Prec@5 90.3300	
Best Prec@1: [68.900]	
Starting epoch number: 135 Learning rate: 0.1
Train: [135]	Time 84.909	Data 0.242	Loss 0.489	Prec@1 84.4100	Prec@5 98.5420	
Val: [135]	Time 5.092	Data 0.106	Loss 1.513	Prec@1 65.0500	Prec@5 89.5700	
Best Prec@1: [68.900]	
Starting epoch number: 136 Learning rate: 0.1
Train: [136]	Time 84.753	Data 0.232	Loss 0.478	Prec@1 84.9540	Prec@5 98.5200	
Val: [136]	Time 5.105	Data 0.127	Loss 1.489	Prec@1 66.3100	Prec@5 89.8300	
Best Prec@1: [68.900]	
Starting epoch number: 137 Learning rate: 0.1
Train: [137]	Time 84.779	Data 0.239	Loss 0.490	Prec@1 84.3900	Prec@5 98.4080	
Val: [137]	Time 5.094	Data 0.131	Loss 1.352	Prec@1 67.0400	Prec@5 89.9500	
Best Prec@1: [68.900]	
Starting epoch number: 138 Learning rate: 0.1
Train: [138]	Time 84.888	Data 0.221	Loss 0.490	Prec@1 84.6320	Prec@5 98.4420	
Val: [138]	Time 5.045	Data 0.105	Loss 1.376	Prec@1 67.2900	Prec@5 90.4000	
Best Prec@1: [68.900]	
Starting epoch number: 139 Learning rate: 0.1
Train: [139]	Time 84.956	Data 0.222	Loss 0.488	Prec@1 84.6400	Prec@5 98.4220	
Val: [139]	Time 5.080	Data 0.114	Loss 1.515	Prec@1 65.4600	Prec@5 89.9900	
Best Prec@1: [68.900]	
Starting epoch number: 140 Learning rate: 0.1
Train: [140]	Time 85.023	Data 0.261	Loss 0.489	Prec@1 84.5280	Prec@5 98.3940	
Val: [140]	Time 5.098	Data 0.122	Loss 1.440	Prec@1 66.7500	Prec@5 90.2000	
Best Prec@1: [68.900]	
Starting epoch number: 141 Learning rate: 0.1
Train: [141]	Time 85.002	Data 0.222	Loss 0.477	Prec@1 84.9620	Prec@5 98.4980	
Val: [141]	Time 5.107	Data 0.113	Loss 1.467	Prec@1 66.0300	Prec@5 89.4600	
Best Prec@1: [68.900]	
Starting epoch number: 142 Learning rate: 0.1
Train: [142]	Time 85.053	Data 0.233	Loss 0.485	Prec@1 84.6540	Prec@5 98.4800	
Val: [142]	Time 5.089	Data 0.094	Loss 1.344	Prec@1 67.6500	Prec@5 91.3100	
Best Prec@1: [68.900]	
Starting epoch number: 143 Learning rate: 0.1
Train: [143]	Time 85.096	Data 0.232	Loss 0.484	Prec@1 84.6360	Prec@5 98.5100	
Val: [143]	Time 5.091	Data 0.113	Loss 1.346	Prec@1 67.4800	Prec@5 90.3400	
Best Prec@1: [68.900]	
Starting epoch number: 144 Learning rate: 0.1
Train: [144]	Time 85.130	Data 0.263	Loss 0.493	Prec@1 84.5420	Prec@5 98.3380	
Val: [144]	Time 5.086	Data 0.098	Loss 1.301	Prec@1 68.0300	Prec@5 91.1700	
Best Prec@1: [68.900]	
Starting epoch number: 145 Learning rate: 0.1
Train: [145]	Time 84.952	Data 0.236	Loss 0.483	Prec@1 84.8200	Prec@5 98.4120	
Val: [145]	Time 5.110	Data 0.115	Loss 1.514	Prec@1 66.6800	Prec@5 89.4600	
Best Prec@1: [68.900]	
Starting epoch number: 146 Learning rate: 0.1
Train: [146]	Time 84.909	Data 0.217	Loss 0.481	Prec@1 84.6920	Prec@5 98.5320	
Val: [146]	Time 5.067	Data 0.096	Loss 1.416	Prec@1 67.2200	Prec@5 89.8100	
Best Prec@1: [68.900]	
Starting epoch number: 147 Learning rate: 0.1
Train: [147]	Time 84.802	Data 0.231	Loss 0.488	Prec@1 84.6040	Prec@5 98.4960	
Val: [147]	Time 5.170	Data 0.102	Loss 1.348	Prec@1 67.1500	Prec@5 90.2100	
Best Prec@1: [68.900]	
Starting epoch number: 148 Learning rate: 0.1
Train: [148]	Time 84.819	Data 0.249	Loss 0.475	Prec@1 85.0720	Prec@5 98.5380	
Val: [148]	Time 5.105	Data 0.104	Loss 1.388	Prec@1 67.1900	Prec@5 90.4700	
Best Prec@1: [68.900]	
Starting epoch number: 149 Learning rate: 0.1
Train: [149]	Time 84.936	Data 0.243	Loss 0.492	Prec@1 84.4560	Prec@5 98.4620	
Val: [149]	Time 5.092	Data 0.107	Loss 1.305	Prec@1 68.6000	Prec@5 91.1100	
Best Prec@1: [68.900]	
Starting epoch number: 150 Learning rate: 0.010000000000000002
Train: [150]	Time 84.983	Data 0.238	Loss 0.204	Prec@1 94.1280	Prec@5 99.7240	
Val: [150]	Time 5.080	Data 0.107	Loss 0.886	Prec@1 77.5400	Prec@5 94.5100	
Best Prec@1: [77.540]	
Starting epoch number: 151 Learning rate: 0.010000000000000002
Train: [151]	Time 85.034	Data 0.223	Loss 0.118	Prec@1 97.1240	Prec@5 99.9560	
Val: [151]	Time 5.121	Data 0.117	Loss 0.886	Prec@1 78.3700	Prec@5 94.5800	
Best Prec@1: [78.370]	
Starting epoch number: 152 Learning rate: 0.010000000000000002
Train: [152]	Time 85.173	Data 0.258	Loss 0.092	Prec@1 97.9480	Prec@5 99.9680	
Val: [152]	Time 5.117	Data 0.132	Loss 0.887	Prec@1 78.1400	Prec@5 94.7100	
Best Prec@1: [78.370]	
Starting epoch number: 153 Learning rate: 0.010000000000000002
Train: [153]	Time 85.539	Data 0.243	Loss 0.077	Prec@1 98.3540	Prec@5 99.9740	
Val: [153]	Time 5.139	Data 0.100	Loss 0.899	Prec@1 78.3200	Prec@5 94.7100	
Best Prec@1: [78.370]	
Starting epoch number: 154 Learning rate: 0.010000000000000002
Train: [154]	Time 85.545	Data 0.235	Loss 0.067	Prec@1 98.6780	Prec@5 99.9960	
Val: [154]	Time 5.156	Data 0.112	Loss 0.904	Prec@1 78.3100	Prec@5 94.8200	
Best Prec@1: [78.370]	
Starting epoch number: 155 Learning rate: 0.010000000000000002
Train: [155]	Time 85.451	Data 0.223	Loss 0.059	Prec@1 98.9120	Prec@5 99.9980	
Val: [155]	Time 5.129	Data 0.132	Loss 0.904	Prec@1 78.4200	Prec@5 94.7800	
Best Prec@1: [78.420]	
Starting epoch number: 156 Learning rate: 0.010000000000000002
Train: [156]	Time 85.415	Data 0.231	Loss 0.053	Prec@1 99.0660	Prec@5 99.9940	
Val: [156]	Time 5.117	Data 0.100	Loss 0.909	Prec@1 78.7000	Prec@5 94.8700	
Best Prec@1: [78.700]	
Starting epoch number: 157 Learning rate: 0.010000000000000002
Train: [157]	Time 85.249	Data 0.221	Loss 0.048	Prec@1 99.1960	Prec@5 99.9980	
Val: [157]	Time 5.087	Data 0.097	Loss 0.912	Prec@1 78.6300	Prec@5 94.8400	
Best Prec@1: [78.700]	
Starting epoch number: 158 Learning rate: 0.010000000000000002
Train: [158]	Time 85.139	Data 0.236	Loss 0.044	Prec@1 99.3440	Prec@5 99.9980	
Val: [158]	Time 5.079	Data 0.113	Loss 0.918	Prec@1 78.7100	Prec@5 94.9400	
Best Prec@1: [78.710]	
Starting epoch number: 159 Learning rate: 0.010000000000000002
Train: [159]	Time 85.088	Data 0.227	Loss 0.040	Prec@1 99.3940	Prec@5 99.9980	
Val: [159]	Time 5.094	Data 0.119	Loss 0.924	Prec@1 78.6000	Prec@5 94.9100	
Best Prec@1: [78.710]	
Starting epoch number: 160 Learning rate: 0.010000000000000002
Train: [160]	Time 85.072	Data 0.222	Loss 0.038	Prec@1 99.4300	Prec@5 100.0000	
Val: [160]	Time 5.118	Data 0.124	Loss 0.931	Prec@1 78.6100	Prec@5 94.7900	
Best Prec@1: [78.710]	
Starting epoch number: 161 Learning rate: 0.010000000000000002
Train: [161]	Time 85.063	Data 0.246	Loss 0.037	Prec@1 99.4020	Prec@5 100.0000	
Val: [161]	Time 5.109	Data 0.122	Loss 0.938	Prec@1 78.5800	Prec@5 94.9200	
Best Prec@1: [78.710]	
Starting epoch number: 162 Learning rate: 0.010000000000000002
Train: [162]	Time 84.999	Data 0.241	Loss 0.033	Prec@1 99.5720	Prec@5 99.9980	
Val: [162]	Time 5.075	Data 0.104	Loss 0.934	Prec@1 78.5100	Prec@5 94.9000	
Best Prec@1: [78.710]	
Starting epoch number: 163 Learning rate: 0.010000000000000002
Train: [163]	Time 84.943	Data 0.251	Loss 0.031	Prec@1 99.6020	Prec@5 99.9960	
Val: [163]	Time 5.099	Data 0.114	Loss 0.938	Prec@1 78.5900	Prec@5 94.8600	
Best Prec@1: [78.710]	
Starting epoch number: 164 Learning rate: 0.010000000000000002
Train: [164]	Time 84.969	Data 0.244	Loss 0.030	Prec@1 99.6140	Prec@5 100.0000	
Val: [164]	Time 5.106	Data 0.125	Loss 0.943	Prec@1 78.5500	Prec@5 94.7000	
Best Prec@1: [78.710]	
Starting epoch number: 165 Learning rate: 0.010000000000000002
Train: [165]	Time 85.013	Data 0.231	Loss 0.028	Prec@1 99.6940	Prec@5 99.9980	
Val: [165]	Time 5.106	Data 0.110	Loss 0.941	Prec@1 78.7700	Prec@5 94.7400	
Best Prec@1: [78.770]	
Starting epoch number: 166 Learning rate: 0.010000000000000002
Train: [166]	Time 85.027	Data 0.240	Loss 0.027	Prec@1 99.7000	Prec@5 100.0000	
Val: [166]	Time 5.099	Data 0.107	Loss 0.944	Prec@1 78.7600	Prec@5 94.8100	
Best Prec@1: [78.770]	
Starting epoch number: 167 Learning rate: 0.010000000000000002
Train: [167]	Time 85.045	Data 0.230	Loss 0.026	Prec@1 99.7100	Prec@5 100.0000	
Val: [167]	Time 5.159	Data 0.099	Loss 0.941	Prec@1 78.7000	Prec@5 94.8100	
Best Prec@1: [78.770]	
Starting epoch number: 168 Learning rate: 0.010000000000000002
Train: [168]	Time 85.029	Data 0.223	Loss 0.025	Prec@1 99.7440	Prec@5 100.0000	
Val: [168]	Time 5.118	Data 0.112	Loss 0.949	Prec@1 78.4900	Prec@5 94.8900	
Best Prec@1: [78.770]	
Starting epoch number: 169 Learning rate: 0.010000000000000002
Train: [169]	Time 85.100	Data 0.226	Loss 0.024	Prec@1 99.7760	Prec@5 99.9980	
Val: [169]	Time 5.110	Data 0.098	Loss 0.950	Prec@1 78.4300	Prec@5 94.7400	
Best Prec@1: [78.770]	
Starting epoch number: 170 Learning rate: 0.010000000000000002
Train: [170]	Time 85.154	Data 0.218	Loss 0.023	Prec@1 99.7960	Prec@5 99.9980	
Val: [170]	Time 5.118	Data 0.112	Loss 0.952	Prec@1 78.7400	Prec@5 94.8100	
Best Prec@1: [78.770]	
Starting epoch number: 171 Learning rate: 0.010000000000000002
Train: [171]	Time 85.171	Data 0.217	Loss 0.022	Prec@1 99.7660	Prec@5 100.0000	
Val: [171]	Time 5.106	Data 0.124	Loss 0.955	Prec@1 78.7100	Prec@5 94.8200	
Best Prec@1: [78.770]	
Starting epoch number: 172 Learning rate: 0.010000000000000002
Train: [172]	Time 85.185	Data 0.258	Loss 0.022	Prec@1 99.8200	Prec@5 99.9960	
Val: [172]	Time 5.098	Data 0.099	Loss 0.952	Prec@1 78.7700	Prec@5 94.7900	
Best Prec@1: [78.770]	
Starting epoch number: 173 Learning rate: 0.010000000000000002
Train: [173]	Time 85.088	Data 0.245	Loss 0.021	Prec@1 99.8320	Prec@5 100.0000	
Val: [173]	Time 5.126	Data 0.127	Loss 0.953	Prec@1 78.6400	Prec@5 94.7800	
Best Prec@1: [78.770]	
Starting epoch number: 174 Learning rate: 0.010000000000000002
Train: [174]	Time 85.072	Data 0.258	Loss 0.021	Prec@1 99.8020	Prec@5 100.0000	
Val: [174]	Time 5.116	Data 0.110	Loss 0.958	Prec@1 78.5500	Prec@5 94.9400	
Best Prec@1: [78.770]	
Starting epoch number: 175 Learning rate: 0.010000000000000002
Train: [175]	Time 85.128	Data 0.229	Loss 0.020	Prec@1 99.8340	Prec@5 100.0000	
Val: [175]	Time 5.018	Data 0.106	Loss 0.962	Prec@1 78.4000	Prec@5 94.7300	
Best Prec@1: [78.770]	
Starting epoch number: 176 Learning rate: 0.010000000000000002
Train: [176]	Time 85.082	Data 0.251	Loss 0.019	Prec@1 99.8260	Prec@5 100.0000	
Val: [176]	Time 5.178	Data 0.136	Loss 0.958	Prec@1 78.7300	Prec@5 94.6800	
Best Prec@1: [78.770]	
Starting epoch number: 177 Learning rate: 0.010000000000000002
Train: [177]	Time 84.908	Data 0.218	Loss 0.019	Prec@1 99.8760	Prec@5 100.0000	
Val: [177]	Time 5.084	Data 0.104	Loss 0.960	Prec@1 78.6600	Prec@5 94.7100	
Best Prec@1: [78.770]	
Starting epoch number: 178 Learning rate: 0.010000000000000002
Train: [178]	Time 84.950	Data 0.239	Loss 0.018	Prec@1 99.8720	Prec@5 100.0000	
Val: [178]	Time 5.075	Data 0.103	Loss 0.954	Prec@1 78.6500	Prec@5 94.6900	
Best Prec@1: [78.770]	
Starting epoch number: 179 Learning rate: 0.010000000000000002
Train: [179]	Time 84.976	Data 0.233	Loss 0.018	Prec@1 99.8800	Prec@5 100.0000	
Val: [179]	Time 5.097	Data 0.107	Loss 0.953	Prec@1 78.5200	Prec@5 94.8800	
Best Prec@1: [78.770]	
Starting epoch number: 180 Learning rate: 0.010000000000000002
Train: [180]	Time 85.003	Data 0.216	Loss 0.018	Prec@1 99.8580	Prec@5 100.0000	
Val: [180]	Time 5.097	Data 0.104	Loss 0.959	Prec@1 78.7600	Prec@5 94.6800	
Best Prec@1: [78.770]	
Starting epoch number: 181 Learning rate: 0.010000000000000002
Train: [181]	Time 85.071	Data 0.245	Loss 0.018	Prec@1 99.8660	Prec@5 100.0000	
Val: [181]	Time 5.104	Data 0.110	Loss 0.962	Prec@1 78.5600	Prec@5 94.7400	
Best Prec@1: [78.770]	
Starting epoch number: 182 Learning rate: 0.010000000000000002
Train: [182]	Time 85.102	Data 0.238	Loss 0.017	Prec@1 99.8640	Prec@5 100.0000	
Val: [182]	Time 5.114	Data 0.107	Loss 0.959	Prec@1 78.6100	Prec@5 94.8100	
Best Prec@1: [78.770]	
Starting epoch number: 183 Learning rate: 0.010000000000000002
Train: [183]	Time 85.142	Data 0.257	Loss 0.017	Prec@1 99.8720	Prec@5 100.0000	
Val: [183]	Time 5.108	Data 0.108	Loss 0.954	Prec@1 78.6100	Prec@5 94.6800	
Best Prec@1: [78.770]	
Starting epoch number: 184 Learning rate: 0.010000000000000002
Train: [184]	Time 85.186	Data 0.217	Loss 0.016	Prec@1 99.8780	Prec@5 100.0000	
Val: [184]	Time 5.103	Data 0.098	Loss 0.953	Prec@1 78.7400	Prec@5 94.7800	
Best Prec@1: [78.770]	
Starting epoch number: 185 Learning rate: 0.010000000000000002
Train: [185]	Time 85.231	Data 0.238	Loss 0.016	Prec@1 99.8880	Prec@5 100.0000	
Val: [185]	Time 5.085	Data 0.104	Loss 0.952	Prec@1 78.8100	Prec@5 94.7300	
Best Prec@1: [78.810]	
Starting epoch number: 186 Learning rate: 0.010000000000000002
Train: [186]	Time 85.309	Data 0.238	Loss 0.017	Prec@1 99.8900	Prec@5 100.0000	
Val: [186]	Time 5.105	Data 0.104	Loss 0.949	Prec@1 78.8800	Prec@5 94.7800	
Best Prec@1: [78.880]	
Starting epoch number: 187 Learning rate: 0.010000000000000002
Train: [187]	Time 85.250	Data 0.222	Loss 0.016	Prec@1 99.8900	Prec@5 100.0000	
Val: [187]	Time 5.101	Data 0.109	Loss 0.947	Prec@1 78.7200	Prec@5 94.7100	
Best Prec@1: [78.880]	
Starting epoch number: 188 Learning rate: 0.010000000000000002
Train: [188]	Time 85.180	Data 0.244	Loss 0.016	Prec@1 99.8760	Prec@5 100.0000	
Val: [188]	Time 5.098	Data 0.111	Loss 0.951	Prec@1 78.6600	Prec@5 94.6900	
Best Prec@1: [78.880]	
Starting epoch number: 189 Learning rate: 0.010000000000000002
Train: [189]	Time 85.147	Data 0.251	Loss 0.016	Prec@1 99.8900	Prec@5 100.0000	
Val: [189]	Time 5.118	Data 0.120	Loss 0.949	Prec@1 78.7300	Prec@5 94.7300	
Best Prec@1: [78.880]	
Starting epoch number: 190 Learning rate: 0.010000000000000002
Train: [190]	Time 85.153	Data 0.238	Loss 0.016	Prec@1 99.8960	Prec@5 100.0000	
Val: [190]	Time 5.106	Data 0.104	Loss 0.946	Prec@1 79.0300	Prec@5 94.7400	
Best Prec@1: [79.030]	
Starting epoch number: 191 Learning rate: 0.010000000000000002
Train: [191]	Time 85.134	Data 0.241	Loss 0.016	Prec@1 99.8840	Prec@5 100.0000	
Val: [191]	Time 5.113	Data 0.124	Loss 0.954	Prec@1 78.7600	Prec@5 94.6800	
Best Prec@1: [79.030]	
Starting epoch number: 192 Learning rate: 0.010000000000000002
Train: [192]	Time 85.022	Data 0.241	Loss 0.015	Prec@1 99.9340	Prec@5 100.0000	
Val: [192]	Time 5.106	Data 0.111	Loss 0.949	Prec@1 78.7100	Prec@5 94.5300	
Best Prec@1: [79.030]	
Starting epoch number: 193 Learning rate: 0.010000000000000002
Train: [193]	Time 84.976	Data 0.230	Loss 0.015	Prec@1 99.9200	Prec@5 100.0000	
Val: [193]	Time 5.046	Data 0.098	Loss 0.960	Prec@1 78.4400	Prec@5 94.7000	
Best Prec@1: [79.030]	
Starting epoch number: 194 Learning rate: 0.010000000000000002
Train: [194]	Time 84.995	Data 0.222	Loss 0.015	Prec@1 99.9020	Prec@5 100.0000	
Val: [194]	Time 5.099	Data 0.103	Loss 0.952	Prec@1 78.7300	Prec@5 94.7900	
Best Prec@1: [79.030]	
Starting epoch number: 195 Learning rate: 0.010000000000000002
Train: [195]	Time 84.985	Data 0.223	Loss 0.015	Prec@1 99.9060	Prec@5 100.0000	
Val: [195]	Time 5.058	Data 0.115	Loss 0.959	Prec@1 78.7500	Prec@5 94.5900	
Best Prec@1: [79.030]	
Starting epoch number: 196 Learning rate: 0.010000000000000002
Train: [196]	Time 85.040	Data 0.220	Loss 0.014	Prec@1 99.9060	Prec@5 100.0000	
Val: [196]	Time 5.113	Data 0.123	Loss 0.958	Prec@1 78.6400	Prec@5 94.5300	
Best Prec@1: [79.030]	
Starting epoch number: 197 Learning rate: 0.010000000000000002
Train: [197]	Time 85.029	Data 0.220	Loss 0.015	Prec@1 99.8920	Prec@5 100.0000	
Val: [197]	Time 5.097	Data 0.113	Loss 0.957	Prec@1 78.5000	Prec@5 94.5800	
Best Prec@1: [79.030]	
Starting epoch number: 198 Learning rate: 0.010000000000000002
Train: [198]	Time 85.124	Data 0.257	Loss 0.014	Prec@1 99.9160	Prec@5 100.0000	
Val: [198]	Time 5.100	Data 0.119	Loss 0.938	Prec@1 78.7200	Prec@5 94.6700	
Best Prec@1: [79.030]	
Starting epoch number: 199 Learning rate: 0.010000000000000002
Train: [199]	Time 85.112	Data 0.232	Loss 0.014	Prec@1 99.9140	Prec@5 100.0000	
Val: [199]	Time 5.095	Data 0.096	Loss 0.952	Prec@1 78.9100	Prec@5 94.5700	
Best Prec@1: [79.030]	
Starting epoch number: 200 Learning rate: 0.010000000000000002
Train: [200]	Time 85.090	Data 0.241	Loss 0.014	Prec@1 99.8940	Prec@5 100.0000	
Val: [200]	Time 5.135	Data 0.114	Loss 0.951	Prec@1 78.7000	Prec@5 94.6000	
Best Prec@1: [79.030]	
Starting epoch number: 201 Learning rate: 0.010000000000000002
Train: [201]	Time 85.101	Data 0.216	Loss 0.013	Prec@1 99.9400	Prec@5 100.0000	
Val: [201]	Time 5.098	Data 0.092	Loss 0.950	Prec@1 78.8300	Prec@5 94.5700	
Best Prec@1: [79.030]	
Starting epoch number: 202 Learning rate: 0.010000000000000002
Train: [202]	Time 85.128	Data 0.217	Loss 0.013	Prec@1 99.9440	Prec@5 100.0000	
Val: [202]	Time 5.122	Data 0.110	Loss 0.952	Prec@1 78.7600	Prec@5 94.6300	
Best Prec@1: [79.030]	
Starting epoch number: 203 Learning rate: 0.010000000000000002
Train: [203]	Time 85.181	Data 0.247	Loss 0.013	Prec@1 99.9380	Prec@5 100.0000	
Val: [203]	Time 5.046	Data 0.100	Loss 0.967	Prec@1 78.6200	Prec@5 94.5900	
Best Prec@1: [79.030]	
Starting epoch number: 204 Learning rate: 0.010000000000000002
Train: [204]	Time 85.241	Data 0.239	Loss 0.013	Prec@1 99.9220	Prec@5 100.0000	
Val: [204]	Time 5.111	Data 0.117	Loss 0.949	Prec@1 78.6400	Prec@5 94.4500	
Best Prec@1: [79.030]	
Starting epoch number: 205 Learning rate: 0.010000000000000002
Train: [205]	Time 85.265	Data 0.267	Loss 0.013	Prec@1 99.9380	Prec@5 100.0000	
Val: [205]	Time 5.115	Data 0.114	Loss 0.957	Prec@1 78.4300	Prec@5 94.6200	
Best Prec@1: [79.030]	
Starting epoch number: 206 Learning rate: 0.010000000000000002
Train: [206]	Time 85.245	Data 0.235	Loss 0.013	Prec@1 99.9240	Prec@5 100.0000	
Val: [206]	Time 5.105	Data 0.100	Loss 0.959	Prec@1 78.5100	Prec@5 94.5300	
Best Prec@1: [79.030]	
Starting epoch number: 207 Learning rate: 0.010000000000000002
Train: [207]	Time 85.265	Data 0.243	Loss 0.013	Prec@1 99.9160	Prec@5 100.0000	
Val: [207]	Time 5.069	Data 0.117	Loss 0.950	Prec@1 78.7800	Prec@5 94.5400	
Best Prec@1: [79.030]	
Starting epoch number: 208 Learning rate: 0.010000000000000002
Train: [208]	Time 85.279	Data 0.235	Loss 0.013	Prec@1 99.9360	Prec@5 100.0000	
Val: [208]	Time 5.088	Data 0.096	Loss 0.953	Prec@1 78.4700	Prec@5 94.6000	
Best Prec@1: [79.030]	
Starting epoch number: 209 Learning rate: 0.010000000000000002
Train: [209]	Time 85.271	Data 0.234	Loss 0.012	Prec@1 99.9460	Prec@5 100.0000	
Val: [209]	Time 5.073	Data 0.124	Loss 0.961	Prec@1 78.4200	Prec@5 94.4300	
Best Prec@1: [79.030]	
Starting epoch number: 210 Learning rate: 0.010000000000000002
Train: [210]	Time 85.361	Data 0.222	Loss 0.013	Prec@1 99.9300	Prec@5 100.0000	
Val: [210]	Time 5.111	Data 0.106	Loss 0.957	Prec@1 78.6700	Prec@5 94.5100	
Best Prec@1: [79.030]	
Starting epoch number: 211 Learning rate: 0.010000000000000002
Train: [211]	Time 85.250	Data 0.222	Loss 0.012	Prec@1 99.9380	Prec@5 100.0000	
Val: [211]	Time 5.100	Data 0.099	Loss 0.955	Prec@1 78.7700	Prec@5 94.5200	
Best Prec@1: [79.030]	
Starting epoch number: 212 Learning rate: 0.010000000000000002
Train: [212]	Time 85.264	Data 0.239	Loss 0.013	Prec@1 99.9280	Prec@5 100.0000	
Val: [212]	Time 5.098	Data 0.115	Loss 0.970	Prec@1 78.4200	Prec@5 94.4800	
Best Prec@1: [79.030]	
Starting epoch number: 213 Learning rate: 0.010000000000000002
Train: [213]	Time 85.155	Data 0.225	Loss 0.013	Prec@1 99.9060	Prec@5 100.0000	
Val: [213]	Time 5.084	Data 0.099	Loss 0.963	Prec@1 78.3500	Prec@5 94.4200	
Best Prec@1: [79.030]	
Starting epoch number: 214 Learning rate: 0.010000000000000002
Train: [214]	Time 85.060	Data 0.234	Loss 0.014	Prec@1 99.8940	Prec@5 100.0000	
Val: [214]	Time 5.109	Data 0.100	Loss 0.967	Prec@1 78.3700	Prec@5 94.4100	
Best Prec@1: [79.030]	
Starting epoch number: 215 Learning rate: 0.010000000000000002
Train: [215]	Time 85.121	Data 0.223	Loss 0.013	Prec@1 99.9120	Prec@5 100.0000	
Val: [215]	Time 5.111	Data 0.119	Loss 0.957	Prec@1 78.6200	Prec@5 94.3700	
Best Prec@1: [79.030]	
Starting epoch number: 216 Learning rate: 0.010000000000000002
Train: [216]	Time 85.088	Data 0.234	Loss 0.013	Prec@1 99.9400	Prec@5 100.0000	
Val: [216]	Time 5.096	Data 0.107	Loss 0.953	Prec@1 78.5200	Prec@5 94.5200	
Best Prec@1: [79.030]	
Starting epoch number: 217 Learning rate: 0.010000000000000002
Train: [217]	Time 85.016	Data 0.235	Loss 0.012	Prec@1 99.9480	Prec@5 100.0000	
Val: [217]	Time 5.094	Data 0.111	Loss 0.951	Prec@1 78.6200	Prec@5 94.4800	
Best Prec@1: [79.030]	
Starting epoch number: 218 Learning rate: 0.010000000000000002
Train: [218]	Time 84.926	Data 0.243	Loss 0.012	Prec@1 99.9460	Prec@5 100.0000	
Val: [218]	Time 5.110	Data 0.098	Loss 0.954	Prec@1 78.3700	Prec@5 94.4600	
Best Prec@1: [79.030]	
Starting epoch number: 219 Learning rate: 0.010000000000000002
Train: [219]	Time 84.963	Data 0.231	Loss 0.012	Prec@1 99.9320	Prec@5 100.0000	
Val: [219]	Time 5.050	Data 0.113	Loss 0.960	Prec@1 78.5500	Prec@5 94.6000	
Best Prec@1: [79.030]	
Starting epoch number: 220 Learning rate: 0.010000000000000002
Train: [220]	Time 85.067	Data 0.233	Loss 0.012	Prec@1 99.9500	Prec@5 100.0000	
Val: [220]	Time 5.077	Data 0.098	Loss 0.963	Prec@1 78.4400	Prec@5 94.2900	
Best Prec@1: [79.030]	
Starting epoch number: 221 Learning rate: 0.010000000000000002
Train: [221]	Time 85.014	Data 0.221	Loss 0.012	Prec@1 99.9320	Prec@5 100.0000	
Val: [221]	Time 5.116	Data 0.121	Loss 0.958	Prec@1 78.5900	Prec@5 94.5200	
Best Prec@1: [79.030]	
Starting epoch number: 222 Learning rate: 0.010000000000000002
Train: [222]	Time 85.041	Data 0.242	Loss 0.012	Prec@1 99.9360	Prec@5 100.0000	
Val: [222]	Time 5.037	Data 0.127	Loss 0.960	Prec@1 78.2900	Prec@5 94.3900	
Best Prec@1: [79.030]	
Starting epoch number: 223 Learning rate: 0.010000000000000002
Train: [223]	Time 85.158	Data 0.244	Loss 0.012	Prec@1 99.9440	Prec@5 100.0000	
Val: [223]	Time 5.111	Data 0.118	Loss 0.992	Prec@1 78.0400	Prec@5 94.3700	
Best Prec@1: [79.030]	
Starting epoch number: 224 Learning rate: 0.010000000000000002
Train: [224]	Time 85.071	Data 0.239	Loss 0.012	Prec@1 99.9480	Prec@5 100.0000	
Val: [224]	Time 5.102	Data 0.113	Loss 0.956	Prec@1 78.5900	Prec@5 94.6100	
Best Prec@1: [79.030]	
Starting epoch number: 225 Learning rate: 0.0010000000000000002
Train: [225]	Time 85.106	Data 0.243	Loss 0.010	Prec@1 99.9580	Prec@5 100.0000	
Val: [225]	Time 5.121	Data 0.105	Loss 0.950	Prec@1 78.8300	Prec@5 94.5700	
Best Prec@1: [79.030]	
Starting epoch number: 226 Learning rate: 0.0010000000000000002
Train: [226]	Time 85.092	Data 0.220	Loss 0.010	Prec@1 99.9580	Prec@5 100.0000	
Val: [226]	Time 5.121	Data 0.117	Loss 0.953	Prec@1 78.8500	Prec@5 94.5400	
Best Prec@1: [79.030]	
Starting epoch number: 227 Learning rate: 0.0010000000000000002
Train: [227]	Time 85.201	Data 0.224	Loss 0.009	Prec@1 99.9660	Prec@5 100.0000	
Val: [227]	Time 5.141	Data 0.141	Loss 0.950	Prec@1 78.7400	Prec@5 94.6300	
Best Prec@1: [79.030]	
Starting epoch number: 228 Learning rate: 0.0010000000000000002
Train: [228]	Time 85.200	Data 0.234	Loss 0.009	Prec@1 99.9680	Prec@5 100.0000	
Val: [228]	Time 5.129	Data 0.120	Loss 0.955	Prec@1 78.6600	Prec@5 94.5700	
Best Prec@1: [79.030]	
Starting epoch number: 229 Learning rate: 0.0010000000000000002
Train: [229]	Time 85.249	Data 0.237	Loss 0.009	Prec@1 99.9500	Prec@5 100.0000	
Val: [229]	Time 5.096	Data 0.101	Loss 0.953	Prec@1 78.5400	Prec@5 94.5600	
Best Prec@1: [79.030]	
Starting epoch number: 230 Learning rate: 0.0010000000000000002
Train: [230]	Time 85.263	Data 0.238	Loss 0.009	Prec@1 99.9720	Prec@5 100.0000	
Val: [230]	Time 5.078	Data 0.096	Loss 0.954	Prec@1 78.5700	Prec@5 94.6000	
Best Prec@1: [79.030]	
Starting epoch number: 231 Learning rate: 0.0010000000000000002
Train: [231]	Time 85.254	Data 0.226	Loss 0.009	Prec@1 99.9640	Prec@5 100.0000	
Val: [231]	Time 5.096	Data 0.096	Loss 0.947	Prec@1 78.8400	Prec@5 94.5300	
Best Prec@1: [79.030]	
Starting epoch number: 232 Learning rate: 0.0010000000000000002
Train: [232]	Time 85.213	Data 0.257	Loss 0.009	Prec@1 99.9680	Prec@5 100.0000	
Val: [232]	Time 5.099	Data 0.096	Loss 0.946	Prec@1 78.9200	Prec@5 94.5500	
Best Prec@1: [79.030]	
Starting epoch number: 233 Learning rate: 0.0010000000000000002
Train: [233]	Time 85.113	Data 0.242	Loss 0.009	Prec@1 99.9660	Prec@5 100.0000	
Val: [233]	Time 5.108	Data 0.096	Loss 0.950	Prec@1 78.8800	Prec@5 94.5200	
Best Prec@1: [79.030]	
Starting epoch number: 234 Learning rate: 0.0010000000000000002
Train: [234]	Time 85.104	Data 0.225	Loss 0.009	Prec@1 99.9640	Prec@5 100.0000	
Val: [234]	Time 5.105	Data 0.121	Loss 0.947	Prec@1 78.9800	Prec@5 94.6000	
Best Prec@1: [79.030]	
Starting epoch number: 235 Learning rate: 0.0010000000000000002
Train: [235]	Time 85.124	Data 0.246	Loss 0.009	Prec@1 99.9620	Prec@5 100.0000	
Val: [235]	Time 5.138	Data 0.132	Loss 0.950	Prec@1 78.8400	Prec@5 94.6100	
Best Prec@1: [79.030]	
Starting epoch number: 236 Learning rate: 0.0010000000000000002
Train: [236]	Time 85.075	Data 0.245	Loss 0.009	Prec@1 99.9640	Prec@5 100.0000	
Val: [236]	Time 5.082	Data 0.098	Loss 0.950	Prec@1 78.6600	Prec@5 94.6700	
Best Prec@1: [79.030]	
Starting epoch number: 237 Learning rate: 0.0010000000000000002
Train: [237]	Time 84.980	Data 0.245	Loss 0.009	Prec@1 99.9680	Prec@5 100.0000	
Val: [237]	Time 5.086	Data 0.121	Loss 0.951	Prec@1 78.7700	Prec@5 94.6000	
Best Prec@1: [79.030]	
Starting epoch number: 238 Learning rate: 0.0010000000000000002
Train: [238]	Time 85.008	Data 0.229	Loss 0.009	Prec@1 99.9680	Prec@5 100.0000	
Val: [238]	Time 5.127	Data 0.111	Loss 0.952	Prec@1 78.6300	Prec@5 94.5600	
Best Prec@1: [79.030]	
Starting epoch number: 239 Learning rate: 0.0010000000000000002
Train: [239]	Time 84.998	Data 0.248	Loss 0.009	Prec@1 99.9720	Prec@5 100.0000	
Val: [239]	Time 5.123	Data 0.130	Loss 0.944	Prec@1 78.7100	Prec@5 94.6700	
Best Prec@1: [79.030]	
Starting epoch number: 240 Learning rate: 0.0010000000000000002
Train: [240]	Time 85.043	Data 0.244	Loss 0.008	Prec@1 99.9720	Prec@5 100.0000	
Val: [240]	Time 5.120	Data 0.118	Loss 0.952	Prec@1 78.6500	Prec@5 94.5300	
Best Prec@1: [79.030]	
Starting epoch number: 241 Learning rate: 0.0010000000000000002
Train: [241]	Time 85.064	Data 0.240	Loss 0.009	Prec@1 99.9780	Prec@5 100.0000	
Val: [241]	Time 5.109	Data 0.118	Loss 0.945	Prec@1 78.9100	Prec@5 94.5700	
Best Prec@1: [79.030]	
Starting epoch number: 242 Learning rate: 0.0010000000000000002
Train: [242]	Time 85.083	Data 0.222	Loss 0.008	Prec@1 99.9700	Prec@5 100.0000	
Val: [242]	Time 5.095	Data 0.116	Loss 0.948	Prec@1 78.6800	Prec@5 94.5000	
Best Prec@1: [79.030]	
Starting epoch number: 243 Learning rate: 0.0010000000000000002
Train: [243]	Time 85.124	Data 0.220	Loss 0.009	Prec@1 99.9680	Prec@5 100.0000	
Val: [243]	Time 5.099	Data 0.108	Loss 0.947	Prec@1 78.8800	Prec@5 94.5100	
Best Prec@1: [79.030]	
Starting epoch number: 244 Learning rate: 0.0010000000000000002
Train: [244]	Time 85.125	Data 0.244	Loss 0.008	Prec@1 99.9660	Prec@5 100.0000	
Val: [244]	Time 5.073	Data 0.114	Loss 0.950	Prec@1 78.8400	Prec@5 94.5700	
Best Prec@1: [79.030]	
Starting epoch number: 245 Learning rate: 0.0010000000000000002
Train: [245]	Time 85.216	Data 0.225	Loss 0.008	Prec@1 99.9740	Prec@5 100.0000	
Val: [245]	Time 5.106	Data 0.118	Loss 0.956	Prec@1 78.6100	Prec@5 94.5100	
Best Prec@1: [79.030]	
Starting epoch number: 246 Learning rate: 0.0010000000000000002
Train: [246]	Time 85.204	Data 0.251	Loss 0.008	Prec@1 99.9740	Prec@5 100.0000	
Val: [246]	Time 5.144	Data 0.135	Loss 0.952	Prec@1 78.8400	Prec@5 94.5100	
Best Prec@1: [79.030]	
Starting epoch number: 247 Learning rate: 0.0010000000000000002
Train: [247]	Time 85.222	Data 0.241	Loss 0.008	Prec@1 99.9740	Prec@5 100.0000	
Val: [247]	Time 5.078	Data 0.121	Loss 0.948	Prec@1 78.6500	Prec@5 94.6600	
Best Prec@1: [79.030]	
Starting epoch number: 248 Learning rate: 0.0010000000000000002
Train: [248]	Time 85.259	Data 0.237	Loss 0.008	Prec@1 99.9740	Prec@5 100.0000	
Val: [248]	Time 5.121	Data 0.119	Loss 0.948	Prec@1 78.7400	Prec@5 94.5200	
Best Prec@1: [79.030]	
Starting epoch number: 249 Learning rate: 0.0010000000000000002
Train: [249]	Time 85.279	Data 0.239	Loss 0.008	Prec@1 99.9700	Prec@5 100.0000	
Val: [249]	Time 5.088	Data 0.101	Loss 0.954	Prec@1 78.5600	Prec@5 94.6600	
Best Prec@1: [79.030]	
Starting epoch number: 250 Learning rate: 0.0010000000000000002
Train: [250]	Time 85.327	Data 0.237	Loss 0.008	Prec@1 99.9600	Prec@5 100.0000	
Val: [250]	Time 5.113	Data 0.112	Loss 0.955	Prec@1 78.7200	Prec@5 94.4500	
Best Prec@1: [79.030]	
Starting epoch number: 251 Learning rate: 0.0010000000000000002
Train: [251]	Time 85.250	Data 0.232	Loss 0.008	Prec@1 99.9660	Prec@5 100.0000	
Val: [251]	Time 5.123	Data 0.122	Loss 0.948	Prec@1 78.6000	Prec@5 94.5900	
Best Prec@1: [79.030]	
Starting epoch number: 252 Learning rate: 0.0010000000000000002
Train: [252]	Time 85.123	Data 0.229	Loss 0.008	Prec@1 99.9740	Prec@5 100.0000	
Val: [252]	Time 5.028	Data 0.118	Loss 0.952	Prec@1 78.4500	Prec@5 94.6300	
Best Prec@1: [79.030]	
Starting epoch number: 253 Learning rate: 0.0010000000000000002
Train: [253]	Time 85.157	Data 0.234	Loss 0.008	Prec@1 99.9680	Prec@5 100.0000	
Val: [253]	Time 5.120	Data 0.105	Loss 0.955	Prec@1 78.7100	Prec@5 94.3900	
Best Prec@1: [79.030]	
Starting epoch number: 254 Learning rate: 0.0010000000000000002
Train: [254]	Time 85.048	Data 0.218	Loss 0.008	Prec@1 99.9720	Prec@5 100.0000	
Val: [254]	Time 5.106	Data 0.115	Loss 0.950	Prec@1 78.7500	Prec@5 94.5700	
Best Prec@1: [79.030]	
Starting epoch number: 255 Learning rate: 0.0010000000000000002
Train: [255]	Time 85.113	Data 0.230	Loss 0.008	Prec@1 99.9760	Prec@5 100.0000	
Val: [255]	Time 5.134	Data 0.123	Loss 0.946	Prec@1 78.7500	Prec@5 94.5400	
Best Prec@1: [79.030]	
Starting epoch number: 256 Learning rate: 0.0010000000000000002
Train: [256]	Time 84.988	Data 0.240	Loss 0.008	Prec@1 99.9780	Prec@5 100.0000	
Val: [256]	Time 5.105	Data 0.119	Loss 0.947	Prec@1 78.9700	Prec@5 94.6100	
Best Prec@1: [79.030]	
Starting epoch number: 257 Learning rate: 0.0010000000000000002
Train: [257]	Time 84.971	Data 0.229	Loss 0.008	Prec@1 99.9760	Prec@5 100.0000	
Val: [257]	Time 5.067	Data 0.108	Loss 0.949	Prec@1 78.7600	Prec@5 94.6000	
Best Prec@1: [79.030]	
Starting epoch number: 258 Learning rate: 0.0010000000000000002
Train: [258]	Time 84.996	Data 0.224	Loss 0.008	Prec@1 99.9700	Prec@5 100.0000	
Val: [258]	Time 5.093	Data 0.141	Loss 0.958	Prec@1 78.6000	Prec@5 94.6900	
Best Prec@1: [79.030]	
Starting epoch number: 259 Learning rate: 0.0010000000000000002
Train: [259]	Time 85.003	Data 0.218	Loss 0.008	Prec@1 99.9620	Prec@5 100.0000	
Val: [259]	Time 5.080	Data 0.120	Loss 0.951	Prec@1 78.6400	Prec@5 94.5600	
Best Prec@1: [79.030]	
Starting epoch number: 260 Learning rate: 0.0010000000000000002
Train: [260]	Time 85.052	Data 0.239	Loss 0.008	Prec@1 99.9780	Prec@5 100.0000	
Val: [260]	Time 5.142	Data 0.165	Loss 0.957	Prec@1 78.8000	Prec@5 94.6000	
Best Prec@1: [79.030]	
Starting epoch number: 261 Learning rate: 0.0010000000000000002
Train: [261]	Time 85.039	Data 0.236	Loss 0.008	Prec@1 99.9700	Prec@5 100.0000	
Val: [261]	Time 5.090	Data 0.096	Loss 0.955	Prec@1 78.4700	Prec@5 94.4100	
Best Prec@1: [79.030]	
Starting epoch number: 262 Learning rate: 0.0010000000000000002
Train: [262]	Time 85.061	Data 0.242	Loss 0.008	Prec@1 99.9720	Prec@5 100.0000	
Val: [262]	Time 5.097	Data 0.125	Loss 0.957	Prec@1 78.7200	Prec@5 94.5600	
Best Prec@1: [79.030]	
Starting epoch number: 263 Learning rate: 0.0010000000000000002
Train: [263]	Time 85.110	Data 0.249	Loss 0.008	Prec@1 99.9700	Prec@5 100.0000	
Val: [263]	Time 5.100	Data 0.105	Loss 0.953	Prec@1 78.7200	Prec@5 94.5900	
Best Prec@1: [79.030]	
Starting epoch number: 264 Learning rate: 0.0010000000000000002
Train: [264]	Time 85.043	Data 0.218	Loss 0.008	Prec@1 99.9660	Prec@5 100.0000	
Val: [264]	Time 5.090	Data 0.098	Loss 0.952	Prec@1 78.6400	Prec@5 94.5800	
Best Prec@1: [79.030]	
Starting epoch number: 265 Learning rate: 0.0010000000000000002
Train: [265]	Time 85.093	Data 0.243	Loss 0.008	Prec@1 99.9780	Prec@5 100.0000	
Val: [265]	Time 5.143	Data 0.132	Loss 0.953	Prec@1 78.6300	Prec@5 94.6600	
Best Prec@1: [79.030]	
Starting epoch number: 266 Learning rate: 0.0010000000000000002
Train: [266]	Time 85.123	Data 0.243	Loss 0.008	Prec@1 99.9660	Prec@5 100.0000	
Val: [266]	Time 5.122	Data 0.121	Loss 0.953	Prec@1 78.7400	Prec@5 94.6300	
Best Prec@1: [79.030]	
Starting epoch number: 267 Learning rate: 0.0010000000000000002
Train: [267]	Time 85.141	Data 0.222	Loss 0.008	Prec@1 99.9700	Prec@5 100.0000	
Val: [267]	Time 5.095	Data 0.115	Loss 0.951	Prec@1 78.7100	Prec@5 94.5700	
Best Prec@1: [79.030]	
Starting epoch number: 268 Learning rate: 0.0010000000000000002
Train: [268]	Time 85.182	Data 0.242	Loss 0.008	Prec@1 99.9720	Prec@5 100.0000	
Val: [268]	Time 5.097	Data 0.099	Loss 0.950	Prec@1 78.7400	Prec@5 94.5400	
Best Prec@1: [79.030]	
Starting epoch number: 269 Learning rate: 0.0010000000000000002
Train: [269]	Time 85.195	Data 0.259	Loss 0.008	Prec@1 99.9680	Prec@5 100.0000	
Val: [269]	Time 5.077	Data 0.099	Loss 0.953	Prec@1 78.6400	Prec@5 94.5300	
Best Prec@1: [79.030]	
Starting epoch number: 270 Learning rate: 0.0010000000000000002
Train: [270]	Time 85.203	Data 0.245	Loss 0.008	Prec@1 99.9760	Prec@5 100.0000	
Val: [270]	Time 5.211	Data 0.134	Loss 0.949	Prec@1 78.7200	Prec@5 94.5400	
Best Prec@1: [79.030]	
Starting epoch number: 271 Learning rate: 0.0010000000000000002
Train: [271]	Time 85.195	Data 0.243	Loss 0.008	Prec@1 99.9740	Prec@5 100.0000	
Val: [271]	Time 5.118	Data 0.109	Loss 0.949	Prec@1 78.6300	Prec@5 94.4800	
Best Prec@1: [79.030]	
Starting epoch number: 272 Learning rate: 0.0010000000000000002
Train: [272]	Time 85.210	Data 0.224	Loss 0.008	Prec@1 99.9820	Prec@5 100.0000	
Val: [272]	Time 5.128	Data 0.127	Loss 0.946	Prec@1 78.7300	Prec@5 94.4900	
Best Prec@1: [79.030]	
Starting epoch number: 273 Learning rate: 0.0010000000000000002
Train: [273]	Time 85.175	Data 0.226	Loss 0.008	Prec@1 99.9780	Prec@5 100.0000	
Val: [273]	Time 5.102	Data 0.108	Loss 0.950	Prec@1 78.6600	Prec@5 94.5600	
Best Prec@1: [79.030]	
Starting epoch number: 274 Learning rate: 0.0010000000000000002
Train: [274]	Time 85.263	Data 0.246	Loss 0.008	Prec@1 99.9740	Prec@5 100.0000	
Val: [274]	Time 5.116	Data 0.113	Loss 0.949	Prec@1 78.5500	Prec@5 94.3900	
Best Prec@1: [79.030]	
Starting epoch number: 275 Learning rate: 0.0010000000000000002
Train: [275]	Time 85.230	Data 0.234	Loss 0.008	Prec@1 99.9820	Prec@5 100.0000	
Val: [275]	Time 5.107	Data 0.099	Loss 0.953	Prec@1 78.5100	Prec@5 94.5100	
Best Prec@1: [79.030]	
Starting epoch number: 276 Learning rate: 0.0010000000000000002
Train: [276]	Time 85.248	Data 0.225	Loss 0.008	Prec@1 99.9640	Prec@5 100.0000	
Val: [276]	Time 5.121	Data 0.122	Loss 0.958	Prec@1 78.3700	Prec@5 94.4800	
Best Prec@1: [79.030]	
Starting epoch number: 277 Learning rate: 0.0010000000000000002
Train: [277]	Time 85.266	Data 0.244	Loss 0.008	Prec@1 99.9800	Prec@5 100.0000	
Val: [277]	Time 5.096	Data 0.095	Loss 0.951	Prec@1 78.8200	Prec@5 94.5800	
Best Prec@1: [79.030]	
Starting epoch number: 278 Learning rate: 0.0010000000000000002
Train: [278]	Time 85.257	Data 0.234	Loss 0.008	Prec@1 99.9700	Prec@5 100.0000	
Val: [278]	Time 5.095	Data 0.099	Loss 0.951	Prec@1 78.6600	Prec@5 94.5700	
Best Prec@1: [79.030]	
Starting epoch number: 279 Learning rate: 0.0010000000000000002
Train: [279]	Time 85.269	Data 0.248	Loss 0.008	Prec@1 99.9700	Prec@5 100.0000	
Val: [279]	Time 5.107	Data 0.111	Loss 0.952	Prec@1 78.6200	Prec@5 94.6400	
Best Prec@1: [79.030]	
Starting epoch number: 280 Learning rate: 0.0010000000000000002
Train: [280]	Time 85.245	Data 0.245	Loss 0.008	Prec@1 99.9760	Prec@5 100.0000	
Val: [280]	Time 5.101	Data 0.129	Loss 0.954	Prec@1 78.7600	Prec@5 94.4200	
Best Prec@1: [79.030]	
Starting epoch number: 281 Learning rate: 0.0010000000000000002
Train: [281]	Time 85.297	Data 0.237	Loss 0.008	Prec@1 99.9680	Prec@5 100.0000	
Val: [281]	Time 5.117	Data 0.100	Loss 0.952	Prec@1 78.7500	Prec@5 94.5400	
Best Prec@1: [79.030]	
Starting epoch number: 282 Learning rate: 0.0010000000000000002
Train: [282]	Time 85.276	Data 0.233	Loss 0.008	Prec@1 99.9760	Prec@5 100.0000	
Val: [282]	Time 5.111	Data 0.117	Loss 0.955	Prec@1 78.5700	Prec@5 94.5100	
Best Prec@1: [79.030]	
Starting epoch number: 283 Learning rate: 0.0010000000000000002
Train: [283]	Time 85.322	Data 0.233	Loss 0.008	Prec@1 99.9580	Prec@5 100.0000	
Val: [283]	Time 5.091	Data 0.100	Loss 0.946	Prec@1 78.7500	Prec@5 94.6300	
Best Prec@1: [79.030]	
Starting epoch number: 284 Learning rate: 0.0010000000000000002
Train: [284]	Time 85.303	Data 0.236	Loss 0.008	Prec@1 99.9760	Prec@5 100.0000	
Val: [284]	Time 5.125	Data 0.115	Loss 0.948	Prec@1 78.7400	Prec@5 94.4000	
Best Prec@1: [79.030]	
Starting epoch number: 285 Learning rate: 0.0010000000000000002
Train: [285]	Time 85.230	Data 0.229	Loss 0.008	Prec@1 99.9700	Prec@5 100.0000	
Val: [285]	Time 5.096	Data 0.104	Loss 0.948	Prec@1 78.6900	Prec@5 94.4900	
Best Prec@1: [79.030]	
Starting epoch number: 286 Learning rate: 0.0010000000000000002
Train: [286]	Time 85.169	Data 0.240	Loss 0.008	Prec@1 99.9760	Prec@5 100.0000	
Val: [286]	Time 5.106	Data 0.122	Loss 0.962	Prec@1 78.5100	Prec@5 94.5400	
Best Prec@1: [79.030]	
Starting epoch number: 287 Learning rate: 0.0010000000000000002
Train: [287]	Time 85.150	Data 0.241	Loss 0.008	Prec@1 99.9760	Prec@5 100.0000	
Val: [287]	Time 5.119	Data 0.123	Loss 0.953	Prec@1 78.6800	Prec@5 94.5000	
Best Prec@1: [79.030]	
Starting epoch number: 288 Learning rate: 0.0010000000000000002
Train: [288]	Time 85.175	Data 0.236	Loss 0.008	Prec@1 99.9660	Prec@5 100.0000	
Val: [288]	Time 5.080	Data 0.095	Loss 0.953	Prec@1 78.5800	Prec@5 94.5300	
Best Prec@1: [79.030]	
Starting epoch number: 289 Learning rate: 0.0010000000000000002
Train: [289]	Time 85.142	Data 0.221	Loss 0.008	Prec@1 99.9700	Prec@5 100.0000	
Val: [289]	Time 5.116	Data 0.117	Loss 0.958	Prec@1 78.7000	Prec@5 94.4500	
Best Prec@1: [79.030]	
Starting epoch number: 290 Learning rate: 0.0010000000000000002
Train: [290]	Time 85.037	Data 0.256	Loss 0.008	Prec@1 99.9640	Prec@5 100.0000	
Val: [290]	Time 5.112	Data 0.103	Loss 0.953	Prec@1 78.6100	Prec@5 94.4700	
Best Prec@1: [79.030]	
Starting epoch number: 291 Learning rate: 0.0010000000000000002
Train: [291]	Time 84.949	Data 0.250	Loss 0.008	Prec@1 99.9720	Prec@5 100.0000	
Val: [291]	Time 5.063	Data 0.101	Loss 0.949	Prec@1 78.7700	Prec@5 94.4800	
Best Prec@1: [79.030]	
Starting epoch number: 292 Learning rate: 0.0010000000000000002
Train: [292]	Time 84.986	Data 0.241	Loss 0.008	Prec@1 99.9760	Prec@5 100.0000	
Val: [292]	Time 5.084	Data 0.113	Loss 0.961	Prec@1 78.7900	Prec@5 94.5400	
Best Prec@1: [79.030]	
Starting epoch number: 293 Learning rate: 0.0010000000000000002
Train: [293]	Time 84.990	Data 0.245	Loss 0.008	Prec@1 99.9860	Prec@5 100.0000	
Val: [293]	Time 5.112	Data 0.095	Loss 0.949	Prec@1 78.7700	Prec@5 94.4700	
Best Prec@1: [79.030]	
Starting epoch number: 294 Learning rate: 0.0010000000000000002
Train: [294]	Time 84.988	Data 0.234	Loss 0.008	Prec@1 99.9720	Prec@5 100.0000	
Val: [294]	Time 5.086	Data 0.097	Loss 0.955	Prec@1 78.6000	Prec@5 94.6300	
Best Prec@1: [79.030]	
Starting epoch number: 295 Learning rate: 0.0010000000000000002
Train: [295]	Time 85.043	Data 0.241	Loss 0.008	Prec@1 99.9660	Prec@5 100.0000	
Val: [295]	Time 5.088	Data 0.113	Loss 0.949	Prec@1 78.6900	Prec@5 94.4800	
Best Prec@1: [79.030]	
Starting epoch number: 296 Learning rate: 0.0010000000000000002
Train: [296]	Time 85.061	Data 0.238	Loss 0.008	Prec@1 99.9760	Prec@5 100.0000	
Val: [296]	Time 5.111	Data 0.112	Loss 0.954	Prec@1 78.8400	Prec@5 94.4500	
Best Prec@1: [79.030]	
Starting epoch number: 297 Learning rate: 0.0010000000000000002
