Namespace(acc_type='class', augment=True, batch_size=64, bottleneck=True, cachemode=True, cardinality=8, criterion='crossentropy', cuda=True, data_dir='../data', dataset='cifar100', decayinterval=None, decaylevel=None, droprate=0, epochs=300, evaluate=False, expandConfig=None, expandSize=2, from_modelzoo=False, growth=100, layers=40, learningratescheduler='cifarschedular', logdir='../logs/densenetbc_cifar100_40_100', lr=0.1, manualSeed=123, maxlr=0.1, minlr=0.0005, model_def='densenet_cifar', momentum=0.9, name='densenetbc_cifar100_40_100', nclasses=100, nesterov=True, ngpus=1, optimType='sgd', pretrained=False, pretrained_file='', printfreq=200, reduce=0.5, resume='', start_epoch=0, store='', tenCrop=False, tensorboard=True, testOnly=False, verbose=False, weightDecay=0.0001, weight_init=False, widen_factor=4, workers=2)
DenseNet3 (
  (conv1): Conv2d(3, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): DenseBlock (
    (layer): Sequential (
      (0): BottleneckBlock (
        (bn1): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(200, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(400, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock (
        (bn1): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(300, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(400, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock (
        (bn1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(400, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock (
        (bn1): BatchNorm2d(500, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(500, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(400, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock (
        (bn1): BatchNorm2d(600, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(400, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock (
        (bn1): BatchNorm2d(700, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(700, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(400, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans1): TransitionBlock (
    (bn1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)
    (relu): ReLU (inplace)
    (conv1): Conv2d(800, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block2): DenseBlock (
    (layer): Sequential (
      (0): BottleneckBlock (
        (bn1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(400, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock (
        (bn1): BatchNorm2d(500, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(500, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(400, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock (
        (bn1): BatchNorm2d(600, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(400, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock (
        (bn1): BatchNorm2d(700, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(700, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(400, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock (
        (bn1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(800, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(400, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock (
        (bn1): BatchNorm2d(900, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(900, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(400, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans2): TransitionBlock (
    (bn1): BatchNorm2d(1000, eps=1e-05, momentum=0.1, affine=True)
    (relu): ReLU (inplace)
    (conv1): Conv2d(1000, 500, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block3): DenseBlock (
    (layer): Sequential (
      (0): BottleneckBlock (
        (bn1): BatchNorm2d(500, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(500, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(400, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock (
        (bn1): BatchNorm2d(600, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(400, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock (
        (bn1): BatchNorm2d(700, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(700, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(400, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock (
        (bn1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(800, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(400, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock (
        (bn1): BatchNorm2d(900, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(900, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(400, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock (
        (bn1): BatchNorm2d(1000, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(1000, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(400, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (bn1): BatchNorm2d(1100, eps=1e-05, momentum=0.1, affine=True)
  (relu): ReLU (inplace)
  (fc): Linear (1100 -> 100)
)
Files already downloaded and verified
Starting epoch number: 0 Learning rate: 0.1
Train: [0]	Time 239.863	Data 0.372	Loss 3.768	Prec@1 12.2820	Prec@5 35.1240	
Val: [0]	Time 15.626	Data 0.106	Loss 3.503	Prec@1 17.9100	Prec@5 45.2400	
Best Prec@1: [17.910]	
Starting epoch number: 1 Learning rate: 0.1
Train: [1]	Time 241.821	Data 0.262	Loss 2.833	Prec@1 27.8220	Prec@5 59.4420	
Val: [1]	Time 15.562	Data 0.108	Loss 2.726	Prec@1 31.2600	Prec@5 65.9600	
Best Prec@1: [31.260]	
Starting epoch number: 2 Learning rate: 0.1
Train: [2]	Time 241.762	Data 0.272	Loss 2.185	Prec@1 41.3160	Prec@5 73.9680	
Val: [2]	Time 15.576	Data 0.099	Loss 2.101	Prec@1 43.9000	Prec@5 76.1400	
Best Prec@1: [43.900]	
Starting epoch number: 3 Learning rate: 0.1
Train: [3]	Time 242.531	Data 0.272	Loss 1.826	Prec@1 49.4540	Prec@5 80.9620	
Val: [3]	Time 15.623	Data 0.103	Loss 2.040	Prec@1 46.7500	Prec@5 78.2600	
Best Prec@1: [46.750]	
Starting epoch number: 4 Learning rate: 0.1
Train: [4]	Time 242.189	Data 0.266	Loss 1.594	Prec@1 55.1980	Prec@5 85.0800	
Val: [4]	Time 15.645	Data 0.189	Loss 1.624	Prec@1 54.5300	Prec@5 84.1500	
Best Prec@1: [54.530]	
Starting epoch number: 5 Learning rate: 0.1
Train: [5]	Time 242.273	Data 0.265	Loss 1.418	Prec@1 59.4540	Prec@5 87.8440	
Val: [5]	Time 15.529	Data 0.107	Loss 1.532	Prec@1 57.5300	Prec@5 85.7100	
Best Prec@1: [57.530]	
Starting epoch number: 6 Learning rate: 0.1
Train: [6]	Time 242.847	Data 0.243	Loss 1.298	Prec@1 62.4540	Prec@5 89.4220	
Val: [6]	Time 15.598	Data 0.116	Loss 1.510	Prec@1 58.2900	Prec@5 86.5900	
Best Prec@1: [58.290]	
Starting epoch number: 7 Learning rate: 0.1
Train: [7]	Time 241.296	Data 0.245	Loss 1.188	Prec@1 65.5960	Prec@5 90.8600	
Val: [7]	Time 15.806	Data 0.185	Loss 1.565	Prec@1 58.7800	Prec@5 85.9900	
Best Prec@1: [58.780]	
Starting epoch number: 8 Learning rate: 0.1
Train: [8]	Time 242.633	Data 0.277	Loss 1.112	Prec@1 67.2840	Prec@5 92.0520	
Val: [8]	Time 15.584	Data 0.100	Loss 1.412	Prec@1 61.3900	Prec@5 88.2700	
Best Prec@1: [61.390]	
Starting epoch number: 9 Learning rate: 0.1
Train: [9]	Time 241.748	Data 0.251	Loss 1.041	Prec@1 69.3840	Prec@5 92.8640	
Val: [9]	Time 15.633	Data 0.122	Loss 1.476	Prec@1 60.0200	Prec@5 87.5500	
Best Prec@1: [61.390]	
Starting epoch number: 10 Learning rate: 0.1
Train: [10]	Time 241.777	Data 0.267	Loss 0.977	Prec@1 71.1160	Prec@5 93.8160	
Val: [10]	Time 15.685	Data 0.103	Loss 1.432	Prec@1 61.6700	Prec@5 88.3900	
Best Prec@1: [61.670]	
Starting epoch number: 11 Learning rate: 0.1
Train: [11]	Time 242.128	Data 0.268	Loss 0.931	Prec@1 72.2780	Prec@5 94.1620	
Val: [11]	Time 15.462	Data 0.118	Loss 1.319	Prec@1 64.2300	Prec@5 88.7300	
Best Prec@1: [64.230]	
Starting epoch number: 12 Learning rate: 0.1
Train: [12]	Time 241.605	Data 0.255	Loss 0.886	Prec@1 73.5140	Prec@5 94.7160	
Val: [12]	Time 15.587	Data 0.116	Loss 1.310	Prec@1 64.2700	Prec@5 89.3500	
Best Prec@1: [64.270]	
Starting epoch number: 13 Learning rate: 0.1
Train: [13]	Time 242.687	Data 0.267	Loss 0.842	Prec@1 74.7080	Prec@5 95.0820	
Val: [13]	Time 15.763	Data 0.110	Loss 1.384	Prec@1 63.4100	Prec@5 89.1200	
Best Prec@1: [64.270]	
Starting epoch number: 14 Learning rate: 0.1
Train: [14]	Time 242.601	Data 0.274	Loss 0.805	Prec@1 75.5180	Prec@5 95.6340	
Val: [14]	Time 15.610	Data 0.097	Loss 1.353	Prec@1 64.6300	Prec@5 89.5300	
Best Prec@1: [64.630]	
Starting epoch number: 15 Learning rate: 0.1
Train: [15]	Time 241.779	Data 0.259	Loss 0.782	Prec@1 76.2260	Prec@5 95.8380	
Val: [15]	Time 15.604	Data 0.099	Loss 1.338	Prec@1 64.4300	Prec@5 89.8900	
Best Prec@1: [64.630]	
Starting epoch number: 16 Learning rate: 0.1
Train: [16]	Time 241.569	Data 0.270	Loss 0.748	Prec@1 77.1300	Prec@5 96.3160	
Val: [16]	Time 15.456	Data 0.117	Loss 1.457	Prec@1 63.6900	Prec@5 88.9900	
Best Prec@1: [64.630]	
Starting epoch number: 17 Learning rate: 0.1
Train: [17]	Time 241.703	Data 0.264	Loss 0.719	Prec@1 77.9020	Prec@5 96.5400	
Val: [17]	Time 15.612	Data 0.107	Loss 1.377	Prec@1 64.7000	Prec@5 89.6900	
Best Prec@1: [64.700]	
Starting epoch number: 18 Learning rate: 0.1
Train: [18]	Time 242.420	Data 0.281	Loss 0.705	Prec@1 78.2160	Prec@5 96.7620	
Val: [18]	Time 15.569	Data 0.102	Loss 1.364	Prec@1 65.6400	Prec@5 90.0200	
Best Prec@1: [65.640]	
Starting epoch number: 19 Learning rate: 0.1
Train: [19]	Time 242.024	Data 0.286	Loss 0.680	Prec@1 79.0040	Prec@5 96.9380	
Val: [19]	Time 15.564	Data 0.103	Loss 1.426	Prec@1 64.7100	Prec@5 89.8400	
Best Prec@1: [65.640]	
Starting epoch number: 20 Learning rate: 0.1
Train: [20]	Time 241.366	Data 0.254	Loss 0.656	Prec@1 79.7360	Prec@5 97.1020	
Val: [20]	Time 15.440	Data 0.112	Loss 1.386	Prec@1 64.6400	Prec@5 89.2400	
Best Prec@1: [65.640]	
Starting epoch number: 21 Learning rate: 0.1
Train: [21]	Time 241.625	Data 0.261	Loss 0.641	Prec@1 80.2380	Prec@5 97.1580	
Val: [21]	Time 15.425	Data 0.113	Loss 1.450	Prec@1 63.9000	Prec@5 89.6000	
Best Prec@1: [65.640]	
Starting epoch number: 22 Learning rate: 0.1
Train: [22]	Time 242.105	Data 0.273	Loss 0.622	Prec@1 80.6500	Prec@5 97.3920	
Val: [22]	Time 15.388	Data 0.107	Loss 1.517	Prec@1 64.5600	Prec@5 88.5300	
Best Prec@1: [65.640]	
Starting epoch number: 23 Learning rate: 0.1
Train: [23]	Time 241.351	Data 0.252	Loss 0.610	Prec@1 80.7900	Prec@5 97.4980	
Val: [23]	Time 15.535	Data 0.094	Loss 1.420	Prec@1 64.8800	Prec@5 89.5800	
Best Prec@1: [65.640]	
Starting epoch number: 24 Learning rate: 0.1
Train: [24]	Time 242.837	Data 0.289	Loss 0.589	Prec@1 81.7880	Prec@5 97.6380	
Val: [24]	Time 15.386	Data 0.118	Loss 1.324	Prec@1 67.2200	Prec@5 90.7300	
Best Prec@1: [67.220]	
Starting epoch number: 25 Learning rate: 0.1
Train: [25]	Time 241.766	Data 0.247	Loss 0.575	Prec@1 82.2700	Prec@5 97.7200	
Val: [25]	Time 15.557	Data 0.102	Loss 1.364	Prec@1 66.4200	Prec@5 89.9900	
Best Prec@1: [67.220]	
Starting epoch number: 26 Learning rate: 0.1
Train: [26]	Time 241.084	Data 0.255	Loss 0.570	Prec@1 82.1600	Prec@5 97.9320	
Val: [26]	Time 15.627	Data 0.099	Loss 1.367	Prec@1 65.5100	Prec@5 89.7300	
Best Prec@1: [67.220]	
Starting epoch number: 27 Learning rate: 0.1
Train: [27]	Time 241.924	Data 0.275	Loss 0.555	Prec@1 82.5460	Prec@5 97.9380	
Val: [27]	Time 15.578	Data 0.105	Loss 1.412	Prec@1 66.0900	Prec@5 90.4800	
Best Prec@1: [67.220]	
Starting epoch number: 28 Learning rate: 0.1
Train: [28]	Time 241.512	Data 0.274	Loss 0.548	Prec@1 82.8520	Prec@5 98.0600	
Val: [28]	Time 15.591	Data 0.102	Loss 1.325	Prec@1 67.5200	Prec@5 90.3300	
Best Prec@1: [67.520]	
Starting epoch number: 29 Learning rate: 0.1
Train: [29]	Time 241.179	Data 0.248	Loss 0.525	Prec@1 83.5160	Prec@5 98.2300	
Val: [29]	Time 15.561	Data 0.119	Loss 1.388	Prec@1 66.2200	Prec@5 90.0700	
Best Prec@1: [67.520]	
Starting epoch number: 30 Learning rate: 0.1
Train: [30]	Time 242.122	Data 0.259	Loss 0.516	Prec@1 83.6580	Prec@5 98.2720	
Val: [30]	Time 15.744	Data 0.106	Loss 1.478	Prec@1 64.8900	Prec@5 89.3000	
Best Prec@1: [67.520]	
Starting epoch number: 31 Learning rate: 0.1
Train: [31]	Time 242.310	Data 0.252	Loss 0.509	Prec@1 83.9300	Prec@5 98.3400	
Val: [31]	Time 15.499	Data 0.103	Loss 1.417	Prec@1 66.5600	Prec@5 90.3000	
Best Prec@1: [67.520]	
Starting epoch number: 32 Learning rate: 0.1
Train: [32]	Time 241.568	Data 0.279	Loss 0.505	Prec@1 84.1660	Prec@5 98.3340	
Val: [32]	Time 15.675	Data 0.097	Loss 1.654	Prec@1 62.3500	Prec@5 88.3100	
Best Prec@1: [67.520]	
Starting epoch number: 33 Learning rate: 0.1
Train: [33]	Time 241.015	Data 0.266	Loss 0.498	Prec@1 84.2060	Prec@5 98.4280	
Val: [33]	Time 15.568	Data 0.115	Loss 1.416	Prec@1 66.2200	Prec@5 89.6600	
Best Prec@1: [67.520]	
Starting epoch number: 34 Learning rate: 0.1
Train: [34]	Time 241.908	Data 0.268	Loss 0.482	Prec@1 84.6140	Prec@5 98.5420	
Val: [34]	Time 15.556	Data 0.115	Loss 1.526	Prec@1 65.2000	Prec@5 89.2500	
Best Prec@1: [67.520]	
Starting epoch number: 35 Learning rate: 0.1
Train: [35]	Time 242.544	Data 0.262	Loss 0.483	Prec@1 84.6420	Prec@5 98.5300	
Val: [35]	Time 15.595	Data 0.121	Loss 1.558	Prec@1 64.9600	Prec@5 89.0900	
Best Prec@1: [67.520]	
Starting epoch number: 36 Learning rate: 0.1
Train: [36]	Time 241.915	Data 0.265	Loss 0.464	Prec@1 85.3260	Prec@5 98.6200	
Val: [36]	Time 15.607	Data 0.119	Loss 1.503	Prec@1 65.7500	Prec@5 89.6600	
Best Prec@1: [67.520]	
Starting epoch number: 37 Learning rate: 0.1
Train: [37]	Time 242.007	Data 0.283	Loss 0.475	Prec@1 84.9060	Prec@5 98.6140	
Val: [37]	Time 15.598	Data 0.099	Loss 1.667	Prec@1 62.9000	Prec@5 88.3100	
Best Prec@1: [67.520]	
Starting epoch number: 38 Learning rate: 0.1
Train: [38]	Time 242.472	Data 0.289	Loss 0.460	Prec@1 85.3280	Prec@5 98.6880	
Val: [38]	Time 15.541	Data 0.100	Loss 1.481	Prec@1 66.5500	Prec@5 89.9100	
Best Prec@1: [67.520]	
Starting epoch number: 39 Learning rate: 0.1
Train: [39]	Time 242.221	Data 0.252	Loss 0.457	Prec@1 85.4560	Prec@5 98.6960	
Val: [39]	Time 15.581	Data 0.093	Loss 1.565	Prec@1 64.9900	Prec@5 89.5600	
Best Prec@1: [67.520]	
Starting epoch number: 40 Learning rate: 0.1
Train: [40]	Time 242.021	Data 0.258	Loss 0.449	Prec@1 85.9020	Prec@5 98.7360	
Val: [40]	Time 15.511	Data 0.098	Loss 1.406	Prec@1 66.6500	Prec@5 90.5100	
Best Prec@1: [67.520]	
Starting epoch number: 41 Learning rate: 0.1
Train: [41]	Time 242.314	Data 0.246	Loss 0.445	Prec@1 85.7920	Prec@5 98.7440	
Val: [41]	Time 15.777	Data 0.125	Loss 1.453	Prec@1 65.6300	Prec@5 89.7100	
Best Prec@1: [67.520]	
Starting epoch number: 42 Learning rate: 0.1
Train: [42]	Time 242.209	Data 0.244	Loss 0.440	Prec@1 86.0180	Prec@5 98.7920	
Val: [42]	Time 15.624	Data 0.100	Loss 1.403	Prec@1 67.1600	Prec@5 91.1700	
Best Prec@1: [67.520]	
Starting epoch number: 43 Learning rate: 0.1
Train: [43]	Time 241.182	Data 0.264	Loss 0.434	Prec@1 86.3060	Prec@5 98.8580	
Val: [43]	Time 15.583	Data 0.103	Loss 1.529	Prec@1 65.8500	Prec@5 89.2200	
Best Prec@1: [67.520]	
Starting epoch number: 44 Learning rate: 0.1
Train: [44]	Time 242.030	Data 0.273	Loss 0.434	Prec@1 86.2360	Prec@5 98.8560	
Val: [44]	Time 15.515	Data 0.121	Loss 1.371	Prec@1 67.9300	Prec@5 90.9900	
Best Prec@1: [67.930]	
Starting epoch number: 45 Learning rate: 0.1
Train: [45]	Time 242.045	Data 0.261	Loss 0.422	Prec@1 86.5260	Prec@5 98.8840	
Val: [45]	Time 15.506	Data 0.098	Loss 1.412	Prec@1 66.7500	Prec@5 89.3000	
Best Prec@1: [67.930]	
Starting epoch number: 46 Learning rate: 0.1
Train: [46]	Time 242.322	Data 0.262	Loss 0.436	Prec@1 86.1260	Prec@5 98.8760	
Val: [46]	Time 15.558	Data 0.106	Loss 1.488	Prec@1 66.2100	Prec@5 90.0700	
Best Prec@1: [67.930]	
Starting epoch number: 47 Learning rate: 0.1
Train: [47]	Time 242.941	Data 0.267	Loss 0.414	Prec@1 86.8420	Prec@5 98.9460	
Val: [47]	Time 15.751	Data 0.122	Loss 1.451	Prec@1 67.0800	Prec@5 90.0000	
Best Prec@1: [67.930]	
Starting epoch number: 48 Learning rate: 0.1
Train: [48]	Time 242.438	Data 0.285	Loss 0.421	Prec@1 86.5080	Prec@5 98.9320	
Val: [48]	Time 15.683	Data 0.099	Loss 1.385	Prec@1 68.0400	Prec@5 90.8800	
Best Prec@1: [68.040]	
Starting epoch number: 49 Learning rate: 0.1
Train: [49]	Time 242.031	Data 0.278	Loss 0.409	Prec@1 86.9520	Prec@5 98.9580	
Val: [49]	Time 15.705	Data 0.120	Loss 1.388	Prec@1 67.7800	Prec@5 90.5600	
Best Prec@1: [68.040]	
Starting epoch number: 50 Learning rate: 0.1
Train: [50]	Time 241.500	Data 0.267	Loss 0.415	Prec@1 86.9140	Prec@5 98.9780	
Val: [50]	Time 15.570	Data 0.116	Loss 1.437	Prec@1 67.1800	Prec@5 90.3900	
Best Prec@1: [68.040]	
Starting epoch number: 51 Learning rate: 0.1
Train: [51]	Time 242.044	Data 0.269	Loss 0.401	Prec@1 87.2300	Prec@5 98.9840	
Val: [51]	Time 15.549	Data 0.100	Loss 1.465	Prec@1 66.8200	Prec@5 90.0600	
Best Prec@1: [68.040]	
Starting epoch number: 52 Learning rate: 0.1
Train: [52]	Time 241.678	Data 0.267	Loss 0.405	Prec@1 87.0620	Prec@5 99.0580	
Val: [52]	Time 15.588	Data 0.109	Loss 1.579	Prec@1 65.1800	Prec@5 88.5000	
Best Prec@1: [68.040]	
Starting epoch number: 53 Learning rate: 0.1
Train: [53]	Time 242.783	Data 0.261	Loss 0.414	Prec@1 86.7120	Prec@5 98.9420	
Val: [53]	Time 15.503	Data 0.105	Loss 1.454	Prec@1 67.5600	Prec@5 90.1900	
Best Prec@1: [68.040]	
Starting epoch number: 54 Learning rate: 0.1
Train: [54]	Time 241.157	Data 0.254	Loss 0.395	Prec@1 87.3400	Prec@5 99.0500	
Val: [54]	Time 15.544	Data 0.115	Loss 1.505	Prec@1 65.6400	Prec@5 89.7000	
Best Prec@1: [68.040]	
Starting epoch number: 55 Learning rate: 0.1
Train: [55]	Time 241.441	Data 0.267	Loss 0.393	Prec@1 87.4300	Prec@5 99.0480	
Val: [55]	Time 15.738	Data 0.111	Loss 1.531	Prec@1 66.1400	Prec@5 89.4600	
Best Prec@1: [68.040]	
Starting epoch number: 56 Learning rate: 0.1
Train: [56]	Time 240.385	Data 0.280	Loss 0.399	Prec@1 87.3040	Prec@5 99.0320	
Val: [56]	Time 15.771	Data 0.105	Loss 1.371	Prec@1 68.0600	Prec@5 91.1100	
Best Prec@1: [68.060]	
Starting epoch number: 57 Learning rate: 0.1
Train: [57]	Time 241.937	Data 0.278	Loss 0.394	Prec@1 87.4480	Prec@5 99.0240	
Val: [57]	Time 15.763	Data 0.112	Loss 1.522	Prec@1 66.1900	Prec@5 90.3000	
Best Prec@1: [68.060]	
Starting epoch number: 58 Learning rate: 0.1
Train: [58]	Time 242.169	Data 0.259	Loss 0.370	Prec@1 88.3620	Prec@5 99.1200	
Val: [58]	Time 15.628	Data 0.110	Loss 1.512	Prec@1 67.2800	Prec@5 89.6200	
Best Prec@1: [68.060]	
Starting epoch number: 59 Learning rate: 0.1
Train: [59]	Time 241.837	Data 0.261	Loss 0.396	Prec@1 87.3420	Prec@5 99.0960	
Val: [59]	Time 15.660	Data 0.105	Loss 1.663	Prec@1 64.3300	Prec@5 89.4000	
Best Prec@1: [68.060]	
Starting epoch number: 60 Learning rate: 0.1
Train: [60]	Time 241.738	Data 0.254	Loss 0.386	Prec@1 87.6720	Prec@5 99.0600	
Val: [60]	Time 15.646	Data 0.097	Loss 1.605	Prec@1 65.2400	Prec@5 89.3600	
Best Prec@1: [68.060]	
Starting epoch number: 61 Learning rate: 0.1
Train: [61]	Time 242.099	Data 0.250	Loss 0.382	Prec@1 87.7080	Prec@5 99.1220	
Val: [61]	Time 15.611	Data 0.107	Loss 1.437	Prec@1 67.0700	Prec@5 90.5900	
Best Prec@1: [68.060]	
Starting epoch number: 62 Learning rate: 0.1
Train: [62]	Time 242.275	Data 0.254	Loss 0.398	Prec@1 87.3460	Prec@5 98.9940	
Val: [62]	Time 15.642	Data 0.112	Loss 1.482	Prec@1 66.9100	Prec@5 89.3300	
Best Prec@1: [68.060]	
Starting epoch number: 63 Learning rate: 0.1
Train: [63]	Time 241.677	Data 0.257	Loss 0.382	Prec@1 87.8140	Prec@5 99.1420	
Val: [63]	Time 15.761	Data 0.127	Loss 1.511	Prec@1 65.7700	Prec@5 89.7100	
Best Prec@1: [68.060]	
Starting epoch number: 64 Learning rate: 0.1
Train: [64]	Time 240.898	Data 0.248	Loss 0.376	Prec@1 87.9520	Prec@5 99.1540	
Val: [64]	Time 15.492	Data 0.118	Loss 1.399	Prec@1 67.8600	Prec@5 90.3300	
Best Prec@1: [68.060]	
Starting epoch number: 65 Learning rate: 0.1
Train: [65]	Time 241.549	Data 0.275	Loss 0.385	Prec@1 87.8120	Prec@5 99.0820	
Val: [65]	Time 15.632	Data 0.100	Loss 1.516	Prec@1 66.4600	Prec@5 90.6900	
Best Prec@1: [68.060]	
Starting epoch number: 66 Learning rate: 0.1
Train: [66]	Time 242.035	Data 0.269	Loss 0.374	Prec@1 88.0660	Prec@5 99.1500	
Val: [66]	Time 15.726	Data 0.094	Loss 1.591	Prec@1 65.1800	Prec@5 89.3300	
Best Prec@1: [68.060]	
Starting epoch number: 67 Learning rate: 0.1
Train: [67]	Time 242.277	Data 0.274	Loss 0.371	Prec@1 88.2540	Prec@5 99.1780	
Val: [67]	Time 15.636	Data 0.101	Loss 1.362	Prec@1 68.5600	Prec@5 91.2800	
Best Prec@1: [68.560]	
Starting epoch number: 68 Learning rate: 0.1
Train: [68]	Time 242.821	Data 0.251	Loss 0.375	Prec@1 87.9980	Prec@5 99.1740	
Val: [68]	Time 15.584	Data 0.126	Loss 1.400	Prec@1 67.3400	Prec@5 89.7700	
Best Prec@1: [68.560]	
Starting epoch number: 69 Learning rate: 0.1
Train: [69]	Time 241.472	Data 0.265	Loss 0.369	Prec@1 88.2340	Prec@5 99.1620	
Val: [69]	Time 15.638	Data 0.098	Loss 1.717	Prec@1 63.5700	Prec@5 88.5100	
Best Prec@1: [68.560]	
Starting epoch number: 70 Learning rate: 0.1
Train: [70]	Time 241.017	Data 0.286	Loss 0.361	Prec@1 88.5120	Prec@5 99.2140	
Val: [70]	Time 15.782	Data 0.123	Loss 1.469	Prec@1 67.9300	Prec@5 90.5700	
Best Prec@1: [68.560]	
Starting epoch number: 71 Learning rate: 0.1
Train: [71]	Time 241.637	Data 0.262	Loss 0.370	Prec@1 88.2400	Prec@5 99.2100	
Val: [71]	Time 15.644	Data 0.105	Loss 1.468	Prec@1 67.1000	Prec@5 90.1000	
Best Prec@1: [68.560]	
Starting epoch number: 72 Learning rate: 0.1
Train: [72]	Time 242.108	Data 0.267	Loss 0.368	Prec@1 88.3160	Prec@5 99.1620	
Val: [72]	Time 15.437	Data 0.093	Loss 1.417	Prec@1 68.3900	Prec@5 91.0600	
Best Prec@1: [68.560]	
Starting epoch number: 73 Learning rate: 0.1
Train: [73]	Time 241.274	Data 0.260	Loss 0.363	Prec@1 88.4500	Prec@5 99.1280	
Val: [73]	Time 15.733	Data 0.105	Loss 1.498	Prec@1 67.5000	Prec@5 90.4100	
Best Prec@1: [68.560]	
Starting epoch number: 74 Learning rate: 0.1
Train: [74]	Time 241.567	Data 0.273	Loss 0.365	Prec@1 88.3220	Prec@5 99.1920	
Val: [74]	Time 15.568	Data 0.100	Loss 1.392	Prec@1 68.6100	Prec@5 90.8300	
Best Prec@1: [68.610]	
Starting epoch number: 75 Learning rate: 0.1
Train: [75]	Time 241.019	Data 0.268	Loss 0.361	Prec@1 88.5920	Prec@5 99.1760	
Val: [75]	Time 15.705	Data 0.102	Loss 1.352	Prec@1 69.4200	Prec@5 90.9900	
Best Prec@1: [69.420]	
Starting epoch number: 76 Learning rate: 0.1
Train: [76]	Time 241.149	Data 0.274	Loss 0.355	Prec@1 88.8340	Prec@5 99.2220	
Val: [76]	Time 15.712	Data 0.119	Loss 1.559	Prec@1 66.2000	Prec@5 89.7800	
Best Prec@1: [69.420]	
Starting epoch number: 77 Learning rate: 0.1
Train: [77]	Time 242.280	Data 0.281	Loss 0.372	Prec@1 88.1920	Prec@5 99.1620	
Val: [77]	Time 15.637	Data 0.104	Loss 1.425	Prec@1 68.2400	Prec@5 90.8600	
Best Prec@1: [69.420]	
Starting epoch number: 78 Learning rate: 0.1
Train: [78]	Time 241.405	Data 0.272	Loss 0.362	Prec@1 88.4400	Prec@5 99.1780	
Val: [78]	Time 15.502	Data 0.121	Loss 1.569	Prec@1 65.4900	Prec@5 89.3500	
Best Prec@1: [69.420]	
Starting epoch number: 79 Learning rate: 0.1
Train: [79]	Time 240.905	Data 0.281	Loss 0.348	Prec@1 89.1540	Prec@5 99.2560	
Val: [79]	Time 16.301	Data 0.663	Loss 1.469	Prec@1 67.7900	Prec@5 90.5400	
Best Prec@1: [69.420]	
Starting epoch number: 80 Learning rate: 0.1
Train: [80]	Time 241.470	Data 0.257	Loss 0.369	Prec@1 88.2380	Prec@5 99.2200	
Val: [80]	Time 15.400	Data 0.102	Loss 1.538	Prec@1 66.3700	Prec@5 90.0400	
Best Prec@1: [69.420]	
Starting epoch number: 81 Learning rate: 0.1
Train: [81]	Time 241.457	Data 0.277	Loss 0.360	Prec@1 88.5520	Prec@5 99.2720	
Val: [81]	Time 15.704	Data 0.109	Loss 1.542	Prec@1 66.1300	Prec@5 89.6600	
Best Prec@1: [69.420]	
Starting epoch number: 82 Learning rate: 0.1
Train: [82]	Time 241.309	Data 0.262	Loss 0.347	Prec@1 89.0080	Prec@5 99.2360	
Val: [82]	Time 15.589	Data 0.097	Loss 1.414	Prec@1 69.1200	Prec@5 90.9500	
Best Prec@1: [69.420]	
Starting epoch number: 83 Learning rate: 0.1
Train: [83]	Time 242.455	Data 0.270	Loss 0.353	Prec@1 88.7860	Prec@5 99.2080	
Val: [83]	Time 15.585	Data 0.101	Loss 1.754	Prec@1 64.1300	Prec@5 88.9300	
Best Prec@1: [69.420]	
Starting epoch number: 84 Learning rate: 0.1
Train: [84]	Time 242.487	Data 0.270	Loss 0.358	Prec@1 88.5980	Prec@5 99.2640	
Val: [84]	Time 15.702	Data 0.107	Loss 1.502	Prec@1 67.2600	Prec@5 90.1600	
Best Prec@1: [69.420]	
Starting epoch number: 85 Learning rate: 0.1
Train: [85]	Time 241.488	Data 0.281	Loss 0.353	Prec@1 88.6900	Prec@5 99.2100	
Val: [85]	Time 15.421	Data 0.129	Loss 1.457	Prec@1 66.9300	Prec@5 90.2700	
Best Prec@1: [69.420]	
Starting epoch number: 86 Learning rate: 0.1
Train: [86]	Time 241.311	Data 0.277	Loss 0.355	Prec@1 88.7040	Prec@5 99.2400	
Val: [86]	Time 15.631	Data 0.115	Loss 1.418	Prec@1 67.6800	Prec@5 90.4800	
Best Prec@1: [69.420]	
Starting epoch number: 87 Learning rate: 0.1
Train: [87]	Time 241.744	Data 0.270	Loss 0.343	Prec@1 89.1040	Prec@5 99.3360	
Val: [87]	Time 15.693	Data 0.102	Loss 1.560	Prec@1 67.0300	Prec@5 90.1000	
Best Prec@1: [69.420]	
Starting epoch number: 88 Learning rate: 0.1
Train: [88]	Time 241.736	Data 0.252	Loss 0.338	Prec@1 89.2780	Prec@5 99.3520	
Val: [88]	Time 15.537	Data 0.104	Loss 1.490	Prec@1 67.1500	Prec@5 90.1800	
Best Prec@1: [69.420]	
Starting epoch number: 89 Learning rate: 0.1
Train: [89]	Time 242.306	Data 0.260	Loss 0.356	Prec@1 88.5940	Prec@5 99.1340	
Val: [89]	Time 15.545	Data 0.104	Loss 1.539	Prec@1 66.4000	Prec@5 90.0600	
Best Prec@1: [69.420]	
Starting epoch number: 90 Learning rate: 0.1
Train: [90]	Time 242.115	Data 0.281	Loss 0.344	Prec@1 89.0640	Prec@5 99.3440	
Val: [90]	Time 15.628	Data 0.107	Loss 1.498	Prec@1 66.4400	Prec@5 90.3700	
Best Prec@1: [69.420]	
Starting epoch number: 91 Learning rate: 0.1
Train: [91]	Time 241.768	Data 0.273	Loss 0.359	Prec@1 88.6200	Prec@5 99.2040	
Val: [91]	Time 15.778	Data 0.098	Loss 1.573	Prec@1 67.0300	Prec@5 90.1700	
Best Prec@1: [69.420]	
Starting epoch number: 92 Learning rate: 0.1
Train: [92]	Time 242.127	Data 0.271	Loss 0.335	Prec@1 89.2580	Prec@5 99.3300	
Val: [92]	Time 15.661	Data 0.096	Loss 1.423	Prec@1 68.8000	Prec@5 90.7800	
Best Prec@1: [69.420]	
Starting epoch number: 93 Learning rate: 0.1
Train: [93]	Time 241.990	Data 0.260	Loss 0.340	Prec@1 89.1880	Prec@5 99.2820	
Val: [93]	Time 15.759	Data 0.109	Loss 1.498	Prec@1 65.9200	Prec@5 89.9400	
Best Prec@1: [69.420]	
Starting epoch number: 94 Learning rate: 0.1
Train: [94]	Time 241.564	Data 0.256	Loss 0.352	Prec@1 88.7500	Prec@5 99.2940	
Val: [94]	Time 15.774	Data 0.105	Loss 1.582	Prec@1 65.5800	Prec@5 89.7200	
Best Prec@1: [69.420]	
Starting epoch number: 95 Learning rate: 0.1
Train: [95]	Time 242.655	Data 0.245	Loss 0.349	Prec@1 88.8560	Prec@5 99.2280	
Val: [95]	Time 15.568	Data 0.102	Loss 1.620	Prec@1 66.2800	Prec@5 89.2700	
Best Prec@1: [69.420]	
Starting epoch number: 96 Learning rate: 0.1
Train: [96]	Time 242.788	Data 0.267	Loss 0.339	Prec@1 89.1900	Prec@5 99.3080	
Val: [96]	Time 15.639	Data 0.114	Loss 1.399	Prec@1 68.4700	Prec@5 90.8500	
Best Prec@1: [69.420]	
Starting epoch number: 97 Learning rate: 0.1
Train: [97]	Time 242.194	Data 0.261	Loss 0.343	Prec@1 89.1000	Prec@5 99.2540	
Val: [97]	Time 15.723	Data 0.097	Loss 1.419	Prec@1 68.5000	Prec@5 90.6800	
Best Prec@1: [69.420]	
Starting epoch number: 98 Learning rate: 0.1
Train: [98]	Time 242.050	Data 0.262	Loss 0.345	Prec@1 89.0960	Prec@5 99.3400	
Val: [98]	Time 15.614	Data 0.117	Loss 1.467	Prec@1 68.1500	Prec@5 90.3800	
Best Prec@1: [69.420]	
Starting epoch number: 99 Learning rate: 0.1
Train: [99]	Time 242.505	Data 0.282	Loss 0.339	Prec@1 89.2280	Prec@5 99.3140	
Val: [99]	Time 15.449	Data 0.105	Loss 1.405	Prec@1 68.6200	Prec@5 90.8500	
Best Prec@1: [69.420]	
Starting epoch number: 100 Learning rate: 0.1
Train: [100]	Time 241.928	Data 0.257	Loss 0.357	Prec@1 88.7360	Prec@5 99.2120	
Val: [100]	Time 15.584	Data 0.115	Loss 1.476	Prec@1 67.0100	Prec@5 89.8200	
Best Prec@1: [69.420]	
Starting epoch number: 101 Learning rate: 0.1
Train: [101]	Time 241.413	Data 0.268	Loss 0.339	Prec@1 89.1640	Prec@5 99.3000	
Val: [101]	Time 15.639	Data 0.118	Loss 1.567	Prec@1 66.1000	Prec@5 89.6600	
Best Prec@1: [69.420]	
Starting epoch number: 102 Learning rate: 0.1
Train: [102]	Time 241.775	Data 0.288	Loss 0.354	Prec@1 88.5580	Prec@5 99.2380	
Val: [102]	Time 15.550	Data 0.101	Loss 1.443	Prec@1 67.7900	Prec@5 90.3100	
Best Prec@1: [69.420]	
Starting epoch number: 103 Learning rate: 0.1
Train: [103]	Time 242.279	Data 0.274	Loss 0.325	Prec@1 89.6000	Prec@5 99.3040	
Val: [103]	Time 15.569	Data 0.099	Loss 1.411	Prec@1 68.1200	Prec@5 90.4600	
Best Prec@1: [69.420]	
Starting epoch number: 104 Learning rate: 0.1
Train: [104]	Time 243.054	Data 0.271	Loss 0.344	Prec@1 89.1340	Prec@5 99.2140	
Val: [104]	Time 15.783	Data 0.174	Loss 1.469	Prec@1 66.8500	Prec@5 90.0000	
Best Prec@1: [69.420]	
Starting epoch number: 105 Learning rate: 0.1
Train: [105]	Time 241.770	Data 0.266	Loss 0.344	Prec@1 89.1420	Prec@5 99.3440	
Val: [105]	Time 15.562	Data 0.123	Loss 1.352	Prec@1 69.5600	Prec@5 91.2500	
Best Prec@1: [69.560]	
Starting epoch number: 106 Learning rate: 0.1
Train: [106]	Time 241.796	Data 0.257	Loss 0.328	Prec@1 89.6160	Prec@5 99.3980	
Val: [106]	Time 15.458	Data 0.105	Loss 1.532	Prec@1 67.0400	Prec@5 89.5000	
Best Prec@1: [69.560]	
Starting epoch number: 107 Learning rate: 0.1
Train: [107]	Time 241.221	Data 0.267	Loss 0.330	Prec@1 89.4740	Prec@5 99.3460	
Val: [107]	Time 15.604	Data 0.110	Loss 1.715	Prec@1 64.8500	Prec@5 88.8600	
Best Prec@1: [69.560]	
Starting epoch number: 108 Learning rate: 0.1
Train: [108]	Time 241.853	Data 0.283	Loss 0.349	Prec@1 88.9080	Prec@5 99.2280	
Val: [108]	Time 15.575	Data 0.111	Loss 1.675	Prec@1 63.7000	Prec@5 88.9700	
Best Prec@1: [69.560]	
Starting epoch number: 109 Learning rate: 0.1
Train: [109]	Time 241.629	Data 0.291	Loss 0.337	Prec@1 89.2700	Prec@5 99.2920	
Val: [109]	Time 15.737	Data 0.119	Loss 1.529	Prec@1 66.5100	Prec@5 90.3000	
Best Prec@1: [69.560]	
Starting epoch number: 110 Learning rate: 0.1
Train: [110]	Time 241.162	Data 0.274	Loss 0.347	Prec@1 89.0700	Prec@5 99.2680	
Val: [110]	Time 15.746	Data 0.091	Loss 1.550	Prec@1 66.2700	Prec@5 90.6300	
Best Prec@1: [69.560]	
Starting epoch number: 111 Learning rate: 0.1
Train: [111]	Time 240.954	Data 0.256	Loss 0.331	Prec@1 89.3540	Prec@5 99.3540	
Val: [111]	Time 15.732	Data 0.093	Loss 1.359	Prec@1 68.9800	Prec@5 90.6200	
Best Prec@1: [69.560]	
Starting epoch number: 112 Learning rate: 0.1
Train: [112]	Time 241.160	Data 0.260	Loss 0.327	Prec@1 89.6180	Prec@5 99.3100	
Val: [112]	Time 15.550	Data 0.109	Loss 1.672	Prec@1 65.7200	Prec@5 89.7000	
Best Prec@1: [69.560]	
Starting epoch number: 113 Learning rate: 0.1
Train: [113]	Time 241.229	Data 0.264	Loss 0.339	Prec@1 89.1660	Prec@5 99.3120	
Val: [113]	Time 15.743	Data 0.100	Loss 1.416	Prec@1 68.6000	Prec@5 90.6700	
Best Prec@1: [69.560]	
Starting epoch number: 114 Learning rate: 0.1
Train: [114]	Time 242.182	Data 0.276	Loss 0.328	Prec@1 89.6500	Prec@5 99.3420	
Val: [114]	Time 15.660	Data 0.098	Loss 1.472	Prec@1 67.9100	Prec@5 90.4500	
Best Prec@1: [69.560]	
Starting epoch number: 115 Learning rate: 0.1
Train: [115]	Time 241.637	Data 0.261	Loss 0.334	Prec@1 89.4220	Prec@5 99.3480	
Val: [115]	Time 15.609	Data 0.101	Loss 1.507	Prec@1 67.4800	Prec@5 90.1600	
Best Prec@1: [69.560]	
Starting epoch number: 116 Learning rate: 0.1
Train: [116]	Time 242.504	Data 0.259	Loss 0.333	Prec@1 89.4800	Prec@5 99.2740	
Val: [116]	Time 15.542	Data 0.109	Loss 1.421	Prec@1 67.9100	Prec@5 91.1600	
Best Prec@1: [69.560]	
Starting epoch number: 117 Learning rate: 0.1
Train: [117]	Time 243.361	Data 0.283	Loss 0.327	Prec@1 89.6480	Prec@5 99.3180	
Val: [117]	Time 15.605	Data 0.115	Loss 1.339	Prec@1 68.3400	Prec@5 91.1800	
Best Prec@1: [69.560]	
Starting epoch number: 118 Learning rate: 0.1
Train: [118]	Time 242.396	Data 0.261	Loss 0.341	Prec@1 89.1060	Prec@5 99.2700	
Val: [118]	Time 15.606	Data 0.104	Loss 1.608	Prec@1 66.4400	Prec@5 89.8900	
Best Prec@1: [69.560]	
Starting epoch number: 119 Learning rate: 0.1
Train: [119]	Time 242.121	Data 0.269	Loss 0.316	Prec@1 89.8420	Prec@5 99.3660	
Val: [119]	Time 15.381	Data 0.114	Loss 1.524	Prec@1 67.1000	Prec@5 90.0800	
Best Prec@1: [69.560]	
Starting epoch number: 120 Learning rate: 0.1
Train: [120]	Time 241.118	Data 0.269	Loss 0.336	Prec@1 89.2680	Prec@5 99.2800	
Val: [120]	Time 15.382	Data 0.107	Loss 1.583	Prec@1 65.8700	Prec@5 88.8600	
Best Prec@1: [69.560]	
Starting epoch number: 121 Learning rate: 0.1
Train: [121]	Time 240.178	Data 0.278	Loss 0.335	Prec@1 89.3960	Prec@5 99.2360	
Val: [121]	Time 15.688	Data 0.090	Loss 1.554	Prec@1 66.1200	Prec@5 88.9400	
Best Prec@1: [69.560]	
Starting epoch number: 122 Learning rate: 0.1
Train: [122]	Time 242.099	Data 0.249	Loss 0.315	Prec@1 89.9520	Prec@5 99.3760	
Val: [122]	Time 15.415	Data 0.110	Loss 1.438	Prec@1 67.8200	Prec@5 90.3200	
Best Prec@1: [69.560]	
Starting epoch number: 123 Learning rate: 0.1
Train: [123]	Time 242.192	Data 0.285	Loss 0.335	Prec@1 89.1700	Prec@5 99.3160	
Val: [123]	Time 15.577	Data 0.105	Loss 1.504	Prec@1 67.0400	Prec@5 90.2000	
Best Prec@1: [69.560]	
Starting epoch number: 124 Learning rate: 0.1
Train: [124]	Time 241.128	Data 0.292	Loss 0.337	Prec@1 89.1640	Prec@5 99.3540	
Val: [124]	Time 15.466	Data 0.095	Loss 1.518	Prec@1 68.2200	Prec@5 90.0300	
Best Prec@1: [69.560]	
Starting epoch number: 125 Learning rate: 0.1
Train: [125]	Time 242.348	Data 0.254	Loss 0.317	Prec@1 89.9180	Prec@5 99.4160	
Val: [125]	Time 15.439	Data 0.106	Loss 1.505	Prec@1 67.4900	Prec@5 90.4300	
Best Prec@1: [69.560]	
Starting epoch number: 126 Learning rate: 0.1
Train: [126]	Time 242.334	Data 0.259	Loss 0.325	Prec@1 89.7540	Prec@5 99.3460	
Val: [126]	Time 15.639	Data 0.107	Loss 1.542	Prec@1 67.1400	Prec@5 89.3000	
Best Prec@1: [69.560]	
Starting epoch number: 127 Learning rate: 0.1
Train: [127]	Time 241.932	Data 0.274	Loss 0.331	Prec@1 89.5580	Prec@5 99.3000	
Val: [127]	Time 15.640	Data 0.106	Loss 1.591	Prec@1 67.0300	Prec@5 90.0300	
Best Prec@1: [69.560]	
Starting epoch number: 128 Learning rate: 0.1
Train: [128]	Time 241.480	Data 0.293	Loss 0.327	Prec@1 89.6640	Prec@5 99.3400	
Val: [128]	Time 15.780	Data 0.118	Loss 1.514	Prec@1 67.4700	Prec@5 89.7300	
Best Prec@1: [69.560]	
Starting epoch number: 129 Learning rate: 0.1
Train: [129]	Time 242.123	Data 0.254	Loss 0.322	Prec@1 89.7380	Prec@5 99.3660	
Val: [129]	Time 15.575	Data 0.115	Loss 1.493	Prec@1 67.9400	Prec@5 90.4300	
Best Prec@1: [69.560]	
Starting epoch number: 130 Learning rate: 0.1
Train: [130]	Time 241.123	Data 0.246	Loss 0.327	Prec@1 89.5300	Prec@5 99.3140	
Val: [130]	Time 15.551	Data 0.100	Loss 1.612	Prec@1 66.1300	Prec@5 89.2800	
Best Prec@1: [69.560]	
Starting epoch number: 131 Learning rate: 0.1
Train: [131]	Time 242.218	Data 0.251	Loss 0.336	Prec@1 89.3620	Prec@5 99.3580	
Val: [131]	Time 15.611	Data 0.118	Loss 1.448	Prec@1 68.3000	Prec@5 90.2100	
Best Prec@1: [69.560]	
Starting epoch number: 132 Learning rate: 0.1
Train: [132]	Time 241.973	Data 0.263	Loss 0.309	Prec@1 90.3140	Prec@5 99.4820	
Val: [132]	Time 15.757	Data 0.119	Loss 1.449	Prec@1 68.7000	Prec@5 90.3000	
Best Prec@1: [69.560]	
Starting epoch number: 133 Learning rate: 0.1
Train: [133]	Time 241.442	Data 0.273	Loss 0.327	Prec@1 89.4660	Prec@5 99.3460	
Val: [133]	Time 15.662	Data 0.116	Loss 1.484	Prec@1 68.1200	Prec@5 90.5000	
Best Prec@1: [69.560]	
Starting epoch number: 134 Learning rate: 0.1
Train: [134]	Time 241.779	Data 0.262	Loss 0.324	Prec@1 89.6580	Prec@5 99.3440	
Val: [134]	Time 15.764	Data 0.097	Loss 1.514	Prec@1 67.6400	Prec@5 90.3500	
Best Prec@1: [69.560]	
Starting epoch number: 135 Learning rate: 0.1
Train: [135]	Time 241.515	Data 0.264	Loss 0.314	Prec@1 89.9320	Prec@5 99.4080	
Val: [135]	Time 15.574	Data 0.106	Loss 1.478	Prec@1 68.2500	Prec@5 90.2700	
Best Prec@1: [69.560]	
Starting epoch number: 136 Learning rate: 0.1
Train: [136]	Time 241.361	Data 0.258	Loss 0.323	Prec@1 89.6940	Prec@5 99.3920	
Val: [136]	Time 15.757	Data 0.099	Loss 1.571	Prec@1 66.9600	Prec@5 89.1600	
Best Prec@1: [69.560]	
Starting epoch number: 137 Learning rate: 0.1
Train: [137]	Time 242.029	Data 0.278	Loss 0.330	Prec@1 89.5960	Prec@5 99.3040	
Val: [137]	Time 15.570	Data 0.101	Loss 1.509	Prec@1 67.4500	Prec@5 89.5700	
Best Prec@1: [69.560]	
Starting epoch number: 138 Learning rate: 0.1
Train: [138]	Time 242.219	Data 0.268	Loss 0.316	Prec@1 89.9940	Prec@5 99.3880	
Val: [138]	Time 15.694	Data 0.125	Loss 1.549	Prec@1 67.3600	Prec@5 89.5800	
Best Prec@1: [69.560]	
Starting epoch number: 139 Learning rate: 0.1
Train: [139]	Time 240.195	Data 0.274	Loss 0.321	Prec@1 89.8220	Prec@5 99.3720	
Val: [139]	Time 15.587	Data 0.100	Loss 1.319	Prec@1 69.8200	Prec@5 91.5800	
Best Prec@1: [69.820]	
Starting epoch number: 140 Learning rate: 0.1
Train: [140]	Time 241.405	Data 0.244	Loss 0.332	Prec@1 89.4560	Prec@5 99.3500	
Val: [140]	Time 15.606	Data 0.107	Loss 1.412	Prec@1 69.0000	Prec@5 90.5300	
Best Prec@1: [69.820]	
Starting epoch number: 141 Learning rate: 0.1
Train: [141]	Time 241.904	Data 0.259	Loss 0.325	Prec@1 89.7460	Prec@5 99.3700	
Val: [141]	Time 15.562	Data 0.104	Loss 1.458	Prec@1 67.5200	Prec@5 90.8100	
Best Prec@1: [69.820]	
Starting epoch number: 142 Learning rate: 0.1
Train: [142]	Time 240.987	Data 0.259	Loss 0.315	Prec@1 89.8240	Prec@5 99.3620	
Val: [142]	Time 15.553	Data 0.109	Loss 1.611	Prec@1 66.6400	Prec@5 89.1500	
Best Prec@1: [69.820]	
Starting epoch number: 143 Learning rate: 0.1
Train: [143]	Time 242.019	Data 0.247	Loss 0.312	Prec@1 90.0320	Prec@5 99.3740	
Val: [143]	Time 15.644	Data 0.113	Loss 1.488	Prec@1 68.0800	Prec@5 90.2500	
Best Prec@1: [69.820]	
Starting epoch number: 144 Learning rate: 0.1
Train: [144]	Time 241.880	Data 0.256	Loss 0.327	Prec@1 89.6660	Prec@5 99.3880	
Val: [144]	Time 15.769	Data 0.100	Loss 1.449	Prec@1 68.2700	Prec@5 89.9800	
Best Prec@1: [69.820]	
Starting epoch number: 145 Learning rate: 0.1
Train: [145]	Time 242.105	Data 0.252	Loss 0.318	Prec@1 89.7880	Prec@5 99.4180	
Val: [145]	Time 15.549	Data 0.115	Loss 1.387	Prec@1 69.4000	Prec@5 90.8700	
Best Prec@1: [69.820]	
Starting epoch number: 146 Learning rate: 0.1
Train: [146]	Time 241.652	Data 0.266	Loss 0.315	Prec@1 89.7820	Prec@5 99.4100	
Val: [146]	Time 15.396	Data 0.102	Loss 1.610	Prec@1 65.7500	Prec@5 89.4100	
Best Prec@1: [69.820]	
Starting epoch number: 147 Learning rate: 0.1
Train: [147]	Time 241.910	Data 0.266	Loss 0.324	Prec@1 89.7380	Prec@5 99.3620	
Val: [147]	Time 15.715	Data 0.091	Loss 1.320	Prec@1 69.1400	Prec@5 91.2400	
Best Prec@1: [69.820]	
Starting epoch number: 148 Learning rate: 0.1
Train: [148]	Time 241.161	Data 0.255	Loss 0.311	Prec@1 90.2280	Prec@5 99.4480	
Val: [148]	Time 15.676	Data 0.100	Loss 1.505	Prec@1 67.7200	Prec@5 90.0500	
Best Prec@1: [69.820]	
Starting epoch number: 149 Learning rate: 0.1
Train: [149]	Time 240.540	Data 0.250	Loss 0.315	Prec@1 89.8480	Prec@5 99.3680	
Val: [149]	Time 15.592	Data 0.101	Loss 1.539	Prec@1 67.4700	Prec@5 89.7300	
Best Prec@1: [69.820]	
Starting epoch number: 150 Learning rate: 0.010000000000000002
Train: [150]	Time 242.167	Data 0.270	Loss 0.098	Prec@1 97.3520	Prec@5 99.9420	
Val: [150]	Time 15.579	Data 0.102	Loss 0.890	Prec@1 78.6700	Prec@5 94.9800	
Best Prec@1: [78.670]	
Starting epoch number: 151 Learning rate: 0.010000000000000002
Train: [151]	Time 241.948	Data 0.273	Loss 0.045	Prec@1 99.1020	Prec@5 99.9960	
Val: [151]	Time 15.628	Data 0.097	Loss 0.877	Prec@1 79.0000	Prec@5 95.1700	
Best Prec@1: [79.000]	
Starting epoch number: 152 Learning rate: 0.010000000000000002
Train: [152]	Time 242.030	Data 0.274	Loss 0.031	Prec@1 99.5220	Prec@5 100.0000	
Val: [152]	Time 15.632	Data 0.112	Loss 0.875	Prec@1 79.4000	Prec@5 95.1700	
Best Prec@1: [79.400]	
Starting epoch number: 153 Learning rate: 0.010000000000000002
Train: [153]	Time 241.960	Data 0.267	Loss 0.025	Prec@1 99.6740	Prec@5 100.0000	
Val: [153]	Time 15.545	Data 0.121	Loss 0.874	Prec@1 79.5900	Prec@5 95.2200	
Best Prec@1: [79.590]	
Starting epoch number: 154 Learning rate: 0.010000000000000002
Train: [154]	Time 242.009	Data 0.274	Loss 0.021	Prec@1 99.7660	Prec@5 99.9980	
Val: [154]	Time 15.761	Data 0.096	Loss 0.870	Prec@1 79.5500	Prec@5 95.2700	
Best Prec@1: [79.590]	
Starting epoch number: 155 Learning rate: 0.010000000000000002
Train: [155]	Time 242.310	Data 0.256	Loss 0.018	Prec@1 99.8100	Prec@5 100.0000	
Val: [155]	Time 15.626	Data 0.099	Loss 0.868	Prec@1 79.6400	Prec@5 95.2400	
Best Prec@1: [79.640]	
Starting epoch number: 156 Learning rate: 0.010000000000000002
Train: [156]	Time 241.906	Data 0.264	Loss 0.017	Prec@1 99.8520	Prec@5 100.0000	
Val: [156]	Time 15.580	Data 0.115	Loss 0.867	Prec@1 80.0600	Prec@5 95.2500	
Best Prec@1: [80.060]	
Starting epoch number: 157 Learning rate: 0.010000000000000002
Train: [157]	Time 240.862	Data 0.272	Loss 0.016	Prec@1 99.8600	Prec@5 99.9960	
Val: [157]	Time 15.571	Data 0.118	Loss 0.873	Prec@1 80.1800	Prec@5 95.1800	
Best Prec@1: [80.180]	
Starting epoch number: 158 Learning rate: 0.010000000000000002
Train: [158]	Time 241.073	Data 0.270	Loss 0.014	Prec@1 99.8880	Prec@5 100.0000	
Val: [158]	Time 15.781	Data 0.102	Loss 0.861	Prec@1 80.0300	Prec@5 95.3000	
Best Prec@1: [80.180]	
Starting epoch number: 159 Learning rate: 0.010000000000000002
Train: [159]	Time 241.049	Data 0.261	Loss 0.013	Prec@1 99.8780	Prec@5 100.0000	
Val: [159]	Time 15.738	Data 0.098	Loss 0.861	Prec@1 80.3500	Prec@5 95.3800	
Best Prec@1: [80.350]	
Starting epoch number: 160 Learning rate: 0.010000000000000002
Train: [160]	Time 239.554	Data 0.258	Loss 0.013	Prec@1 99.9060	Prec@5 100.0000	
Val: [160]	Time 15.481	Data 0.105	Loss 0.861	Prec@1 80.4000	Prec@5 95.3300	
Best Prec@1: [80.400]	
Starting epoch number: 161 Learning rate: 0.010000000000000002
Train: [161]	Time 241.283	Data 0.272	Loss 0.012	Prec@1 99.9200	Prec@5 100.0000	
Val: [161]	Time 15.726	Data 0.128	Loss 0.857	Prec@1 80.4800	Prec@5 95.3300	
Best Prec@1: [80.480]	
Starting epoch number: 162 Learning rate: 0.010000000000000002
Train: [162]	Time 241.982	Data 0.275	Loss 0.011	Prec@1 99.9360	Prec@5 100.0000	
Val: [162]	Time 15.682	Data 0.103	Loss 0.861	Prec@1 80.4000	Prec@5 95.2600	
Best Prec@1: [80.480]	
Starting epoch number: 163 Learning rate: 0.010000000000000002
Train: [163]	Time 242.516	Data 0.274	Loss 0.011	Prec@1 99.9280	Prec@5 100.0000	
Val: [163]	Time 15.616	Data 0.121	Loss 0.867	Prec@1 80.2000	Prec@5 95.2000	
Best Prec@1: [80.480]	
Starting epoch number: 164 Learning rate: 0.010000000000000002
Train: [164]	Time 241.446	Data 0.284	Loss 0.011	Prec@1 99.9300	Prec@5 100.0000	
Val: [164]	Time 15.665	Data 0.123	Loss 0.853	Prec@1 80.5500	Prec@5 95.3400	
Best Prec@1: [80.550]	
Starting epoch number: 165 Learning rate: 0.010000000000000002
Train: [165]	Time 240.683	Data 0.275	Loss 0.010	Prec@1 99.9180	Prec@5 100.0000	
Val: [165]	Time 15.469	Data 0.094	Loss 0.854	Prec@1 80.4100	Prec@5 95.2500	
Best Prec@1: [80.550]	
Starting epoch number: 166 Learning rate: 0.010000000000000002
Train: [166]	Time 240.777	Data 0.267	Loss 0.010	Prec@1 99.9340	Prec@5 100.0000	
Val: [166]	Time 15.657	Data 0.106	Loss 0.849	Prec@1 80.5000	Prec@5 95.2800	
Best Prec@1: [80.550]	
Starting epoch number: 167 Learning rate: 0.010000000000000002
Train: [167]	Time 242.071	Data 0.254	Loss 0.010	Prec@1 99.9380	Prec@5 100.0000	
Val: [167]	Time 15.417	Data 0.108	Loss 0.855	Prec@1 80.5100	Prec@5 95.2700	
Best Prec@1: [80.550]	
Starting epoch number: 168 Learning rate: 0.010000000000000002
Train: [168]	Time 241.733	Data 0.279	Loss 0.010	Prec@1 99.9440	Prec@5 100.0000	
Val: [168]	Time 15.579	Data 0.115	Loss 0.849	Prec@1 80.6100	Prec@5 95.2300	
Best Prec@1: [80.610]	
Starting epoch number: 169 Learning rate: 0.010000000000000002
Train: [169]	Time 241.794	Data 0.278	Loss 0.009	Prec@1 99.9460	Prec@5 100.0000	
Val: [169]	Time 15.596	Data 0.107	Loss 0.851	Prec@1 80.3700	Prec@5 95.2000	
Best Prec@1: [80.610]	
Starting epoch number: 170 Learning rate: 0.010000000000000002
Train: [170]	Time 242.462	Data 0.261	Loss 0.009	Prec@1 99.9580	Prec@5 100.0000	
Val: [170]	Time 15.734	Data 0.106	Loss 0.853	Prec@1 80.7600	Prec@5 95.3000	
Best Prec@1: [80.760]	
Starting epoch number: 171 Learning rate: 0.010000000000000002
Train: [171]	Time 242.274	Data 0.270	Loss 0.009	Prec@1 99.9560	Prec@5 100.0000	
Val: [171]	Time 15.635	Data 0.112	Loss 0.848	Prec@1 80.7200	Prec@5 95.2500	
Best Prec@1: [80.760]	
Starting epoch number: 172 Learning rate: 0.010000000000000002
Train: [172]	Time 241.017	Data 0.266	Loss 0.009	Prec@1 99.9380	Prec@5 100.0000	
Val: [172]	Time 15.690	Data 0.097	Loss 0.845	Prec@1 80.3100	Prec@5 95.3300	
Best Prec@1: [80.760]	
Starting epoch number: 173 Learning rate: 0.010000000000000002
Train: [173]	Time 241.092	Data 0.264	Loss 0.009	Prec@1 99.9420	Prec@5 100.0000	
Val: [173]	Time 15.640	Data 0.105	Loss 0.846	Prec@1 80.2900	Prec@5 95.2500	
Best Prec@1: [80.760]	
Starting epoch number: 174 Learning rate: 0.010000000000000002
Train: [174]	Time 242.079	Data 0.266	Loss 0.009	Prec@1 99.9540	Prec@5 100.0000	
Val: [174]	Time 15.614	Data 0.100	Loss 0.842	Prec@1 80.5400	Prec@5 95.2000	
Best Prec@1: [80.760]	
Starting epoch number: 175 Learning rate: 0.010000000000000002
Train: [175]	Time 241.202	Data 0.267	Loss 0.009	Prec@1 99.9540	Prec@5 100.0000	
Val: [175]	Time 15.572	Data 0.105	Loss 0.837	Prec@1 80.5600	Prec@5 95.3300	
Best Prec@1: [80.760]	
Starting epoch number: 176 Learning rate: 0.010000000000000002
Train: [176]	Time 241.356	Data 0.259	Loss 0.008	Prec@1 99.9540	Prec@5 100.0000	
Val: [176]	Time 15.625	Data 0.100	Loss 0.836	Prec@1 80.6700	Prec@5 95.2500	
Best Prec@1: [80.760]	
Starting epoch number: 177 Learning rate: 0.010000000000000002
Train: [177]	Time 240.934	Data 0.274	Loss 0.008	Prec@1 99.9500	Prec@5 100.0000	
Val: [177]	Time 15.710	Data 0.100	Loss 0.833	Prec@1 80.4800	Prec@5 95.3800	
Best Prec@1: [80.760]	
Starting epoch number: 178 Learning rate: 0.010000000000000002
Train: [178]	Time 241.729	Data 0.267	Loss 0.008	Prec@1 99.9740	Prec@5 100.0000	
Val: [178]	Time 15.656	Data 0.094	Loss 0.841	Prec@1 80.4100	Prec@5 95.2000	
Best Prec@1: [80.760]	
Starting epoch number: 179 Learning rate: 0.010000000000000002
Train: [179]	Time 241.674	Data 0.248	Loss 0.008	Prec@1 99.9540	Prec@5 100.0000	
Val: [179]	Time 15.540	Data 0.097	Loss 0.827	Prec@1 80.8200	Prec@5 95.1700	
Best Prec@1: [80.820]	
Starting epoch number: 180 Learning rate: 0.010000000000000002
Train: [180]	Time 241.526	Data 0.275	Loss 0.008	Prec@1 99.9520	Prec@5 100.0000	
Val: [180]	Time 15.589	Data 0.099	Loss 0.828	Prec@1 80.7900	Prec@5 95.1600	
Best Prec@1: [80.820]	
Starting epoch number: 181 Learning rate: 0.010000000000000002
Train: [181]	Time 242.095	Data 0.297	Loss 0.008	Prec@1 99.9600	Prec@5 100.0000	
Val: [181]	Time 15.657	Data 0.118	Loss 0.827	Prec@1 80.8100	Prec@5 95.2100	
Best Prec@1: [80.820]	
Starting epoch number: 182 Learning rate: 0.010000000000000002
Train: [182]	Time 240.768	Data 0.259	Loss 0.008	Prec@1 99.9660	Prec@5 100.0000	
Val: [182]	Time 15.613	Data 0.100	Loss 0.828	Prec@1 80.4100	Prec@5 95.1700	
Best Prec@1: [80.820]	
Starting epoch number: 183 Learning rate: 0.010000000000000002
Train: [183]	Time 241.286	Data 0.259	Loss 0.008	Prec@1 99.9700	Prec@5 100.0000	
Val: [183]	Time 15.554	Data 0.111	Loss 0.829	Prec@1 80.6700	Prec@5 95.3400	
Best Prec@1: [80.820]	
Starting epoch number: 184 Learning rate: 0.010000000000000002
Train: [184]	Time 242.202	Data 0.255	Loss 0.008	Prec@1 99.9700	Prec@5 100.0000	
Val: [184]	Time 15.498	Data 0.105	Loss 0.825	Prec@1 80.6900	Prec@5 95.1800	
Best Prec@1: [80.820]	
Starting epoch number: 185 Learning rate: 0.010000000000000002
Train: [185]	Time 241.493	Data 0.278	Loss 0.008	Prec@1 99.9540	Prec@5 100.0000	
Val: [185]	Time 15.578	Data 0.119	Loss 0.819	Prec@1 80.7200	Prec@5 95.2900	
Best Prec@1: [80.820]	
Starting epoch number: 186 Learning rate: 0.010000000000000002
Train: [186]	Time 241.972	Data 0.284	Loss 0.008	Prec@1 99.9660	Prec@5 100.0000	
Val: [186]	Time 15.605	Data 0.103	Loss 0.830	Prec@1 80.3500	Prec@5 95.2900	
Best Prec@1: [80.820]	
Starting epoch number: 187 Learning rate: 0.010000000000000002
Train: [187]	Time 241.360	Data 0.251	Loss 0.008	Prec@1 99.9700	Prec@5 100.0000	
Val: [187]	Time 15.532	Data 0.105	Loss 0.819	Prec@1 80.6900	Prec@5 95.2300	
Best Prec@1: [80.820]	
Starting epoch number: 188 Learning rate: 0.010000000000000002
Train: [188]	Time 241.694	Data 0.271	Loss 0.008	Prec@1 99.9600	Prec@5 100.0000	
Val: [188]	Time 15.565	Data 0.111	Loss 0.821	Prec@1 80.8300	Prec@5 95.2200	
Best Prec@1: [80.830]	
Starting epoch number: 189 Learning rate: 0.010000000000000002
Train: [189]	Time 240.953	Data 0.278	Loss 0.007	Prec@1 99.9680	Prec@5 100.0000	
Val: [189]	Time 15.610	Data 0.102	Loss 0.822	Prec@1 80.4900	Prec@5 95.2400	
Best Prec@1: [80.830]	
Starting epoch number: 190 Learning rate: 0.010000000000000002
Train: [190]	Time 241.586	Data 0.268	Loss 0.007	Prec@1 99.9700	Prec@5 100.0000	
Val: [190]	Time 15.731	Data 0.100	Loss 0.819	Prec@1 80.6500	Prec@5 95.3100	
Best Prec@1: [80.830]	
Starting epoch number: 191 Learning rate: 0.010000000000000002
Train: [191]	Time 241.252	Data 0.274	Loss 0.008	Prec@1 99.9640	Prec@5 100.0000	
Val: [191]	Time 15.660	Data 0.103	Loss 0.822	Prec@1 80.5100	Prec@5 95.1800	
Best Prec@1: [80.830]	
Starting epoch number: 192 Learning rate: 0.010000000000000002
Train: [192]	Time 242.038	Data 0.281	Loss 0.008	Prec@1 99.9620	Prec@5 100.0000	
Val: [192]	Time 15.559	Data 0.110	Loss 0.820	Prec@1 80.6000	Prec@5 95.0300	
Best Prec@1: [80.830]	
Starting epoch number: 193 Learning rate: 0.010000000000000002
Train: [193]	Time 242.261	Data 0.258	Loss 0.007	Prec@1 99.9640	Prec@5 100.0000	
Val: [193]	Time 15.562	Data 0.100	Loss 0.812	Prec@1 80.6800	Prec@5 95.1300	
Best Prec@1: [80.830]	
Starting epoch number: 194 Learning rate: 0.010000000000000002
Train: [194]	Time 241.769	Data 0.250	Loss 0.007	Prec@1 99.9800	Prec@5 100.0000	
Val: [194]	Time 15.450	Data 0.101	Loss 0.811	Prec@1 80.7200	Prec@5 95.1300	
Best Prec@1: [80.830]	
Starting epoch number: 195 Learning rate: 0.010000000000000002
Train: [195]	Time 242.280	Data 0.256	Loss 0.007	Prec@1 99.9660	Prec@5 100.0000	
Val: [195]	Time 15.676	Data 0.108	Loss 0.814	Prec@1 80.6300	Prec@5 95.1100	
Best Prec@1: [80.830]	
Starting epoch number: 196 Learning rate: 0.010000000000000002
Train: [196]	Time 241.200	Data 0.260	Loss 0.007	Prec@1 99.9900	Prec@5 100.0000	
Val: [196]	Time 15.596	Data 0.108	Loss 0.808	Prec@1 80.7800	Prec@5 95.2200	
Best Prec@1: [80.830]	
Starting epoch number: 197 Learning rate: 0.010000000000000002
Train: [197]	Time 240.760	Data 0.261	Loss 0.007	Prec@1 99.9740	Prec@5 100.0000	
Val: [197]	Time 15.562	Data 0.107	Loss 0.810	Prec@1 80.6200	Prec@5 95.1300	
Best Prec@1: [80.830]	
Starting epoch number: 198 Learning rate: 0.010000000000000002
Train: [198]	Time 241.328	Data 0.271	Loss 0.007	Prec@1 99.9680	Prec@5 100.0000	
Val: [198]	Time 15.750	Data 0.102	Loss 0.816	Prec@1 80.8600	Prec@5 95.3000	
Best Prec@1: [80.860]	
Starting epoch number: 199 Learning rate: 0.010000000000000002
Train: [199]	Time 241.383	Data 0.262	Loss 0.007	Prec@1 99.9700	Prec@5 100.0000	
Val: [199]	Time 15.541	Data 0.103	Loss 0.810	Prec@1 80.7800	Prec@5 95.1500	
Best Prec@1: [80.860]	
Starting epoch number: 200 Learning rate: 0.010000000000000002
Train: [200]	Time 241.139	Data 0.258	Loss 0.007	Prec@1 99.9780	Prec@5 100.0000	
Val: [200]	Time 15.715	Data 0.101	Loss 0.812	Prec@1 80.6900	Prec@5 95.2000	
Best Prec@1: [80.860]	
Starting epoch number: 201 Learning rate: 0.010000000000000002
Train: [201]	Time 240.897	Data 0.271	Loss 0.007	Prec@1 99.9700	Prec@5 100.0000	
Val: [201]	Time 15.738	Data 0.108	Loss 0.809	Prec@1 80.7100	Prec@5 95.0700	
Best Prec@1: [80.860]	
Starting epoch number: 202 Learning rate: 0.010000000000000002
Train: [202]	Time 241.373	Data 0.257	Loss 0.007	Prec@1 99.9620	Prec@5 100.0000	
Val: [202]	Time 15.635	Data 0.098	Loss 0.811	Prec@1 80.6100	Prec@5 95.2900	
Best Prec@1: [80.860]	
Starting epoch number: 203 Learning rate: 0.010000000000000002
Train: [203]	Time 239.359	Data 0.254	Loss 0.007	Prec@1 99.9720	Prec@5 100.0000	
Val: [203]	Time 15.487	Data 0.093	Loss 0.809	Prec@1 80.7500	Prec@5 95.2000	
Best Prec@1: [80.860]	
Starting epoch number: 204 Learning rate: 0.010000000000000002
Train: [204]	Time 240.710	Data 0.264	Loss 0.007	Prec@1 99.9680	Prec@5 100.0000	
Val: [204]	Time 15.566	Data 0.100	Loss 0.811	Prec@1 80.6200	Prec@5 95.2600	
Best Prec@1: [80.860]	
Starting epoch number: 205 Learning rate: 0.010000000000000002
Train: [205]	Time 241.124	Data 0.250	Loss 0.007	Prec@1 99.9760	Prec@5 100.0000	
Val: [205]	Time 15.737	Data 0.102	Loss 0.810	Prec@1 80.7400	Prec@5 95.1600	
Best Prec@1: [80.860]	
Starting epoch number: 206 Learning rate: 0.010000000000000002
Train: [206]	Time 241.025	Data 0.273	Loss 0.007	Prec@1 99.9680	Prec@5 100.0000	
Val: [206]	Time 15.611	Data 0.117	Loss 0.803	Prec@1 80.6500	Prec@5 95.1100	
Best Prec@1: [80.860]	
Starting epoch number: 207 Learning rate: 0.010000000000000002
Train: [207]	Time 241.089	Data 0.258	Loss 0.007	Prec@1 99.9680	Prec@5 100.0000	
Val: [207]	Time 15.523	Data 0.102	Loss 0.814	Prec@1 80.5700	Prec@5 94.9200	
Best Prec@1: [80.860]	
Starting epoch number: 208 Learning rate: 0.010000000000000002
Train: [208]	Time 241.112	Data 0.259	Loss 0.007	Prec@1 99.9720	Prec@5 100.0000	
Val: [208]	Time 15.649	Data 0.096	Loss 0.807	Prec@1 80.7200	Prec@5 95.2900	
Best Prec@1: [80.860]	
Starting epoch number: 209 Learning rate: 0.010000000000000002
Train: [209]	Time 241.559	Data 0.261	Loss 0.007	Prec@1 99.9740	Prec@5 100.0000	
Val: [209]	Time 15.737	Data 0.101	Loss 0.805	Prec@1 80.6200	Prec@5 95.2600	
Best Prec@1: [80.860]	
Starting epoch number: 210 Learning rate: 0.010000000000000002
Train: [210]	Time 240.899	Data 0.261	Loss 0.007	Prec@1 99.9660	Prec@5 100.0000	
Val: [210]	Time 15.569	Data 0.098	Loss 0.807	Prec@1 80.6000	Prec@5 95.2800	
Best Prec@1: [80.860]	
Starting epoch number: 211 Learning rate: 0.010000000000000002
Train: [211]	Time 241.038	Data 0.273	Loss 0.007	Prec@1 99.9720	Prec@5 100.0000	
Val: [211]	Time 15.776	Data 0.118	Loss 0.811	Prec@1 80.4800	Prec@5 95.2500	
Best Prec@1: [80.860]	
Starting epoch number: 212 Learning rate: 0.010000000000000002
Train: [212]	Time 240.877	Data 0.257	Loss 0.007	Prec@1 99.9680	Prec@5 100.0000	
Val: [212]	Time 15.513	Data 0.108	Loss 0.804	Prec@1 80.6400	Prec@5 95.1900	
Best Prec@1: [80.860]	
Starting epoch number: 213 Learning rate: 0.010000000000000002
Train: [213]	Time 240.239	Data 0.260	Loss 0.007	Prec@1 99.9640	Prec@5 100.0000	
Val: [213]	Time 15.436	Data 0.115	Loss 0.802	Prec@1 80.5500	Prec@5 95.2800	
Best Prec@1: [80.860]	
Starting epoch number: 214 Learning rate: 0.010000000000000002
Train: [214]	Time 240.265	Data 0.272	Loss 0.006	Prec@1 99.9780	Prec@5 100.0000	
Val: [214]	Time 15.584	Data 0.123	Loss 0.802	Prec@1 80.6800	Prec@5 95.2400	
Best Prec@1: [80.860]	
Starting epoch number: 215 Learning rate: 0.010000000000000002
Train: [215]	Time 240.469	Data 0.270	Loss 0.007	Prec@1 99.9660	Prec@5 100.0000	
Val: [215]	Time 15.722	Data 0.095	Loss 0.812	Prec@1 80.5500	Prec@5 95.1600	
Best Prec@1: [80.860]	
Starting epoch number: 216 Learning rate: 0.010000000000000002
Train: [216]	Time 240.759	Data 0.263	Loss 0.006	Prec@1 99.9720	Prec@5 100.0000	
Val: [216]	Time 15.646	Data 0.119	Loss 0.819	Prec@1 80.3800	Prec@5 95.1100	
Best Prec@1: [80.860]	
Starting epoch number: 217 Learning rate: 0.010000000000000002
Train: [217]	Time 239.571	Data 0.271	Loss 0.007	Prec@1 99.9660	Prec@5 100.0000	
Val: [217]	Time 15.677	Data 0.122	Loss 0.812	Prec@1 80.6200	Prec@5 95.2500	
Best Prec@1: [80.860]	
Starting epoch number: 218 Learning rate: 0.010000000000000002
Train: [218]	Time 240.559	Data 0.278	Loss 0.007	Prec@1 99.9720	Prec@5 100.0000	
Val: [218]	Time 15.587	Data 0.120	Loss 0.814	Prec@1 80.4600	Prec@5 95.1100	
Best Prec@1: [80.860]	
Starting epoch number: 219 Learning rate: 0.010000000000000002
Train: [219]	Time 240.906	Data 0.268	Loss 0.006	Prec@1 99.9740	Prec@5 100.0000	
Val: [219]	Time 15.426	Data 0.119	Loss 0.819	Prec@1 80.4100	Prec@5 95.0600	
Best Prec@1: [80.860]	
Starting epoch number: 220 Learning rate: 0.010000000000000002
Train: [220]	Time 240.753	Data 0.281	Loss 0.006	Prec@1 99.9760	Prec@5 100.0000	
Val: [220]	Time 15.567	Data 0.111	Loss 0.806	Prec@1 80.7500	Prec@5 95.0500	
Best Prec@1: [80.860]	
Starting epoch number: 221 Learning rate: 0.010000000000000002
Train: [221]	Time 240.580	Data 0.274	Loss 0.006	Prec@1 99.9680	Prec@5 100.0000	
Val: [221]	Time 15.401	Data 0.117	Loss 0.817	Prec@1 80.6300	Prec@5 95.1600	
Best Prec@1: [80.860]	
Starting epoch number: 222 Learning rate: 0.010000000000000002
Train: [222]	Time 241.283	Data 0.263	Loss 0.006	Prec@1 99.9720	Prec@5 100.0000	
Val: [222]	Time 15.631	Data 0.114	Loss 0.820	Prec@1 80.5800	Prec@5 95.1500	
Best Prec@1: [80.860]	
Starting epoch number: 223 Learning rate: 0.010000000000000002
Train: [223]	Time 241.181	Data 0.271	Loss 0.006	Prec@1 99.9800	Prec@5 100.0000	
Val: [223]	Time 15.635	Data 0.105	Loss 0.815	Prec@1 80.4000	Prec@5 95.0600	
Best Prec@1: [80.860]	
Starting epoch number: 224 Learning rate: 0.010000000000000002
Train: [224]	Time 240.627	Data 0.270	Loss 0.006	Prec@1 99.9760	Prec@5 100.0000	
Val: [224]	Time 15.322	Data 0.096	Loss 0.811	Prec@1 80.6100	Prec@5 95.0400	
Best Prec@1: [80.860]	
Starting epoch number: 225 Learning rate: 0.0010000000000000002
Train: [225]	Time 240.396	Data 0.269	Loss 0.006	Prec@1 99.9700	Prec@5 100.0000	
Val: [225]	Time 15.547	Data 0.102	Loss 0.811	Prec@1 80.6000	Prec@5 95.1000	
Best Prec@1: [80.860]	
Starting epoch number: 226 Learning rate: 0.0010000000000000002
Train: [226]	Time 240.509	Data 0.273	Loss 0.006	Prec@1 99.9720	Prec@5 100.0000	
Val: [226]	Time 15.779	Data 0.103	Loss 0.806	Prec@1 80.5400	Prec@5 95.1500	
Best Prec@1: [80.860]	
Starting epoch number: 227 Learning rate: 0.0010000000000000002
Train: [227]	Time 241.671	Data 0.261	Loss 0.005	Prec@1 99.9740	Prec@5 100.0000	
Val: [227]	Time 15.618	Data 0.106	Loss 0.808	Prec@1 80.5500	Prec@5 95.0000	
Best Prec@1: [80.860]	
Starting epoch number: 228 Learning rate: 0.0010000000000000002
Train: [228]	Time 242.049	Data 0.255	Loss 0.005	Prec@1 99.9800	Prec@5 100.0000	
Val: [228]	Time 15.489	Data 0.115	Loss 0.816	Prec@1 80.7600	Prec@5 95.0000	
Best Prec@1: [80.860]	
Starting epoch number: 229 Learning rate: 0.0010000000000000002
Train: [229]	Time 241.674	Data 0.259	Loss 0.005	Prec@1 99.9740	Prec@5 100.0000	
Val: [229]	Time 15.712	Data 0.103	Loss 0.813	Prec@1 80.7000	Prec@5 94.9400	
Best Prec@1: [80.860]	
Starting epoch number: 230 Learning rate: 0.0010000000000000002
Train: [230]	Time 240.589	Data 0.272	Loss 0.005	Prec@1 99.9860	Prec@5 100.0000	
Val: [230]	Time 15.641	Data 0.105	Loss 0.807	Prec@1 80.6800	Prec@5 95.1800	
Best Prec@1: [80.860]	
Starting epoch number: 231 Learning rate: 0.0010000000000000002
Train: [231]	Time 240.959	Data 0.249	Loss 0.005	Prec@1 99.9740	Prec@5 100.0000	
Val: [231]	Time 15.744	Data 0.100	Loss 0.814	Prec@1 80.5000	Prec@5 95.0600	
Best Prec@1: [80.860]	
Starting epoch number: 232 Learning rate: 0.0010000000000000002
Train: [232]	Time 241.210	Data 0.269	Loss 0.005	Prec@1 99.9840	Prec@5 100.0000	
Val: [232]	Time 15.576	Data 0.108	Loss 0.810	Prec@1 80.5800	Prec@5 95.2500	
Best Prec@1: [80.860]	
Starting epoch number: 233 Learning rate: 0.0010000000000000002
Train: [233]	Time 241.013	Data 0.248	Loss 0.005	Prec@1 99.9820	Prec@5 100.0000	
Val: [233]	Time 15.659	Data 0.118	Loss 0.808	Prec@1 80.7300	Prec@5 95.1400	
Best Prec@1: [80.860]	
Starting epoch number: 234 Learning rate: 0.0010000000000000002
Train: [234]	Time 240.461	Data 0.267	Loss 0.005	Prec@1 99.9760	Prec@5 100.0000	
Val: [234]	Time 15.452	Data 0.102	Loss 0.809	Prec@1 80.6400	Prec@5 95.0800	
Best Prec@1: [80.860]	
Starting epoch number: 235 Learning rate: 0.0010000000000000002
Train: [235]	Time 240.114	Data 0.259	Loss 0.005	Prec@1 99.9800	Prec@5 100.0000	
Val: [235]	Time 15.580	Data 0.111	Loss 0.809	Prec@1 80.9000	Prec@5 95.1700	
Best Prec@1: [80.900]	
Starting epoch number: 236 Learning rate: 0.0010000000000000002
Train: [236]	Time 241.415	Data 0.248	Loss 0.005	Prec@1 99.9720	Prec@5 100.0000	
Val: [236]	Time 15.579	Data 0.116	Loss 0.810	Prec@1 80.5700	Prec@5 95.1800	
Best Prec@1: [80.900]	
Starting epoch number: 237 Learning rate: 0.0010000000000000002
Train: [237]	Time 241.653	Data 0.257	Loss 0.005	Prec@1 99.9880	Prec@5 100.0000	
Val: [237]	Time 15.764	Data 0.111	Loss 0.806	Prec@1 80.8000	Prec@5 95.0800	
Best Prec@1: [80.900]	
Starting epoch number: 238 Learning rate: 0.0010000000000000002
Train: [238]	Time 241.683	Data 0.258	Loss 0.005	Prec@1 99.9860	Prec@5 100.0000	
Val: [238]	Time 15.419	Data 0.112	Loss 0.812	Prec@1 80.5900	Prec@5 94.9800	
Best Prec@1: [80.900]	
Starting epoch number: 239 Learning rate: 0.0010000000000000002
Train: [239]	Time 241.587	Data 0.263	Loss 0.005	Prec@1 99.9800	Prec@5 100.0000	
Val: [239]	Time 15.584	Data 0.117	Loss 0.816	Prec@1 80.7900	Prec@5 95.1100	
Best Prec@1: [80.900]	
Starting epoch number: 240 Learning rate: 0.0010000000000000002
Train: [240]	Time 241.268	Data 0.261	Loss 0.005	Prec@1 99.9760	Prec@5 100.0000	
Val: [240]	Time 15.741	Data 0.107	Loss 0.810	Prec@1 80.5700	Prec@5 95.0800	
Best Prec@1: [80.900]	
Starting epoch number: 241 Learning rate: 0.0010000000000000002
Train: [241]	Time 241.212	Data 0.260	Loss 0.005	Prec@1 99.9860	Prec@5 100.0000	
Val: [241]	Time 15.718	Data 0.112	Loss 0.804	Prec@1 80.6300	Prec@5 95.1400	
Best Prec@1: [80.900]	
Starting epoch number: 242 Learning rate: 0.0010000000000000002
Train: [242]	Time 241.425	Data 0.275	Loss 0.005	Prec@1 99.9780	Prec@5 100.0000	
Val: [242]	Time 15.465	Data 0.111	Loss 0.815	Prec@1 80.7300	Prec@5 95.0400	
Best Prec@1: [80.900]	
Starting epoch number: 243 Learning rate: 0.0010000000000000002
Train: [243]	Time 241.252	Data 0.256	Loss 0.005	Prec@1 99.9760	Prec@5 100.0000	
Val: [243]	Time 15.472	Data 0.116	Loss 0.811	Prec@1 80.6600	Prec@5 95.0700	
Best Prec@1: [80.900]	
Starting epoch number: 244 Learning rate: 0.0010000000000000002
Train: [244]	Time 242.309	Data 0.260	Loss 0.005	Prec@1 99.9760	Prec@5 100.0000	
Val: [244]	Time 15.705	Data 0.102	Loss 0.811	Prec@1 80.6300	Prec@5 95.0400	
Best Prec@1: [80.900]	
Starting epoch number: 245 Learning rate: 0.0010000000000000002
Train: [245]	Time 242.319	Data 0.260	Loss 0.005	Prec@1 99.9800	Prec@5 100.0000	
Val: [245]	Time 15.559	Data 0.097	Loss 0.813	Prec@1 80.7000	Prec@5 95.2200	
Best Prec@1: [80.900]	
Starting epoch number: 246 Learning rate: 0.0010000000000000002
Train: [246]	Time 241.749	Data 0.266	Loss 0.005	Prec@1 99.9700	Prec@5 100.0000	
Val: [246]	Time 15.473	Data 0.109	Loss 0.809	Prec@1 80.5700	Prec@5 95.0700	
Best Prec@1: [80.900]	
Starting epoch number: 247 Learning rate: 0.0010000000000000002
Train: [247]	Time 240.834	Data 0.282	Loss 0.005	Prec@1 99.9780	Prec@5 100.0000	
Val: [247]	Time 15.512	Data 0.126	Loss 0.811	Prec@1 80.7700	Prec@5 95.0000	
Best Prec@1: [80.900]	
Starting epoch number: 248 Learning rate: 0.0010000000000000002
Train: [248]	Time 241.100	Data 0.260	Loss 0.005	Prec@1 99.9740	Prec@5 100.0000	
Val: [248]	Time 15.421	Data 0.093	Loss 0.819	Prec@1 80.4800	Prec@5 95.0700	
Best Prec@1: [80.900]	
Starting epoch number: 249 Learning rate: 0.0010000000000000002
Train: [249]	Time 241.379	Data 0.281	Loss 0.005	Prec@1 99.9800	Prec@5 100.0000	
Val: [249]	Time 15.579	Data 0.122	Loss 0.812	Prec@1 80.4900	Prec@5 95.0200	
Best Prec@1: [80.900]	
Starting epoch number: 250 Learning rate: 0.0010000000000000002
Train: [250]	Time 242.654	Data 0.266	Loss 0.005	Prec@1 99.9840	Prec@5 100.0000	
Val: [250]	Time 15.494	Data 0.125	Loss 0.813	Prec@1 80.6700	Prec@5 94.9800	
Best Prec@1: [80.900]	
Starting epoch number: 251 Learning rate: 0.0010000000000000002
Train: [251]	Time 241.562	Data 0.251	Loss 0.005	Prec@1 99.9720	Prec@5 100.0000	
Val: [251]	Time 15.677	Data 0.114	Loss 0.811	Prec@1 80.8300	Prec@5 95.2300	
Best Prec@1: [80.900]	
Starting epoch number: 252 Learning rate: 0.0010000000000000002
Train: [252]	Time 241.300	Data 0.254	Loss 0.005	Prec@1 99.9820	Prec@5 100.0000	
Val: [252]	Time 15.503	Data 0.103	Loss 0.811	Prec@1 80.6000	Prec@5 95.0500	
Best Prec@1: [80.900]	
Starting epoch number: 253 Learning rate: 0.0010000000000000002
Train: [253]	Time 242.351	Data 0.250	Loss 0.005	Prec@1 99.9840	Prec@5 100.0000	
Val: [253]	Time 15.648	Data 0.107	Loss 0.812	Prec@1 80.6300	Prec@5 95.0500	
Best Prec@1: [80.900]	
Starting epoch number: 254 Learning rate: 0.0010000000000000002
Train: [254]	Time 241.730	Data 0.272	Loss 0.005	Prec@1 99.9800	Prec@5 100.0000	
Val: [254]	Time 15.691	Data 0.116	Loss 0.807	Prec@1 80.7500	Prec@5 95.0600	
Best Prec@1: [80.900]	
Starting epoch number: 255 Learning rate: 0.0010000000000000002
Train: [255]	Time 240.869	Data 0.250	Loss 0.005	Prec@1 99.9780	Prec@5 100.0000	
Val: [255]	Time 15.780	Data 0.116	Loss 0.809	Prec@1 80.7100	Prec@5 95.0700	
Best Prec@1: [80.900]	
Starting epoch number: 256 Learning rate: 0.0010000000000000002
Train: [256]	Time 240.748	Data 0.254	Loss 0.005	Prec@1 99.9780	Prec@5 100.0000	
Val: [256]	Time 15.434	Data 0.182	Loss 0.809	Prec@1 80.6300	Prec@5 95.0500	
Best Prec@1: [80.900]	
Starting epoch number: 257 Learning rate: 0.0010000000000000002
Train: [257]	Time 241.770	Data 0.265	Loss 0.005	Prec@1 99.9740	Prec@5 100.0000	
Val: [257]	Time 15.582	Data 0.110	Loss 0.813	Prec@1 80.7000	Prec@5 95.0500	
Best Prec@1: [80.900]	
Starting epoch number: 258 Learning rate: 0.0010000000000000002
Train: [258]	Time 242.106	Data 0.268	Loss 0.005	Prec@1 99.9860	Prec@5 100.0000	
Val: [258]	Time 15.674	Data 0.104	Loss 0.811	Prec@1 80.7300	Prec@5 95.0200	
Best Prec@1: [80.900]	
Starting epoch number: 259 Learning rate: 0.0010000000000000002
Train: [259]	Time 242.004	Data 0.275	Loss 0.005	Prec@1 99.9840	Prec@5 100.0000	
Val: [259]	Time 15.775	Data 0.095	Loss 0.810	Prec@1 80.5900	Prec@5 94.9600	
Best Prec@1: [80.900]	
Starting epoch number: 260 Learning rate: 0.0010000000000000002
Train: [260]	Time 240.758	Data 0.261	Loss 0.005	Prec@1 99.9900	Prec@5 100.0000	
Val: [260]	Time 15.549	Data 0.109	Loss 0.814	Prec@1 80.5600	Prec@5 95.1100	
Best Prec@1: [80.900]	
Starting epoch number: 261 Learning rate: 0.0010000000000000002
Train: [261]	Time 241.118	Data 0.266	Loss 0.005	Prec@1 99.9780	Prec@5 100.0000	
Val: [261]	Time 15.786	Data 0.120	Loss 0.810	Prec@1 80.6400	Prec@5 95.0800	
Best Prec@1: [80.900]	
Starting epoch number: 262 Learning rate: 0.0010000000000000002
Train: [262]	Time 241.235	Data 0.259	Loss 0.005	Prec@1 99.9800	Prec@5 100.0000	
Val: [262]	Time 15.650	Data 0.104	Loss 0.808	Prec@1 80.7700	Prec@5 95.0200	
Best Prec@1: [80.900]	
Starting epoch number: 263 Learning rate: 0.0010000000000000002
Train: [263]	Time 240.323	Data 0.248	Loss 0.005	Prec@1 99.9780	Prec@5 100.0000	
Val: [263]	Time 15.550	Data 0.102	Loss 0.811	Prec@1 80.4900	Prec@5 95.0100	
Best Prec@1: [80.900]	
Starting epoch number: 264 Learning rate: 0.0010000000000000002
Train: [264]	Time 241.039	Data 0.276	Loss 0.005	Prec@1 99.9840	Prec@5 100.0000	
Val: [264]	Time 15.541	Data 0.106	Loss 0.813	Prec@1 80.6600	Prec@5 95.1100	
Best Prec@1: [80.900]	
Starting epoch number: 265 Learning rate: 0.0010000000000000002
Train: [265]	Time 241.849	Data 0.281	Loss 0.005	Prec@1 99.9860	Prec@5 100.0000	
Val: [265]	Time 15.644	Data 0.106	Loss 0.815	Prec@1 80.7500	Prec@5 95.0800	
Best Prec@1: [80.900]	
Starting epoch number: 266 Learning rate: 0.0010000000000000002
Train: [266]	Time 241.973	Data 0.296	Loss 0.005	Prec@1 99.9840	Prec@5 100.0000	
Val: [266]	Time 15.642	Data 0.114	Loss 0.809	Prec@1 80.7800	Prec@5 95.0300	
Best Prec@1: [80.900]	
Starting epoch number: 267 Learning rate: 0.0010000000000000002
Train: [267]	Time 242.241	Data 0.258	Loss 0.005	Prec@1 99.9820	Prec@5 100.0000	
Val: [267]	Time 15.609	Data 0.090	Loss 0.813	Prec@1 80.6000	Prec@5 94.9900	
Best Prec@1: [80.900]	
Starting epoch number: 268 Learning rate: 0.0010000000000000002
Train: [268]	Time 241.766	Data 0.251	Loss 0.005	Prec@1 99.9740	Prec@5 100.0000	
Val: [268]	Time 15.586	Data 0.119	Loss 0.815	Prec@1 80.7100	Prec@5 95.0500	
Best Prec@1: [80.900]	
Starting epoch number: 269 Learning rate: 0.0010000000000000002
Train: [269]	Time 241.526	Data 0.255	Loss 0.005	Prec@1 99.9740	Prec@5 100.0000	
Val: [269]	Time 15.653	Data 0.116	Loss 0.814	Prec@1 80.6300	Prec@5 95.0200	
Best Prec@1: [80.900]	
Starting epoch number: 270 Learning rate: 0.0010000000000000002
Train: [270]	Time 241.106	Data 0.263	Loss 0.005	Prec@1 99.9840	Prec@5 100.0000	
Val: [270]	Time 15.175	Data 0.117	Loss 0.811	Prec@1 80.7100	Prec@5 95.0900	
Best Prec@1: [80.900]	
Starting epoch number: 271 Learning rate: 0.0010000000000000002
Train: [271]	Time 241.323	Data 0.263	Loss 0.005	Prec@1 99.9820	Prec@5 100.0000	
Val: [271]	Time 15.638	Data 0.097	Loss 0.814	Prec@1 80.6500	Prec@5 95.1100	
Best Prec@1: [80.900]	
Starting epoch number: 272 Learning rate: 0.0010000000000000002
Train: [272]	Time 241.236	Data 0.260	Loss 0.005	Prec@1 99.9760	Prec@5 100.0000	
Val: [272]	Time 15.475	Data 0.112	Loss 0.812	Prec@1 80.6900	Prec@5 95.0900	
Best Prec@1: [80.900]	
Starting epoch number: 273 Learning rate: 0.0010000000000000002
Train: [273]	Time 241.060	Data 0.283	Loss 0.005	Prec@1 99.9780	Prec@5 100.0000	
Val: [273]	Time 15.620	Data 0.126	Loss 0.810	Prec@1 80.6500	Prec@5 95.0700	
Best Prec@1: [80.900]	
Starting epoch number: 274 Learning rate: 0.0010000000000000002
Train: [274]	Time 242.214	Data 0.276	Loss 0.005	Prec@1 99.9860	Prec@5 100.0000	
Val: [274]	Time 15.567	Data 0.113	Loss 0.816	Prec@1 80.5700	Prec@5 95.0300	
Best Prec@1: [80.900]	
Starting epoch number: 275 Learning rate: 0.0010000000000000002
Train: [275]	Time 242.741	Data 0.275	Loss 0.005	Prec@1 99.9780	Prec@5 100.0000	
Val: [275]	Time 15.788	Data 0.112	Loss 0.813	Prec@1 80.5900	Prec@5 94.9100	
Best Prec@1: [80.900]	
Starting epoch number: 276 Learning rate: 0.0010000000000000002
Train: [276]	Time 241.415	Data 0.248	Loss 0.005	Prec@1 99.9760	Prec@5 100.0000	
Val: [276]	Time 15.678	Data 0.104	Loss 0.813	Prec@1 80.6800	Prec@5 95.1000	
Best Prec@1: [80.900]	
Starting epoch number: 277 Learning rate: 0.0010000000000000002
Train: [277]	Time 241.483	Data 0.256	Loss 0.005	Prec@1 99.9860	Prec@5 100.0000	
Val: [277]	Time 15.566	Data 0.122	Loss 0.816	Prec@1 80.5300	Prec@5 95.0600	
Best Prec@1: [80.900]	
Starting epoch number: 278 Learning rate: 0.0010000000000000002
Train: [278]	Time 241.656	Data 0.272	Loss 0.005	Prec@1 99.9660	Prec@5 100.0000	
Val: [278]	Time 15.527	Data 0.107	Loss 0.818	Prec@1 80.6900	Prec@5 95.0700	
Best Prec@1: [80.900]	
Starting epoch number: 279 Learning rate: 0.0010000000000000002
Train: [279]	Time 241.657	Data 0.289	Loss 0.005	Prec@1 99.9780	Prec@5 100.0000	
Val: [279]	Time 15.407	Data 0.112	Loss 0.813	Prec@1 80.6200	Prec@5 94.9500	
Best Prec@1: [80.900]	
Starting epoch number: 280 Learning rate: 0.0010000000000000002
Train: [280]	Time 242.416	Data 0.269	Loss 0.005	Prec@1 99.9780	Prec@5 100.0000	
Val: [280]	Time 15.760	Data 0.091	Loss 0.813	Prec@1 80.5300	Prec@5 95.0100	
Best Prec@1: [80.900]	
Starting epoch number: 281 Learning rate: 0.0010000000000000002
Train: [281]	Time 241.086	Data 0.257	Loss 0.005	Prec@1 99.9820	Prec@5 100.0000	
Val: [281]	Time 15.595	Data 0.097	Loss 0.814	Prec@1 80.4600	Prec@5 95.0400	
Best Prec@1: [80.900]	
Starting epoch number: 282 Learning rate: 0.0010000000000000002
Train: [282]	Time 242.135	Data 0.254	Loss 0.005	Prec@1 99.9780	Prec@5 100.0000	
Val: [282]	Time 15.621	Data 0.100	Loss 0.812	Prec@1 80.6800	Prec@5 95.0300	
Best Prec@1: [80.900]	
Starting epoch number: 283 Learning rate: 0.0010000000000000002
Train: [283]	Time 242.285	Data 0.257	Loss 0.005	Prec@1 99.9800	Prec@5 100.0000	
Val: [283]	Time 15.685	Data 0.107	Loss 0.814	Prec@1 80.5700	Prec@5 95.0900	
Best Prec@1: [80.900]	
Starting epoch number: 284 Learning rate: 0.0010000000000000002
Train: [284]	Time 241.635	Data 0.268	Loss 0.005	Prec@1 99.9760	Prec@5 100.0000	
Val: [284]	Time 15.598	Data 0.100	Loss 0.814	Prec@1 80.5900	Prec@5 95.0400	
Best Prec@1: [80.900]	
Starting epoch number: 285 Learning rate: 0.0010000000000000002
Train: [285]	Time 241.721	Data 0.257	Loss 0.005	Prec@1 99.9740	Prec@5 100.0000	
Val: [285]	Time 15.620	Data 0.102	Loss 0.811	Prec@1 80.7000	Prec@5 94.9400	
Best Prec@1: [80.900]	
Starting epoch number: 286 Learning rate: 0.0010000000000000002
Train: [286]	Time 242.369	Data 0.312	Loss 0.005	Prec@1 99.9880	Prec@5 100.0000	
Val: [286]	Time 15.691	Data 0.121	Loss 0.813	Prec@1 80.6900	Prec@5 95.1100	
Best Prec@1: [80.900]	
Starting epoch number: 287 Learning rate: 0.0010000000000000002
Train: [287]	Time 242.383	Data 0.270	Loss 0.005	Prec@1 99.9820	Prec@5 100.0000	
Val: [287]	Time 15.702	Data 0.119	Loss 0.812	Prec@1 80.4800	Prec@5 94.9800	
Best Prec@1: [80.900]	
Starting epoch number: 288 Learning rate: 0.0010000000000000002
Train: [288]	Time 242.007	Data 0.251	Loss 0.005	Prec@1 99.9880	Prec@5 100.0000	
Val: [288]	Time 15.698	Data 0.095	Loss 0.813	Prec@1 80.6100	Prec@5 95.0400	
Best Prec@1: [80.900]	
Starting epoch number: 289 Learning rate: 0.0010000000000000002
Train: [289]	Time 241.973	Data 0.259	Loss 0.005	Prec@1 99.9840	Prec@5 100.0000	
Val: [289]	Time 15.500	Data 0.101	Loss 0.809	Prec@1 80.6600	Prec@5 95.0100	
Best Prec@1: [80.900]	
Starting epoch number: 290 Learning rate: 0.0010000000000000002
Train: [290]	Time 242.476	Data 0.289	Loss 0.005	Prec@1 99.9760	Prec@5 100.0000	
Val: [290]	Time 15.735	Data 0.110	Loss 0.815	Prec@1 80.4900	Prec@5 94.9400	
Best Prec@1: [80.900]	
Starting epoch number: 291 Learning rate: 0.0010000000000000002
Train: [291]	Time 241.540	Data 0.256	Loss 0.005	Prec@1 99.9760	Prec@5 100.0000	
Val: [291]	Time 15.466	Data 0.111	Loss 0.819	Prec@1 80.6500	Prec@5 94.9700	
Best Prec@1: [80.900]	
Starting epoch number: 292 Learning rate: 0.0010000000000000002
Train: [292]	Time 241.384	Data 0.264	Loss 0.005	Prec@1 99.9820	Prec@5 100.0000	
Val: [292]	Time 15.555	Data 0.119	Loss 0.813	Prec@1 80.5000	Prec@5 95.0400	
Best Prec@1: [80.900]	
Starting epoch number: 293 Learning rate: 0.0010000000000000002
Train: [293]	Time 241.575	Data 0.273	Loss 0.005	Prec@1 99.9800	Prec@5 100.0000	
Val: [293]	Time 15.718	Data 0.120	Loss 0.811	Prec@1 80.6400	Prec@5 95.0300	
Best Prec@1: [80.900]	
Starting epoch number: 294 Learning rate: 0.0010000000000000002
