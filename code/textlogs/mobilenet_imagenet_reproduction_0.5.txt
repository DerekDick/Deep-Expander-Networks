Namespace(acc_type='class', augment=True, batch_size=256, bottleneck=True, cachemode=True, cardinality=8, criterion='crossentropy', cuda=True, data_dir='/ssd_scratch/cvit/Imagenet12/', dataset='imagenet12', decayinterval=15, decaylevel=0.316227766, droprate=0, epochs=150, evaluate=False, exp=1, expandConfig=None, expandSize=2, from_modelzoo=False, grouptype='full', growth=48, inpsize=224, layers=100, learningratescheduler='decayschedular', logdir='../logs/mobilenet_imagenet_reproduction_0.5', lr=0.1, manualSeed=123, maxlr=0.1, minlr=5e-05, model_def='mobilenet', momentum=0.9, name='mobilenet_imagenet_reproduction_0.5', nclasses=1000, nesterov=False, ngpus=2, optimType='sgd', pretrained=False, pretrained_file='', printfreq=200, reduce=0.5, resume='', sp=1, start_epoch=0, store='', tenCrop=False, tensorboard=False, testOnly=False, verbose=False, weightDecay=4e-05, weight_init=False, widen_factor=4, widthmult=0.5, workers=8)
DataParallel(
  (module): Net(
    (model): Sequential(
      (0): Sequential(
        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (2): ReLU(inplace)
      )
      (1): Sequential(
        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (2): ReLU6(inplace)
        (3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (5): ReLU6(inplace)
      )
      (2): Sequential(
        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (2): ReLU6(inplace)
        (3): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
        (5): ReLU6(inplace)
      )
      (3): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
        (2): ReLU6(inplace)
        (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
        (5): ReLU6(inplace)
      )
      (4): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
        (2): ReLU6(inplace)
        (3): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
        (5): ReLU6(inplace)
      )
      (5): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
        (2): ReLU6(inplace)
        (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
        (5): ReLU6(inplace)
      )
      (6): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
        (2): ReLU6(inplace)
        (3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (5): ReLU6(inplace)
      )
      (7): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (2): ReLU6(inplace)
        (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (5): ReLU6(inplace)
      )
      (8): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (2): ReLU6(inplace)
        (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (5): ReLU6(inplace)
      )
      (9): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (2): ReLU6(inplace)
        (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (5): ReLU6(inplace)
      )
      (10): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (2): ReLU6(inplace)
        (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (5): ReLU6(inplace)
      )
      (11): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (2): ReLU6(inplace)
        (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (5): ReLU6(inplace)
      )
      (12): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (2): ReLU6(inplace)
        (3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
        (5): ReLU6(inplace)
      )
      (13): Sequential(
        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
        (2): ReLU6(inplace)
        (3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
        (5): ReLU6(inplace)
      )
      (14): AvgPool2d(kernel_size=7, stride=7, padding=0, ceil_mode=False, count_include_pad=True)
    )
    (fc): Linear(in_features=1024, out_features=1000, bias=True)
  )
)
('Starting epoch number:', 0, 'Learning rate:', 0.1)
Train: [0]	Time 2358.591	Data 789.423	Loss 5.069	Prec@1 9.6279	Prec@5 23.6753	
Val: [0]	Time 90.839	Data 71.991	Loss 4.036	Prec@1 19.1960	Prec@5 41.4500	
Best Prec@1: [19.196]	
('Starting epoch number:', 1, 'Learning rate:', 0.1)
Train: [1]	Time 2430.552	Data 853.384	Loss 3.798	Prec@1 23.7353	Prec@5 46.6337	
Val: [1]	Time 92.283	Data 73.517	Loss 3.424	Prec@1 27.9400	Prec@5 53.2560	
Best Prec@1: [27.940]	
('Starting epoch number:', 2, 'Learning rate:', 0.1)
Train: [2]	Time 2488.309	Data 916.381	Loss 3.346	Prec@1 30.7026	Prec@5 55.0905	
Val: [2]	Time 85.507	Data 67.124	Loss 3.072	Prec@1 34.3740	Prec@5 59.8900	
Best Prec@1: [34.374]	
('Starting epoch number:', 3, 'Learning rate:', 0.1)
Train: [3]	Time 2386.001	Data 822.151	Loss 3.104	Prec@1 34.7577	Prec@5 59.4979	
Val: [3]	Time 85.465	Data 66.894	Loss 2.839	Prec@1 38.1060	Prec@5 64.2500	
Best Prec@1: [38.106]	
('Starting epoch number:', 4, 'Learning rate:', 0.1)
Train: [4]	Time 2513.794	Data 952.732	Loss 2.954	Prec@1 37.2430	Prec@5 62.0912	
Val: [4]	Time 85.074	Data 66.680	Loss 2.716	Prec@1 40.1120	Prec@5 66.2100	
Best Prec@1: [40.112]	
('Starting epoch number:', 5, 'Learning rate:', 0.1)
Train: [5]	Time 2304.407	Data 733.703	Loss 2.849	Prec@1 39.0714	Prec@5 63.9251	
Val: [5]	Time 86.375	Data 68.005	Loss 2.698	Prec@1 40.5820	Prec@5 66.6720	
Best Prec@1: [40.582]	
('Starting epoch number:', 6, 'Learning rate:', 0.1)
Train: [6]	Time 2389.961	Data 815.107	Loss 2.774	Prec@1 40.3946	Prec@5 65.1971	
Val: [6]	Time 94.846	Data 75.991	Loss 2.594	Prec@1 42.1760	Prec@5 68.5400	
Best Prec@1: [42.176]	
('Starting epoch number:', 7, 'Learning rate:', 0.1)
Train: [7]	Time 2389.304	Data 824.235	Loss 2.712	Prec@1 41.4981	Prec@5 66.2655	
Val: [7]	Time 86.047	Data 67.562	Loss 2.495	Prec@1 43.8480	Prec@5 70.0140	
Best Prec@1: [43.848]	
('Starting epoch number:', 8, 'Learning rate:', 0.1)
Train: [8]	Time 2380.104	Data 816.537	Loss 2.668	Prec@1 42.2777	Prec@5 66.9943	
Val: [8]	Time 84.825	Data 66.402	Loss 2.515	Prec@1 43.7640	Prec@5 69.7120	
Best Prec@1: [43.848]	
('Starting epoch number:', 9, 'Learning rate:', 0.1)
Train: [9]	Time 2367.116	Data 802.535	Loss 2.627	Prec@1 43.0232	Prec@5 67.6830	
Val: [9]	Time 85.765	Data 67.434	Loss 2.417	Prec@1 45.6360	Prec@5 71.3060	
Best Prec@1: [45.636]	
('Starting epoch number:', 10, 'Learning rate:', 0.1)
Train: [10]	Time 2367.718	Data 804.056	Loss 2.595	Prec@1 43.6133	Prec@5 68.1696	
Val: [10]	Time 83.176	Data 64.854	Loss 2.437	Prec@1 45.2740	Prec@5 71.2680	
Best Prec@1: [45.636]	
('Starting epoch number:', 11, 'Learning rate:', 0.1)
Train: [11]	Time 2362.340	Data 799.671	Loss 2.565	Prec@1 44.1674	Prec@5 68.6915	
Val: [11]	Time 88.608	Data 69.634	Loss 2.432	Prec@1 45.2400	Prec@5 71.2300	
Best Prec@1: [45.636]	
('Starting epoch number:', 12, 'Learning rate:', 0.1)
Train: [12]	Time 2382.068	Data 818.390	Loss 2.545	Prec@1 44.5791	Prec@5 69.0701	
Val: [12]	Time 86.394	Data 68.009	Loss 2.312	Prec@1 47.3060	Prec@5 73.0700	
Best Prec@1: [47.306]	
('Starting epoch number:', 13, 'Learning rate:', 0.1)
Train: [13]	Time 2375.358	Data 812.347	Loss 2.526	Prec@1 44.8562	Prec@5 69.3464	
Val: [13]	Time 85.675	Data 67.344	Loss 2.398	Prec@1 45.9000	Prec@5 71.8440	
Best Prec@1: [47.306]	
('Starting epoch number:', 14, 'Learning rate:', 0.1)
Train: [14]	Time 2371.206	Data 808.198	Loss 2.507	Prec@1 45.2043	Prec@5 69.7061	
Val: [14]	Time 88.051	Data 69.464	Loss 2.309	Prec@1 47.5520	Prec@5 73.2240	
Best Prec@1: [47.552]	
('Starting epoch number:', 15, 'Learning rate:', 0.031622776600000006)
Train: [15]	Time 2371.762	Data 798.983	Loss 2.197	Prec@1 51.2785	Prec@5 74.5432	
Val: [15]	Time 86.770	Data 68.447	Loss 1.948	Prec@1 54.5360	Prec@5 78.7320	
Best Prec@1: [54.536]	
('Starting epoch number:', 16, 'Learning rate:', 0.031622776600000006)
Train: [16]	Time 2349.607	Data 778.578	Loss 2.154	Prec@1 52.0446	Prec@5 75.2208	
Val: [16]	Time 87.715	Data 69.317	Loss 1.960	Prec@1 54.4060	Prec@5 78.5820	
Best Prec@1: [54.536]	
('Starting epoch number:', 17, 'Learning rate:', 0.031622776600000006)
Train: [17]	Time 2400.615	Data 826.620	Loss 2.147	Prec@1 52.1586	Prec@5 75.3384	
Val: [17]	Time 85.490	Data 67.043	Loss 1.903	Prec@1 55.4760	Prec@5 79.3640	
Best Prec@1: [55.476]	
('Starting epoch number:', 18, 'Learning rate:', 0.031622776600000006)
Train: [18]	Time 2392.535	Data 833.129	Loss 2.145	Prec@1 52.1677	Prec@5 75.3394	
Val: [18]	Time 84.642	Data 66.275	Loss 1.922	Prec@1 55.4200	Prec@5 79.1440	
Best Prec@1: [55.476]	
('Starting epoch number:', 19, 'Learning rate:', 0.031622776600000006)
Train: [19]	Time 2376.676	Data 817.153	Loss 2.147	Prec@1 52.1080	Prec@5 75.3251	
Val: [19]	Time 86.375	Data 67.936	Loss 1.932	Prec@1 55.2400	Prec@5 79.1640	
Best Prec@1: [55.476]	
('Starting epoch number:', 20, 'Learning rate:', 0.031622776600000006)
Train: [20]	Time 2365.533	Data 805.742	Loss 2.144	Prec@1 52.1355	Prec@5 75.3364	
Val: [20]	Time 87.140	Data 68.801	Loss 1.954	Prec@1 54.5040	Prec@5 78.7860	
Best Prec@1: [55.476]	
('Starting epoch number:', 21, 'Learning rate:', 0.031622776600000006)
Train: [21]	Time 2403.530	Data 835.618	Loss 2.145	Prec@1 52.1733	Prec@5 75.4150	
Val: [21]	Time 85.210	Data 66.786	Loss 1.955	Prec@1 54.4360	Prec@5 78.7880	
Best Prec@1: [55.476]	
('Starting epoch number:', 22, 'Learning rate:', 0.031622776600000006)
Train: [22]	Time 2382.656	Data 823.240	Loss 2.143	Prec@1 52.1744	Prec@5 75.4030	
Val: [22]	Time 86.101	Data 67.607	Loss 1.993	Prec@1 53.7140	Prec@5 78.1180	
Best Prec@1: [55.476]	
('Starting epoch number:', 23, 'Learning rate:', 0.031622776600000006)
Train: [23]	Time 2398.104	Data 838.184	Loss 2.145	Prec@1 52.1317	Prec@5 75.3188	
Val: [23]	Time 88.572	Data 70.263	Loss 1.983	Prec@1 54.0680	Prec@5 78.3120	
Best Prec@1: [55.476]	
('Starting epoch number:', 24, 'Learning rate:', 0.031622776600000006)
Train: [24]	Time 2450.187	Data 887.794	Loss 2.140	Prec@1 52.1832	Prec@5 75.4641	
Val: [24]	Time 85.926	Data 67.543	Loss 1.934	Prec@1 55.0520	Prec@5 79.0980	
Best Prec@1: [55.476]	
('Starting epoch number:', 25, 'Learning rate:', 0.031622776600000006)
Train: [25]	Time 2349.501	Data 781.768	Loss 2.137	Prec@1 52.2308	Prec@5 75.4854	
Val: [25]	Time 85.689	Data 67.025	Loss 1.966	Prec@1 54.4440	Prec@5 78.6940	
Best Prec@1: [55.476]	
('Starting epoch number:', 26, 'Learning rate:', 0.031622776600000006)
Train: [26]	Time 2405.566	Data 839.113	Loss 2.135	Prec@1 52.2926	Prec@5 75.5501	
Val: [26]	Time 95.770	Data 77.481	Loss 1.922	Prec@1 55.3020	Prec@5 79.3000	
Best Prec@1: [55.476]	
('Starting epoch number:', 27, 'Learning rate:', 0.031622776600000006)
Train: [27]	Time 2593.623	Data 1033.312	Loss 2.129	Prec@1 52.3610	Prec@5 75.6761	
Val: [27]	Time 90.907	Data 71.948	Loss 1.954	Prec@1 54.3260	Prec@5 78.8280	
Best Prec@1: [55.476]	
('Starting epoch number:', 28, 'Learning rate:', 0.031622776600000006)
Train: [28]	Time 2599.406	Data 1034.128	Loss 2.127	Prec@1 52.4087	Prec@5 75.6942	
Val: [28]	Time 91.991	Data 73.706	Loss 1.918	Prec@1 55.1620	Prec@5 79.3600	
Best Prec@1: [55.476]	
('Starting epoch number:', 29, 'Learning rate:', 0.031622776600000006)
Train: [29]	Time 2373.378	Data 801.528	Loss 2.122	Prec@1 52.5419	Prec@5 75.7443	
Val: [29]	Time 93.956	Data 75.509	Loss 1.942	Prec@1 55.0180	Prec@5 78.9380	
Best Prec@1: [55.476]	
('Starting epoch number:', 30, 'Learning rate:', 0.009999999998935078)
Train: [30]	Time 2556.641	Data 991.081	Loss 1.932	Prec@1 56.5289	Prec@5 78.6133	
Val: [30]	Time 90.514	Data 72.150	Loss 1.725	Prec@1 59.1780	Prec@5 82.1180	
Best Prec@1: [59.178]	
('Starting epoch number:', 31, 'Learning rate:', 0.009999999998935078)
Train: [31]	Time 2609.265	Data 1047.470	Loss 1.905	Prec@1 57.0684	Prec@5 78.9704	
Val: [31]	Time 89.002	Data 70.440	Loss 1.713	Prec@1 59.6680	Prec@5 82.3000	
Best Prec@1: [59.668]	
('Starting epoch number:', 32, 'Learning rate:', 0.009999999998935078)
Train: [32]	Time 2667.009	Data 1099.632	Loss 1.895	Prec@1 57.2431	Prec@5 79.1079	
Val: [32]	Time 93.157	Data 74.807	Loss 1.712	Prec@1 59.6160	Prec@5 82.2680	
Best Prec@1: [59.668]	
('Starting epoch number:', 33, 'Learning rate:', 0.009999999998935078)
Train: [33]	Time 2378.996	Data 819.123	Loss 1.890	Prec@1 57.2871	Prec@5 79.1993	
Val: [33]	Time 86.252	Data 67.947	Loss 1.704	Prec@1 59.7380	Prec@5 82.4600	
Best Prec@1: [59.738]	
('Starting epoch number:', 34, 'Learning rate:', 0.009999999998935078)
Train: [34]	Time 2388.349	Data 828.082	Loss 1.889	Prec@1 57.3680	Prec@5 79.1981	
Val: [34]	Time 87.219	Data 68.539	Loss 1.709	Prec@1 59.6500	Prec@5 82.4080	
Best Prec@1: [59.738]	
('Starting epoch number:', 35, 'Learning rate:', 0.009999999998935078)
Train: [35]	Time 2393.900	Data 832.959	Loss 1.885	Prec@1 57.3442	Prec@5 79.2426	
Val: [35]	Time 90.476	Data 72.144	Loss 1.726	Prec@1 59.1040	Prec@5 82.0680	
Best Prec@1: [59.738]	
('Starting epoch number:', 36, 'Learning rate:', 0.009999999998935078)
Train: [36]	Time 2383.166	Data 823.222	Loss 1.883	Prec@1 57.4286	Prec@5 79.3018	
Val: [36]	Time 92.771	Data 74.500	Loss 1.690	Prec@1 59.9700	Prec@5 82.7800	
Best Prec@1: [59.970]	
('Starting epoch number:', 37, 'Learning rate:', 0.009999999998935078)
Train: [37]	Time 2338.292	Data 771.117	Loss 1.879	Prec@1 57.4709	Prec@5 79.3900	
Val: [37]	Time 92.113	Data 73.844	Loss 1.696	Prec@1 59.7560	Prec@5 82.6700	
Best Prec@1: [59.970]	
('Starting epoch number:', 38, 'Learning rate:', 0.009999999998935078)
Train: [38]	Time 2353.114	Data 783.573	Loss 1.880	Prec@1 57.4714	Prec@5 79.3268	
Val: [38]	Time 92.802	Data 74.444	Loss 1.701	Prec@1 59.6760	Prec@5 82.5020	
Best Prec@1: [59.970]	
('Starting epoch number:', 39, 'Learning rate:', 0.009999999998935078)
Train: [39]	Time 2389.510	Data 817.628	Loss 1.881	Prec@1 57.4475	Prec@5 79.3226	
Val: [39]	Time 89.799	Data 70.879	Loss 1.700	Prec@1 59.8860	Prec@5 82.4740	
Best Prec@1: [59.970]	
('Starting epoch number:', 40, 'Learning rate:', 0.009999999998935078)
Train: [40]	Time 2397.455	Data 826.552	Loss 1.877	Prec@1 57.5094	Prec@5 79.4080	
Val: [40]	Time 86.497	Data 67.966	Loss 1.706	Prec@1 59.4480	Prec@5 82.3780	
Best Prec@1: [59.970]	
('Starting epoch number:', 41, 'Learning rate:', 0.009999999998935078)
Train: [41]	Time 2432.758	Data 869.795	Loss 1.875	Prec@1 57.5276	Prec@5 79.4273	
Val: [41]	Time 102.268	Data 83.988	Loss 1.704	Prec@1 59.8000	Prec@5 82.4060	
Best Prec@1: [59.970]	
('Starting epoch number:', 42, 'Learning rate:', 0.009999999998935078)
Train: [42]	Time 2353.185	Data 784.160	Loss 1.877	Prec@1 57.5349	Prec@5 79.4145	
Val: [42]	Time 93.223	Data 74.899	Loss 1.688	Prec@1 59.8920	Prec@5 82.7180	
Best Prec@1: [59.970]	
('Starting epoch number:', 43, 'Learning rate:', 0.009999999998935078)
Train: [43]	Time 2406.858	Data 844.342	Loss 1.875	Prec@1 57.5133	Prec@5 79.4048	
Val: [43]	Time 93.030	Data 74.723	Loss 1.743	Prec@1 58.9500	Prec@5 81.9040	
Best Prec@1: [59.970]	
('Starting epoch number:', 44, 'Learning rate:', 0.009999999998935078)
Train: [44]	Time 2403.812	Data 841.651	Loss 1.875	Prec@1 57.5377	Prec@5 79.4633	
Val: [44]	Time 88.048	Data 69.692	Loss 1.738	Prec@1 59.1720	Prec@5 81.7900	
Best Prec@1: [59.970]	
('Starting epoch number:', 45, 'Learning rate:', 0.0031622776596632422)
Train: [45]	Time 2333.166	Data 766.079	Loss 1.779	Prec@1 59.6294	Prec@5 80.8308	
Val: [45]	Time 87.190	Data 68.799	Loss 1.614	Prec@1 61.5120	Prec@5 83.7640	
Best Prec@1: [61.512]	
('Starting epoch number:', 46, 'Learning rate:', 0.0031622776596632422)
Train: [46]	Time 2381.202	Data 816.614	Loss 1.766	Prec@1 59.9213	Prec@5 80.9806	
Val: [46]	Time 96.091	Data 77.705	Loss 1.612	Prec@1 61.5940	Prec@5 83.8240	
Best Prec@1: [61.594]	
('Starting epoch number:', 47, 'Learning rate:', 0.0031622776596632422)
Train: [47]	Time 2487.315	Data 915.265	Loss 1.760	Prec@1 60.0501	Prec@5 81.0959	
Val: [47]	Time 102.841	Data 84.355	Loss 1.610	Prec@1 61.7320	Prec@5 83.7220	
Best Prec@1: [61.732]	
('Starting epoch number:', 48, 'Learning rate:', 0.0031622776596632422)
Train: [48]	Time 2392.735	Data 831.168	Loss 1.757	Prec@1 60.0374	Prec@5 81.1386	
Val: [48]	Time 90.462	Data 72.150	Loss 1.611	Prec@1 61.6000	Prec@5 83.7900	
Best Prec@1: [61.732]	
('Starting epoch number:', 49, 'Learning rate:', 0.0031622776596632422)
Train: [49]	Time 2375.859	Data 814.950	Loss 1.753	Prec@1 60.1466	Prec@5 81.1890	
Val: [49]	Time 91.131	Data 72.872	Loss 1.608	Prec@1 61.8980	Prec@5 83.9300	
Best Prec@1: [61.898]	
('Starting epoch number:', 50, 'Learning rate:', 0.0031622776596632422)
Train: [50]	Time 2371.768	Data 810.777	Loss 1.752	Prec@1 60.1730	Prec@5 81.2040	
Val: [50]	Time 84.648	Data 66.359	Loss 1.605	Prec@1 61.9820	Prec@5 83.9560	
Best Prec@1: [61.982]	
('Starting epoch number:', 51, 'Learning rate:', 0.0031622776596632422)
Train: [51]	Time 2384.528	Data 823.119	Loss 1.749	Prec@1 60.2235	Prec@5 81.2353	
Val: [51]	Time 93.764	Data 75.473	Loss 1.605	Prec@1 61.7420	Prec@5 83.8360	
Best Prec@1: [61.982]	
('Starting epoch number:', 52, 'Learning rate:', 0.0031622776596632422)
Train: [52]	Time 2422.092	Data 852.303	Loss 1.750	Prec@1 60.2075	Prec@5 81.2479	
Val: [52]	Time 103.973	Data 85.563	Loss 1.596	Prec@1 61.9800	Prec@5 84.0380	
Best Prec@1: [61.982]	
('Starting epoch number:', 53, 'Learning rate:', 0.0031622776596632422)
Train: [53]	Time 2471.428	Data 907.483	Loss 1.749	Prec@1 60.1973	Prec@5 81.2489	
Val: [53]	Time 93.903	Data 75.573	Loss 1.597	Prec@1 62.0020	Prec@5 84.0680	
Best Prec@1: [62.002]	
('Starting epoch number:', 54, 'Learning rate:', 0.0031622776596632422)
Train: [54]	Time 2391.905	Data 827.842	Loss 1.745	Prec@1 60.2679	Prec@5 81.3079	
Val: [54]	Time 90.627	Data 72.320	Loss 1.594	Prec@1 62.1820	Prec@5 83.9440	
Best Prec@1: [62.182]	
('Starting epoch number:', 55, 'Learning rate:', 0.0031622776596632422)
Train: [55]	Time 2397.205	Data 835.190	Loss 1.743	Prec@1 60.3642	Prec@5 81.3313	
Val: [55]	Time 94.279	Data 76.014	Loss 1.601	Prec@1 61.9400	Prec@5 83.9100	
Best Prec@1: [62.182]	
('Starting epoch number:', 56, 'Learning rate:', 0.0031622776596632422)
Train: [56]	Time 2342.467	Data 773.381	Loss 1.742	Prec@1 60.3580	Prec@5 81.3338	
Val: [56]	Time 87.602	Data 69.364	Loss 1.597	Prec@1 62.0940	Prec@5 84.0620	
Best Prec@1: [62.182]	
('Starting epoch number:', 57, 'Learning rate:', 0.0031622776596632422)
Train: [57]	Time 2368.220	Data 806.259	Loss 1.742	Prec@1 60.3136	Prec@5 81.3509	
Val: [57]	Time 91.785	Data 73.488	Loss 1.595	Prec@1 62.0460	Prec@5 84.0720	
Best Prec@1: [62.182]	
('Starting epoch number:', 58, 'Learning rate:', 0.0031622776596632422)
Train: [58]	Time 2338.477	Data 771.159	Loss 1.740	Prec@1 60.3140	Prec@5 81.4045	
Val: [58]	Time 93.614	Data 75.307	Loss 1.600	Prec@1 61.8440	Prec@5 83.9740	
Best Prec@1: [62.182]	
('Starting epoch number:', 59, 'Learning rate:', 0.0031622776596632422)
Train: [59]	Time 2336.222	Data 765.418	Loss 1.739	Prec@1 60.4210	Prec@5 81.4200	
Val: [59]	Time 98.296	Data 79.960	Loss 1.599	Prec@1 61.9300	Prec@5 83.9820	
Best Prec@1: [62.182]	
('Starting epoch number:', 60, 'Learning rate:', 0.0009999999997870154)
Train: [60]	Time 2391.480	Data 830.708	Loss 1.702	Prec@1 61.2078	Prec@5 81.9566	
Val: [60]	Time 92.187	Data 73.839	Loss 1.565	Prec@1 62.6020	Prec@5 84.5680	
Best Prec@1: [62.602]	
('Starting epoch number:', 61, 'Learning rate:', 0.0009999999997870154)
Train: [61]	Time 2386.416	Data 827.293	Loss 1.695	Prec@1 61.3799	Prec@5 82.0540	
Val: [61]	Time 92.191	Data 73.922	Loss 1.565	Prec@1 62.6660	Prec@5 84.4540	
Best Prec@1: [62.666]	
('Starting epoch number:', 62, 'Learning rate:', 0.0009999999997870154)
Train: [62]	Time 2404.033	Data 836.587	Loss 1.693	Prec@1 61.4454	Prec@5 82.0488	
Val: [62]	Time 85.396	Data 66.653	Loss 1.561	Prec@1 62.7600	Prec@5 84.5240	
Best Prec@1: [62.760]	
('Starting epoch number:', 63, 'Learning rate:', 0.0009999999997870154)
Train: [63]	Time 2645.360	Data 1085.876	Loss 1.691	Prec@1 61.4607	Prec@5 82.0775	
Val: [63]	Time 91.770	Data 73.379	Loss 1.562	Prec@1 62.7140	Prec@5 84.4980	
Best Prec@1: [62.760]	
('Starting epoch number:', 64, 'Learning rate:', 0.0009999999997870154)
Train: [64]	Time 2420.924	Data 848.867	Loss 1.691	Prec@1 61.4629	Prec@5 82.0783	
Val: [64]	Time 103.334	Data 84.989	Loss 1.559	Prec@1 62.7980	Prec@5 84.5160	
Best Prec@1: [62.798]	
('Starting epoch number:', 65, 'Learning rate:', 0.0009999999997870154)
Train: [65]	Time 2435.880	Data 866.130	Loss 1.690	Prec@1 61.4457	Prec@5 82.0998	
Val: [65]	Time 102.870	Data 84.463	Loss 1.563	Prec@1 62.7440	Prec@5 84.3900	
Best Prec@1: [62.798]	
('Starting epoch number:', 66, 'Learning rate:', 0.0009999999997870154)
Train: [66]	Time 2420.384	Data 849.459	Loss 1.685	Prec@1 61.6004	Prec@5 82.1937	
Val: [66]	Time 101.801	Data 83.421	Loss 1.562	Prec@1 62.8760	Prec@5 84.5080	
Best Prec@1: [62.876]	
('Starting epoch number:', 67, 'Learning rate:', 0.0009999999997870154)
Train: [67]	Time 2518.628	Data 958.317	Loss 1.687	Prec@1 61.5059	Prec@5 82.1577	
Val: [67]	Time 93.368	Data 75.105	Loss 1.556	Prec@1 62.8980	Prec@5 84.5820	
Best Prec@1: [62.898]	
('Starting epoch number:', 68, 'Learning rate:', 0.0009999999997870154)
Train: [68]	Time 2344.504	Data 784.309	Loss 1.685	Prec@1 61.6041	Prec@5 82.1853	
Val: [68]	Time 87.197	Data 68.919	Loss 1.562	Prec@1 62.8000	Prec@5 84.4840	
Best Prec@1: [62.898]	
('Starting epoch number:', 69, 'Learning rate:', 0.0009999999997870154)
Train: [69]	Time 2442.423	Data 871.007	Loss 1.686	Prec@1 61.5986	Prec@5 82.1607	
Val: [69]	Time 100.047	Data 81.226	Loss 1.559	Prec@1 62.7500	Prec@5 84.5340	
Best Prec@1: [62.898]	
('Starting epoch number:', 70, 'Learning rate:', 0.0009999999997870154)
Train: [70]	Time 2744.364	Data 1173.073	Loss 1.681	Prec@1 61.6537	Prec@5 82.2156	
Val: [70]	Time 86.827	Data 68.463	Loss 1.561	Prec@1 62.7500	Prec@5 84.4300	
Best Prec@1: [62.898]	
('Starting epoch number:', 71, 'Learning rate:', 0.0009999999997870154)
Train: [71]	Time 2371.737	Data 810.839	Loss 1.682	Prec@1 61.6138	Prec@5 82.2073	
Val: [71]	Time 86.041	Data 67.768	Loss 1.559	Prec@1 62.7500	Prec@5 84.4700	
Best Prec@1: [62.898]	
('Starting epoch number:', 72, 'Learning rate:', 0.0009999999997870154)
Train: [72]	Time 2393.169	Data 832.229	Loss 1.683	Prec@1 61.5878	Prec@5 82.2112	
Val: [72]	Time 85.431	Data 66.983	Loss 1.555	Prec@1 62.8540	Prec@5 84.5180	
Best Prec@1: [62.898]	
('Starting epoch number:', 73, 'Learning rate:', 0.0009999999997870154)
Train: [73]	Time 2371.341	Data 811.056	Loss 1.681	Prec@1 61.6762	Prec@5 82.2527	
Val: [73]	Time 96.678	Data 78.050	Loss 1.558	Prec@1 62.8760	Prec@5 84.5500	
Best Prec@1: [62.898]	
('Starting epoch number:', 74, 'Learning rate:', 0.0009999999997870154)
Train: [74]	Time 2522.097	Data 961.389	Loss 1.679	Prec@1 61.7111	Prec@5 82.2714	
Val: [74]	Time 85.591	Data 67.250	Loss 1.560	Prec@1 62.6640	Prec@5 84.5560	
Best Prec@1: [62.898]	
('Starting epoch number:', 75, 'Learning rate:', 0.0003162277659326484)
Train: [75]	Time 2310.089	Data 737.504	Loss 1.669	Prec@1 61.9246	Prec@5 82.4175	
Val: [75]	Time 85.590	Data 67.244	Loss 1.548	Prec@1 62.8940	Prec@5 84.6900	
Best Prec@1: [62.898]	
('Starting epoch number:', 76, 'Learning rate:', 0.0003162277659326484)
Train: [76]	Time 2330.976	Data 764.092	Loss 1.666	Prec@1 62.0013	Prec@5 82.4831	
Val: [76]	Time 84.949	Data 66.406	Loss 1.555	Prec@1 62.7740	Prec@5 84.5980	
Best Prec@1: [62.898]	
('Starting epoch number:', 77, 'Learning rate:', 0.0003162277659326484)
Train: [77]	Time 2355.767	Data 795.961	Loss 1.663	Prec@1 62.0105	Prec@5 82.4957	
Val: [77]	Time 85.126	Data 66.804	Loss 1.547	Prec@1 63.0200	Prec@5 84.6880	
Best Prec@1: [63.020]	
('Starting epoch number:', 78, 'Learning rate:', 0.0003162277659326484)
Train: [78]	Time 2386.289	Data 825.936	Loss 1.664	Prec@1 62.0386	Prec@5 82.4733	
Val: [78]	Time 91.279	Data 72.998	Loss 1.547	Prec@1 62.9680	Prec@5 84.7100	
Best Prec@1: [63.020]	
('Starting epoch number:', 79, 'Learning rate:', 0.0003162277659326484)
Train: [79]	Time 2457.391	Data 898.678	Loss 1.662	Prec@1 62.0351	Prec@5 82.5129	
Val: [79]	Time 86.088	Data 67.808	Loss 1.549	Prec@1 62.9980	Prec@5 84.6740	
Best Prec@1: [63.020]	
('Starting epoch number:', 80, 'Learning rate:', 0.0003162277659326484)
Train: [80]	Time 2393.905	Data 833.441	Loss 1.663	Prec@1 62.0241	Prec@5 82.5010	
Val: [80]	Time 85.922	Data 67.477	Loss 1.544	Prec@1 63.0980	Prec@5 84.7240	
Best Prec@1: [63.098]	
('Starting epoch number:', 81, 'Learning rate:', 0.0003162277659326484)
Train: [81]	Time 2372.499	Data 811.450	Loss 1.662	Prec@1 62.0996	Prec@5 82.5365	
Val: [81]	Time 84.730	Data 66.298	Loss 1.548	Prec@1 62.9920	Prec@5 84.6560	
Best Prec@1: [63.098]	
('Starting epoch number:', 82, 'Learning rate:', 0.0003162277659326484)
Train: [82]	Time 2373.847	Data 811.571	Loss 1.662	Prec@1 62.0486	Prec@5 82.4767	
Val: [82]	Time 87.176	Data 68.246	Loss 1.547	Prec@1 63.1000	Prec@5 84.7360	
Best Prec@1: [63.100]	
('Starting epoch number:', 83, 'Learning rate:', 0.0003162277659326484)
Train: [83]	Time 2598.285	Data 1032.889	Loss 1.662	Prec@1 62.0881	Prec@5 82.5037	
Val: [83]	Time 90.314	Data 72.035	Loss 1.546	Prec@1 63.0720	Prec@5 84.6620	
Best Prec@1: [63.100]	
('Starting epoch number:', 84, 'Learning rate:', 0.0003162277659326484)
Train: [84]	Time 2383.502	Data 822.372	Loss 1.660	Prec@1 62.1120	Prec@5 82.5261	
Val: [84]	Time 94.029	Data 75.622	Loss 1.546	Prec@1 63.0140	Prec@5 84.6580	
Best Prec@1: [63.100]	
('Starting epoch number:', 85, 'Learning rate:', 0.0003162277659326484)
Train: [85]	Time 2376.334	Data 806.090	Loss 1.660	Prec@1 62.0683	Prec@5 82.5319	
Val: [85]	Time 94.744	Data 76.399	Loss 1.549	Prec@1 63.0440	Prec@5 84.6900	
Best Prec@1: [63.100]	
('Starting epoch number:', 86, 'Learning rate:', 0.0003162277659326484)
Train: [86]	Time 2387.997	Data 827.320	Loss 1.660	Prec@1 62.1436	Prec@5 82.5554	
Val: [86]	Time 92.208	Data 73.853	Loss 1.544	Prec@1 63.1620	Prec@5 84.7560	
Best Prec@1: [63.162]	
('Starting epoch number:', 87, 'Learning rate:', 0.0003162277659326484)
Train: [87]	Time 2405.299	Data 843.103	Loss 1.661	Prec@1 62.1205	Prec@5 82.5386	
Val: [87]	Time 85.113	Data 66.714	Loss 1.548	Prec@1 63.0720	Prec@5 84.6820	
Best Prec@1: [63.162]	
('Starting epoch number:', 88, 'Learning rate:', 0.0003162277659326484)
Train: [88]	Time 2364.556	Data 803.615	Loss 1.660	Prec@1 62.1002	Prec@5 82.5118	
Val: [88]	Time 85.919	Data 67.495	Loss 1.549	Prec@1 63.1360	Prec@5 84.6580	
Best Prec@1: [63.162]	
('Starting epoch number:', 89, 'Learning rate:', 0.0003162277659326484)
Train: [89]	Time 2394.722	Data 830.886	Loss 1.660	Prec@1 62.1375	Prec@5 82.5707	
Val: [89]	Time 95.319	Data 76.959	Loss 1.542	Prec@1 63.1520	Prec@5 84.7240	
Best Prec@1: [63.162]	
('Starting epoch number:', 90, 'Learning rate:', 9.999999996805232e-05)
Train: [90]	Time 2418.145	Data 853.549	Loss 1.655	Prec@1 62.2250	Prec@5 82.6159	
Val: [90]	Time 103.531	Data 85.180	Loss 1.542	Prec@1 63.2180	Prec@5 84.7100	
Best Prec@1: [63.218]	
('Starting epoch number:', 91, 'Learning rate:', 9.999999996805232e-05)
Train: [91]	Time 2420.593	Data 851.074	Loss 1.653	Prec@1 62.2160	Prec@5 82.6256	
Val: [91]	Time 90.409	Data 72.120	Loss 1.541	Prec@1 63.2940	Prec@5 84.7600	
Best Prec@1: [63.294]	
('Starting epoch number:', 92, 'Learning rate:', 9.999999996805232e-05)
Train: [92]	Time 2369.977	Data 807.938	Loss 1.654	Prec@1 62.2120	Prec@5 82.6129	
Val: [92]	Time 86.203	Data 67.861	Loss 1.544	Prec@1 63.2440	Prec@5 84.7000	
Best Prec@1: [63.294]	
('Starting epoch number:', 93, 'Learning rate:', 9.999999996805232e-05)
Train: [93]	Time 2377.183	Data 816.480	Loss 1.654	Prec@1 62.2257	Prec@5 82.5951	
Val: [93]	Time 85.040	Data 66.664	Loss 1.543	Prec@1 63.1920	Prec@5 84.7360	
Best Prec@1: [63.294]	
('Starting epoch number:', 94, 'Learning rate:', 9.999999996805232e-05)
Train: [94]	Time 2350.833	Data 780.711	Loss 1.656	Prec@1 62.1628	Prec@5 82.5346	
Val: [94]	Time 86.130	Data 67.577	Loss 1.545	Prec@1 63.2160	Prec@5 84.7540	
Best Prec@1: [63.294]	
('Starting epoch number:', 95, 'Learning rate:', 9.999999996805232e-05)
Train: [95]	Time 2420.451	Data 853.034	Loss 1.655	Prec@1 62.2435	Prec@5 82.5947	
Val: [95]	Time 95.038	Data 76.549	Loss 1.543	Prec@1 63.2320	Prec@5 84.7780	
Best Prec@1: [63.294]	
('Starting epoch number:', 96, 'Learning rate:', 9.999999996805232e-05)
Train: [96]	Time 2454.986	Data 887.089	Loss 1.651	Prec@1 62.2768	Prec@5 82.6573	
Val: [96]	Time 92.171	Data 73.845	Loss 1.543	Prec@1 63.1740	Prec@5 84.7560	
Best Prec@1: [63.294]	
('Starting epoch number:', 97, 'Learning rate:', 9.999999996805232e-05)
Train: [97]	Time 2368.100	Data 807.962	Loss 1.654	Prec@1 62.2320	Prec@5 82.6077	
Val: [97]	Time 89.620	Data 71.367	Loss 1.542	Prec@1 63.2900	Prec@5 84.7760	
Best Prec@1: [63.294]	
('Starting epoch number:', 98, 'Learning rate:', 9.999999996805232e-05)
Train: [98]	Time 2375.851	Data 815.840	Loss 1.653	Prec@1 62.2350	Prec@5 82.5965	
Val: [98]	Time 85.419	Data 67.024	Loss 1.544	Prec@1 63.1640	Prec@5 84.7440	
Best Prec@1: [63.294]	
('Starting epoch number:', 99, 'Learning rate:', 9.999999996805232e-05)
Train: [99]	Time 2384.038	Data 824.279	Loss 1.653	Prec@1 62.2503	Prec@5 82.6308	
Val: [99]	Time 85.470	Data 67.119	Loss 1.542	Prec@1 63.1660	Prec@5 84.7780	
Best Prec@1: [63.294]	
('Starting epoch number:', 100, 'Learning rate:', 9.999999996805232e-05)
Train: [100]	Time 2406.136	Data 840.113	Loss 1.652	Prec@1 62.2814	Prec@5 82.6582	
Val: [100]	Time 85.401	Data 67.013	Loss 1.546	Prec@1 63.1680	Prec@5 84.7120	
Best Prec@1: [63.294]	
('Starting epoch number:', 101, 'Learning rate:', 9.999999996805232e-05)
Train: [101]	Time 2422.499	Data 858.513	Loss 1.651	Prec@1 62.2915	Prec@5 82.6488	
Val: [101]	Time 94.181	Data 75.665	Loss 1.543	Prec@1 63.2060	Prec@5 84.7520	
Best Prec@1: [63.294]	
('Starting epoch number:', 102, 'Learning rate:', 9.999999996805232e-05)
Train: [102]	Time 2544.178	Data 985.122	Loss 1.651	Prec@1 62.3011	Prec@5 82.6771	
Val: [102]	Time 92.543	Data 74.253	Loss 1.544	Prec@1 63.1100	Prec@5 84.7480	
Best Prec@1: [63.294]	
('Starting epoch number:', 103, 'Learning rate:', 9.999999996805232e-05)
Train: [103]	Time 2338.882	Data 767.742	Loss 1.651	Prec@1 62.2791	Prec@5 82.6686	
Val: [103]	Time 92.561	Data 74.192	Loss 1.544	Prec@1 63.1100	Prec@5 84.7320	
Best Prec@1: [63.294]	
('Starting epoch number:', 104, 'Learning rate:', 9.999999996805232e-05)
Train: [104]	Time 2387.271	Data 823.791	Loss 1.651	Prec@1 62.3356	Prec@5 82.6715	
Val: [104]	Time 91.625	Data 73.317	Loss 1.544	Prec@1 63.2000	Prec@5 84.7380	
Best Prec@1: [63.294]	
('Starting epoch number:', 105, 'Learning rate:', 5e-05)
Train: [105]	Time 2375.636	Data 815.398	Loss 1.651	Prec@1 62.2830	Prec@5 82.6605	
Val: [105]	Time 89.712	Data 71.165	Loss 1.539	Prec@1 63.2420	Prec@5 84.7380	
Best Prec@1: [63.294]	
('Starting epoch number:', 106, 'Learning rate:', 5e-05)
Train: [106]	Time 2484.484	Data 912.825	Loss 1.649	Prec@1 62.3355	Prec@5 82.6730	
Val: [106]	Time 91.885	Data 73.327	Loss 1.542	Prec@1 63.2760	Prec@5 84.6900	
Best Prec@1: [63.294]	
('Starting epoch number:', 107, 'Learning rate:', 5e-05)
Train: [107]	Time 2362.755	Data 793.551	Loss 1.652	Prec@1 62.2364	Prec@5 82.6552	
Val: [107]	Time 86.417	Data 67.769	Loss 1.542	Prec@1 63.1860	Prec@5 84.8080	
Best Prec@1: [63.294]	
('Starting epoch number:', 108, 'Learning rate:', 5e-05)
Train: [108]	Time 2415.141	Data 848.547	Loss 1.650	Prec@1 62.3668	Prec@5 82.6797	
Val: [108]	Time 94.387	Data 76.004	Loss 1.540	Prec@1 63.2560	Prec@5 84.8240	
Best Prec@1: [63.294]	
('Starting epoch number:', 109, 'Learning rate:', 5e-05)
Train: [109]	Time 2421.228	Data 856.059	Loss 1.649	Prec@1 62.3593	Prec@5 82.6900	
Val: [109]	Time 93.488	Data 75.233	Loss 1.543	Prec@1 63.2100	Prec@5 84.7920	
Best Prec@1: [63.294]	
('Starting epoch number:', 110, 'Learning rate:', 5e-05)
Train: [110]	Time 2378.886	Data 818.820	Loss 1.651	Prec@1 62.3345	Prec@5 82.6433	
Val: [110]	Time 93.388	Data 75.019	Loss 1.541	Prec@1 63.2180	Prec@5 84.7420	
Best Prec@1: [63.294]	
('Starting epoch number:', 111, 'Learning rate:', 5e-05)
Train: [111]	Time 2420.978	Data 858.522	Loss 1.650	Prec@1 62.3080	Prec@5 82.6648	
Val: [111]	Time 93.542	Data 75.206	Loss 1.543	Prec@1 63.1980	Prec@5 84.7480	
Best Prec@1: [63.294]	
('Starting epoch number:', 112, 'Learning rate:', 5e-05)
Train: [112]	Time 2422.873	Data 850.002	Loss 1.649	Prec@1 62.2887	Prec@5 82.7115	
Val: [112]	Time 90.639	Data 71.932	Loss 1.543	Prec@1 63.1620	Prec@5 84.7520	
Best Prec@1: [63.294]	
('Starting epoch number:', 113, 'Learning rate:', 5e-05)
Train: [113]	Time 2385.251	Data 815.044	Loss 1.648	Prec@1 62.3246	Prec@5 82.7312	
Val: [113]	Time 86.966	Data 68.604	Loss 1.543	Prec@1 63.2500	Prec@5 84.7360	
Best Prec@1: [63.294]	
('Starting epoch number:', 114, 'Learning rate:', 5e-05)
Train: [114]	Time 2342.993	Data 775.876	Loss 1.649	Prec@1 62.3312	Prec@5 82.6886	
Val: [114]	Time 93.348	Data 75.083	Loss 1.541	Prec@1 63.2560	Prec@5 84.7980	
Best Prec@1: [63.294]	
('Starting epoch number:', 115, 'Learning rate:', 5e-05)
Train: [115]	Time 2376.896	Data 816.624	Loss 1.648	Prec@1 62.3691	Prec@5 82.6866	
Val: [115]	Time 92.373	Data 74.150	Loss 1.545	Prec@1 63.1680	Prec@5 84.6960	
Best Prec@1: [63.294]	
('Starting epoch number:', 116, 'Learning rate:', 5e-05)
Train: [116]	Time 2362.712	Data 801.919	Loss 1.650	Prec@1 62.2816	Prec@5 82.6634	
Val: [116]	Time 100.434	Data 82.172	Loss 1.543	Prec@1 63.2800	Prec@5 84.7280	
Best Prec@1: [63.294]	
('Starting epoch number:', 117, 'Learning rate:', 5e-05)
Train: [117]	Time 2332.878	Data 760.653	Loss 1.649	Prec@1 62.3468	Prec@5 82.6872	
Val: [117]	Time 94.369	Data 76.033	Loss 1.545	Prec@1 63.1880	Prec@5 84.7160	
Best Prec@1: [63.294]	
('Starting epoch number:', 118, 'Learning rate:', 5e-05)
Train: [118]	Time 2366.493	Data 807.717	Loss 1.650	Prec@1 62.3210	Prec@5 82.6702	
Val: [118]	Time 85.317	Data 67.039	Loss 1.539	Prec@1 63.2880	Prec@5 84.7600	
Best Prec@1: [63.294]	
('Starting epoch number:', 119, 'Learning rate:', 5e-05)
Train: [119]	Time 2328.777	Data 758.430	Loss 1.648	Prec@1 62.3284	Prec@5 82.7111	
Val: [119]	Time 84.747	Data 66.359	Loss 1.539	Prec@1 63.3000	Prec@5 84.7860	
Best Prec@1: [63.300]	
('Starting epoch number:', 120, 'Learning rate:', 5e-05)
Train: [120]	Time 2403.644	Data 837.791	Loss 1.651	Prec@1 62.2867	Prec@5 82.6883	
Val: [120]	Time 103.649	Data 85.240	Loss 1.541	Prec@1 63.2320	Prec@5 84.7640	
Best Prec@1: [63.300]	
('Starting epoch number:', 121, 'Learning rate:', 5e-05)
Train: [121]	Time 2484.446	Data 914.897	Loss 1.650	Prec@1 62.3465	Prec@5 82.6441	
Val: [121]	Time 103.942	Data 85.618	Loss 1.539	Prec@1 63.3920	Prec@5 84.7440	
Best Prec@1: [63.392]	
('Starting epoch number:', 122, 'Learning rate:', 5e-05)
Train: [122]	Time 2304.920	Data 730.750	Loss 1.648	Prec@1 62.3829	Prec@5 82.7274	
Val: [122]	Time 93.187	Data 74.803	Loss 1.544	Prec@1 63.1700	Prec@5 84.7320	
Best Prec@1: [63.392]	
('Starting epoch number:', 123, 'Learning rate:', 5e-05)
Train: [123]	Time 2381.107	Data 814.907	Loss 1.653	Prec@1 62.2335	Prec@5 82.6207	
Val: [123]	Time 89.793	Data 71.488	Loss 1.540	Prec@1 63.2020	Prec@5 84.7860	
Best Prec@1: [63.392]	
('Starting epoch number:', 124, 'Learning rate:', 5e-05)
Train: [124]	Time 2401.680	Data 838.463	Loss 1.648	Prec@1 62.3475	Prec@5 82.6968	
Val: [124]	Time 89.809	Data 71.524	Loss 1.539	Prec@1 63.3100	Prec@5 84.8060	
Best Prec@1: [63.392]	
('Starting epoch number:', 125, 'Learning rate:', 5e-05)
Train: [125]	Time 2390.398	Data 827.856	Loss 1.650	Prec@1 62.3694	Prec@5 82.6570	
Val: [125]	Time 84.642	Data 66.252	Loss 1.539	Prec@1 63.2820	Prec@5 84.8320	
Best Prec@1: [63.392]	
('Starting epoch number:', 126, 'Learning rate:', 5e-05)
Train: [126]	Time 2351.103	Data 788.418	Loss 1.649	Prec@1 62.3377	Prec@5 82.6709	
Val: [126]	Time 92.383	Data 74.089	Loss 1.542	Prec@1 63.2780	Prec@5 84.7440	
Best Prec@1: [63.392]	
('Starting epoch number:', 127, 'Learning rate:', 5e-05)
Train: [127]	Time 2367.922	Data 805.459	Loss 1.648	Prec@1 62.2791	Prec@5 82.7373	
Val: [127]	Time 92.011	Data 73.745	Loss 1.543	Prec@1 63.2120	Prec@5 84.7700	
Best Prec@1: [63.392]	
('Starting epoch number:', 128, 'Learning rate:', 5e-05)
Train: [128]	Time 2454.391	Data 893.656	Loss 1.651	Prec@1 62.2979	Prec@5 82.6313	
Val: [128]	Time 91.233	Data 72.955	Loss 1.541	Prec@1 63.2660	Prec@5 84.7860	
Best Prec@1: [63.392]	
('Starting epoch number:', 129, 'Learning rate:', 5e-05)
Train: [129]	Time 2351.628	Data 787.940	Loss 1.648	Prec@1 62.3212	Prec@5 82.6844	
Val: [129]	Time 91.642	Data 73.382	Loss 1.540	Prec@1 63.2600	Prec@5 84.7600	
Best Prec@1: [63.392]	
('Starting epoch number:', 130, 'Learning rate:', 5e-05)
Train: [130]	Time 2361.744	Data 797.917	Loss 1.649	Prec@1 62.3182	Prec@5 82.6644	
Val: [130]	Time 88.822	Data 70.550	Loss 1.544	Prec@1 63.2040	Prec@5 84.7600	
Best Prec@1: [63.392]	
('Starting epoch number:', 131, 'Learning rate:', 5e-05)
Train: [131]	Time 2457.150	Data 895.797	Loss 1.648	Prec@1 62.3001	Prec@5 82.7262	
Val: [131]	Time 84.962	Data 66.606	Loss 1.542	Prec@1 63.2160	Prec@5 84.7720	
Best Prec@1: [63.392]	
('Starting epoch number:', 132, 'Learning rate:', 5e-05)
Train: [132]	Time 2377.039	Data 814.556	Loss 1.649	Prec@1 62.3205	Prec@5 82.7003	
Val: [132]	Time 91.514	Data 73.227	Loss 1.539	Prec@1 63.2480	Prec@5 84.7540	
Best Prec@1: [63.392]	
('Starting epoch number:', 133, 'Learning rate:', 5e-05)
Train: [133]	Time 2357.181	Data 794.139	Loss 1.648	Prec@1 62.3538	Prec@5 82.6773	
Val: [133]	Time 90.126	Data 71.746	Loss 1.540	Prec@1 63.2280	Prec@5 84.8040	
Best Prec@1: [63.392]	
('Starting epoch number:', 134, 'Learning rate:', 5e-05)
Train: [134]	Time 2470.284	Data 910.251	Loss 1.649	Prec@1 62.3497	Prec@5 82.6720	
Val: [134]	Time 92.603	Data 74.341	Loss 1.540	Prec@1 63.2260	Prec@5 84.7360	
Best Prec@1: [63.392]	
('Starting epoch number:', 135, 'Learning rate:', 5e-05)
Train: [135]	Time 2395.242	Data 833.338	Loss 1.650	Prec@1 62.3358	Prec@5 82.6604	
Val: [135]	Time 90.970	Data 72.657	Loss 1.541	Prec@1 63.2240	Prec@5 84.7380	
Best Prec@1: [63.392]	
('Starting epoch number:', 136, 'Learning rate:', 5e-05)
Train: [136]	Time 2460.341	Data 891.454	Loss 1.648	Prec@1 62.3420	Prec@5 82.7228	
Val: [136]	Time 92.052	Data 73.572	Loss 1.542	Prec@1 63.2640	Prec@5 84.7240	
Best Prec@1: [63.392]	
('Starting epoch number:', 137, 'Learning rate:', 5e-05)
Train: [137]	Time 2418.579	Data 843.292	Loss 1.649	Prec@1 62.3670	Prec@5 82.6814	
Val: [137]	Time 98.949	Data 80.260	Loss 1.540	Prec@1 63.2280	Prec@5 84.7780	
Best Prec@1: [63.392]	
('Starting epoch number:', 138, 'Learning rate:', 5e-05)
