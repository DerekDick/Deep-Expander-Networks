Namespace(acc_type='class', augment=True, batch_size=64, bottleneck=True, cachemode=True, cardinality=8, criterion='crossentropy', cuda=True, data_dir='../data', dataset='cifar10', decayinterval=None, decaylevel=None, droprate=0, epochs=300, evaluate=False, expandConfig=None, expandSize=2, from_modelzoo=False, growth=60, layers=40, learningratescheduler='cifarschedular', logdir='../logs/densenetbc_cifar10_40_60', lr=0.1, manualSeed=123, maxlr=0.1, minlr=0.0005, model_def='densenet_cifar', momentum=0.9, name='densenetbc_cifar10_40_60', nclasses=10, nesterov=True, ngpus=1, optimType='sgd', pretrained=False, pretrained_file='', printfreq=200, reduce=0.5, resume='', start_epoch=0, store='', tenCrop=False, tensorboard=True, testOnly=False, verbose=False, weightDecay=0.0001, weight_init=False, widen_factor=4, workers=2)
DenseNet3 (
  (conv1): Conv2d(3, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): DenseBlock (
    (layer): Sequential (
      (0): BottleneckBlock (
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(120, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(240, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock (
        (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(180, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(240, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock (
        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(240, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock (
        (bn1): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(300, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(240, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock (
        (bn1): BatchNorm2d(360, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(360, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(240, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock (
        (bn1): BatchNorm2d(420, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(420, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(240, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans1): TransitionBlock (
    (bn1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True)
    (relu): ReLU (inplace)
    (conv1): Conv2d(480, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block2): DenseBlock (
    (layer): Sequential (
      (0): BottleneckBlock (
        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(240, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock (
        (bn1): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(300, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(240, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock (
        (bn1): BatchNorm2d(360, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(360, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(240, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock (
        (bn1): BatchNorm2d(420, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(420, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(240, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock (
        (bn1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(480, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(240, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock (
        (bn1): BatchNorm2d(540, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(540, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(240, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans2): TransitionBlock (
    (bn1): BatchNorm2d(600, eps=1e-05, momentum=0.1, affine=True)
    (relu): ReLU (inplace)
    (conv1): Conv2d(600, 300, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block3): DenseBlock (
    (layer): Sequential (
      (0): BottleneckBlock (
        (bn1): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(300, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(240, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock (
        (bn1): BatchNorm2d(360, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(360, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(240, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock (
        (bn1): BatchNorm2d(420, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(420, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(240, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock (
        (bn1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(480, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(240, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock (
        (bn1): BatchNorm2d(540, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(540, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(240, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock (
        (bn1): BatchNorm2d(600, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(600, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(240, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (bn1): BatchNorm2d(660, eps=1e-05, momentum=0.1, affine=True)
  (relu): ReLU (inplace)
  (fc): Linear (660 -> 10)
)
Files already downloaded and verified
Starting epoch number: 0 Learning rate: 0.1
Train: [0]	Time 110.550	Data 0.315	Loss 1.452	Prec@1 47.2860	Prec@5 91.2840	
Val: [0]	Time 6.440	Data 0.119	Loss 1.243	Prec@1 57.0700	Prec@5 94.9400	
Best Prec@1: [57.070]	
Starting epoch number: 1 Learning rate: 0.1
Train: [1]	Time 108.850	Data 0.309	Loss 0.888	Prec@1 68.3840	Prec@5 97.4840	
Val: [1]	Time 6.588	Data 0.098	Loss 0.821	Prec@1 71.3900	Prec@5 98.1600	
Best Prec@1: [71.390]	
Starting epoch number: 2 Learning rate: 0.1
Train: [2]	Time 109.769	Data 0.310	Loss 0.652	Prec@1 77.3720	Prec@5 98.6620	
Val: [2]	Time 6.546	Data 0.099	Loss 0.692	Prec@1 76.0500	Prec@5 98.8600	
Best Prec@1: [76.050]	
Starting epoch number: 3 Learning rate: 0.1
Train: [3]	Time 109.904	Data 0.305	Loss 0.534	Prec@1 81.6040	Prec@5 99.0780	
Val: [3]	Time 6.569	Data 0.111	Loss 0.766	Prec@1 75.3300	Prec@5 98.8200	
Best Prec@1: [76.050]	
Starting epoch number: 4 Learning rate: 0.1
Train: [4]	Time 109.792	Data 0.305	Loss 0.470	Prec@1 83.9620	Prec@5 99.3040	
Val: [4]	Time 6.548	Data 0.095	Loss 0.508	Prec@1 83.1100	Prec@5 99.0700	
Best Prec@1: [83.110]	
Starting epoch number: 5 Learning rate: 0.1
Train: [5]	Time 109.585	Data 0.309	Loss 0.418	Prec@1 85.5840	Prec@5 99.4580	
Val: [5]	Time 6.535	Data 0.106	Loss 0.501	Prec@1 82.9500	Prec@5 99.2800	
Best Prec@1: [83.110]	
Starting epoch number: 6 Learning rate: 0.1
Train: [6]	Time 109.648	Data 0.307	Loss 0.386	Prec@1 86.7120	Prec@5 99.4940	
Val: [6]	Time 6.567	Data 0.107	Loss 0.451	Prec@1 84.4200	Prec@5 99.4300	
Best Prec@1: [84.420]	
Starting epoch number: 7 Learning rate: 0.1
Train: [7]	Time 109.541	Data 0.289	Loss 0.354	Prec@1 87.8380	Prec@5 99.5920	
Val: [7]	Time 6.523	Data 0.098	Loss 0.672	Prec@1 79.0400	Prec@5 99.1200	
Best Prec@1: [84.420]	
Starting epoch number: 8 Learning rate: 0.1
Train: [8]	Time 109.439	Data 0.291	Loss 0.332	Prec@1 88.5260	Prec@5 99.6720	
Val: [8]	Time 6.540	Data 0.096	Loss 0.455	Prec@1 84.6200	Prec@5 99.5400	
Best Prec@1: [84.620]	
Starting epoch number: 9 Learning rate: 0.1
Train: [9]	Time 108.994	Data 0.305	Loss 0.312	Prec@1 89.1900	Prec@5 99.7000	
Val: [9]	Time 6.531	Data 0.099	Loss 0.449	Prec@1 84.4800	Prec@5 99.5600	
Best Prec@1: [84.620]	
Starting epoch number: 10 Learning rate: 0.1
Train: [10]	Time 108.744	Data 0.311	Loss 0.299	Prec@1 89.6680	Prec@5 99.7120	
Val: [10]	Time 6.509	Data 0.101	Loss 0.406	Prec@1 86.7100	Prec@5 99.6000	
Best Prec@1: [86.710]	
Starting epoch number: 11 Learning rate: 0.1
Train: [11]	Time 108.412	Data 0.293	Loss 0.287	Prec@1 90.0740	Prec@5 99.7460	
Val: [11]	Time 6.488	Data 0.094	Loss 0.687	Prec@1 80.1300	Prec@5 98.8900	
Best Prec@1: [86.710]	
Starting epoch number: 12 Learning rate: 0.1
Train: [12]	Time 108.519	Data 0.309	Loss 0.275	Prec@1 90.5300	Prec@5 99.7640	
Val: [12]	Time 6.438	Data 0.095	Loss 0.384	Prec@1 87.1900	Prec@5 99.6700	
Best Prec@1: [87.190]	
Starting epoch number: 13 Learning rate: 0.1
Train: [13]	Time 108.603	Data 0.316	Loss 0.273	Prec@1 90.5760	Prec@5 99.7740	
Val: [13]	Time 6.473	Data 0.104	Loss 0.411	Prec@1 86.8600	Prec@5 99.4300	
Best Prec@1: [87.190]	
Starting epoch number: 14 Learning rate: 0.1
Train: [14]	Time 109.104	Data 0.308	Loss 0.260	Prec@1 90.9280	Prec@5 99.7840	
Val: [14]	Time 6.533	Data 0.092	Loss 0.431	Prec@1 86.1700	Prec@5 99.5300	
Best Prec@1: [87.190]	
Starting epoch number: 15 Learning rate: 0.1
Train: [15]	Time 109.655	Data 0.301	Loss 0.252	Prec@1 91.3580	Prec@5 99.8280	
Val: [15]	Time 6.558	Data 0.098	Loss 0.411	Prec@1 87.0400	Prec@5 99.4600	
Best Prec@1: [87.190]	
Starting epoch number: 16 Learning rate: 0.1
Train: [16]	Time 109.689	Data 0.314	Loss 0.251	Prec@1 91.2500	Prec@5 99.8280	
Val: [16]	Time 6.557	Data 0.092	Loss 0.440	Prec@1 86.6100	Prec@5 99.5500	
Best Prec@1: [87.190]	
Starting epoch number: 17 Learning rate: 0.1
Train: [17]	Time 109.577	Data 0.315	Loss 0.242	Prec@1 91.7020	Prec@5 99.8360	
Val: [17]	Time 6.529	Data 0.095	Loss 0.376	Prec@1 88.1100	Prec@5 99.6400	
Best Prec@1: [88.110]	
Starting epoch number: 18 Learning rate: 0.1
Train: [18]	Time 109.469	Data 0.314	Loss 0.237	Prec@1 91.9340	Prec@5 99.8260	
Val: [18]	Time 6.545	Data 0.093	Loss 0.414	Prec@1 86.5800	Prec@5 99.5700	
Best Prec@1: [88.110]	
Starting epoch number: 19 Learning rate: 0.1
Train: [19]	Time 109.226	Data 0.306	Loss 0.236	Prec@1 91.8220	Prec@5 99.8600	
Val: [19]	Time 6.588	Data 0.108	Loss 0.401	Prec@1 86.9500	Prec@5 99.5100	
Best Prec@1: [88.110]	
Starting epoch number: 20 Learning rate: 0.1
Train: [20]	Time 109.116	Data 0.314	Loss 0.230	Prec@1 92.0600	Prec@5 99.8620	
Val: [20]	Time 6.526	Data 0.138	Loss 0.424	Prec@1 86.4900	Prec@5 99.4600	
Best Prec@1: [88.110]	
Starting epoch number: 21 Learning rate: 0.1
Train: [21]	Time 109.181	Data 0.325	Loss 0.226	Prec@1 92.2420	Prec@5 99.8480	
Val: [21]	Time 6.542	Data 0.095	Loss 0.391	Prec@1 87.0400	Prec@5 99.6100	
Best Prec@1: [88.110]	
Starting epoch number: 22 Learning rate: 0.1
Train: [22]	Time 109.311	Data 0.313	Loss 0.220	Prec@1 92.4460	Prec@5 99.8600	
Val: [22]	Time 6.524	Data 0.097	Loss 0.475	Prec@1 84.7600	Prec@5 99.4000	
Best Prec@1: [88.110]	
Starting epoch number: 23 Learning rate: 0.1
Train: [23]	Time 109.287	Data 0.297	Loss 0.215	Prec@1 92.3760	Prec@5 99.8700	
Val: [23]	Time 6.515	Data 0.109	Loss 0.419	Prec@1 87.1200	Prec@5 99.6900	
Best Prec@1: [88.110]	
Starting epoch number: 24 Learning rate: 0.1
Train: [24]	Time 109.253	Data 0.296	Loss 0.216	Prec@1 92.5700	Prec@5 99.8760	
Val: [24]	Time 6.533	Data 0.098	Loss 0.372	Prec@1 88.3600	Prec@5 99.6000	
Best Prec@1: [88.360]	
Starting epoch number: 25 Learning rate: 0.1
Train: [25]	Time 109.400	Data 0.397	Loss 0.210	Prec@1 92.7300	Prec@5 99.8460	
Val: [25]	Time 6.668	Data 0.266	Loss 0.432	Prec@1 86.6100	Prec@5 99.4400	
Best Prec@1: [88.360]	
Starting epoch number: 26 Learning rate: 0.1
Train: [26]	Time 109.414	Data 0.410	Loss 0.207	Prec@1 92.7200	Prec@5 99.8760	
Val: [26]	Time 6.671	Data 0.186	Loss 0.378	Prec@1 87.8200	Prec@5 99.5600	
Best Prec@1: [88.360]	
Starting epoch number: 27 Learning rate: 0.1
Train: [27]	Time 109.657	Data 0.413	Loss 0.207	Prec@1 92.8980	Prec@5 99.8620	
Val: [27]	Time 6.642	Data 0.187	Loss 0.324	Prec@1 89.2300	Prec@5 99.7000	
Best Prec@1: [89.230]	
Starting epoch number: 28 Learning rate: 0.1
Train: [28]	Time 109.849	Data 0.500	Loss 0.200	Prec@1 92.9840	Prec@5 99.8940	
Val: [28]	Time 6.662	Data 0.326	Loss 0.363	Prec@1 88.9500	Prec@5 99.4800	
Best Prec@1: [89.230]	
Starting epoch number: 29 Learning rate: 0.1
Train: [29]	Time 109.830	Data 0.512	Loss 0.203	Prec@1 92.9520	Prec@5 99.9000	
Val: [29]	Time 6.616	Data 0.234	Loss 0.352	Prec@1 88.7900	Prec@5 99.6700	
Best Prec@1: [89.230]	
Starting epoch number: 30 Learning rate: 0.1
Train: [30]	Time 109.711	Data 0.510	Loss 0.200	Prec@1 92.9820	Prec@5 99.8940	
Val: [30]	Time 6.552	Data 0.232	Loss 0.413	Prec@1 87.0000	Prec@5 99.5700	
Best Prec@1: [89.230]	
Starting epoch number: 31 Learning rate: 0.1
Train: [31]	Time 109.663	Data 0.419	Loss 0.196	Prec@1 93.0960	Prec@5 99.9100	
Val: [31]	Time 6.558	Data 0.117	Loss 0.375	Prec@1 88.4800	Prec@5 99.5200	
Best Prec@1: [89.230]	
Starting epoch number: 32 Learning rate: 0.1
Train: [32]	Time 109.654	Data 0.414	Loss 0.193	Prec@1 93.3360	Prec@5 99.9140	
Val: [32]	Time 6.597	Data 0.155	Loss 0.368	Prec@1 88.8200	Prec@5 99.4800	
Best Prec@1: [89.230]	
Starting epoch number: 33 Learning rate: 0.1
Train: [33]	Time 109.823	Data 0.406	Loss 0.191	Prec@1 93.4020	Prec@5 99.9020	
Val: [33]	Time 6.553	Data 0.127	Loss 0.377	Prec@1 88.9100	Prec@5 99.6100	
Best Prec@1: [89.230]	
Starting epoch number: 34 Learning rate: 0.1
Train: [34]	Time 109.863	Data 0.414	Loss 0.187	Prec@1 93.4800	Prec@5 99.9180	
Val: [34]	Time 6.689	Data 0.189	Loss 0.339	Prec@1 89.2400	Prec@5 99.6600	
Best Prec@1: [89.240]	
Starting epoch number: 35 Learning rate: 0.1
Train: [35]	Time 109.906	Data 0.414	Loss 0.191	Prec@1 93.3560	Prec@5 99.9060	
Val: [35]	Time 6.550	Data 0.113	Loss 0.372	Prec@1 88.5000	Prec@5 99.5400	
Best Prec@1: [89.240]	
Starting epoch number: 36 Learning rate: 0.1
Train: [36]	Time 109.952	Data 0.433	Loss 0.188	Prec@1 93.3280	Prec@5 99.9000	
Val: [36]	Time 6.580	Data 0.129	Loss 0.372	Prec@1 87.7500	Prec@5 99.6700	
Best Prec@1: [89.240]	
Starting epoch number: 37 Learning rate: 0.1
Train: [37]	Time 109.681	Data 0.442	Loss 0.179	Prec@1 93.7420	Prec@5 99.9240	
Val: [37]	Time 6.560	Data 0.113	Loss 0.382	Prec@1 88.1800	Prec@5 99.6600	
Best Prec@1: [89.240]	
Starting epoch number: 38 Learning rate: 0.1
Train: [38]	Time 109.607	Data 0.448	Loss 0.181	Prec@1 93.7020	Prec@5 99.9120	
Val: [38]	Time 6.555	Data 0.105	Loss 0.452	Prec@1 87.0600	Prec@5 99.3300	
Best Prec@1: [89.240]	
Starting epoch number: 39 Learning rate: 0.1
Train: [39]	Time 109.672	Data 0.415	Loss 0.181	Prec@1 93.7180	Prec@5 99.9120	
Val: [39]	Time 6.585	Data 0.128	Loss 0.375	Prec@1 88.3500	Prec@5 99.4400	
Best Prec@1: [89.240]	
Starting epoch number: 40 Learning rate: 0.1
Train: [40]	Time 109.450	Data 0.446	Loss 0.183	Prec@1 93.5160	Prec@5 99.9280	
Val: [40]	Time 6.516	Data 0.118	Loss 0.383	Prec@1 88.3700	Prec@5 99.3600	
Best Prec@1: [89.240]	
Starting epoch number: 41 Learning rate: 0.1
Train: [41]	Time 109.470	Data 0.424	Loss 0.174	Prec@1 93.8460	Prec@5 99.9120	
Val: [41]	Time 6.562	Data 0.117	Loss 0.386	Prec@1 87.9600	Prec@5 99.6000	
Best Prec@1: [89.240]	
Starting epoch number: 42 Learning rate: 0.1
Train: [42]	Time 109.467	Data 0.411	Loss 0.177	Prec@1 93.7980	Prec@5 99.9320	
Val: [42]	Time 6.531	Data 0.181	Loss 0.337	Prec@1 90.0100	Prec@5 99.6200	
Best Prec@1: [90.010]	
Starting epoch number: 43 Learning rate: 0.1
Train: [43]	Time 109.613	Data 0.422	Loss 0.177	Prec@1 93.8060	Prec@5 99.9400	
Val: [43]	Time 6.546	Data 0.113	Loss 0.386	Prec@1 88.1300	Prec@5 99.7100	
Best Prec@1: [90.010]	
Starting epoch number: 44 Learning rate: 0.1
Train: [44]	Time 109.635	Data 0.404	Loss 0.172	Prec@1 94.0640	Prec@5 99.9240	
Val: [44]	Time 6.566	Data 0.126	Loss 0.343	Prec@1 89.2400	Prec@5 99.6000	
Best Prec@1: [90.010]	
Starting epoch number: 45 Learning rate: 0.1
Train: [45]	Time 109.788	Data 0.418	Loss 0.173	Prec@1 93.9880	Prec@5 99.9080	
Val: [45]	Time 6.594	Data 0.118	Loss 0.289	Prec@1 90.8000	Prec@5 99.8400	
Best Prec@1: [90.800]	
Starting epoch number: 46 Learning rate: 0.1
Train: [46]	Time 109.684	Data 0.423	Loss 0.174	Prec@1 93.9600	Prec@5 99.9300	
Val: [46]	Time 6.714	Data 0.219	Loss 0.350	Prec@1 88.9600	Prec@5 99.7200	
Best Prec@1: [90.800]	
Starting epoch number: 47 Learning rate: 0.1
Train: [47]	Time 109.672	Data 0.416	Loss 0.173	Prec@1 93.8980	Prec@5 99.9300	
Val: [47]	Time 6.819	Data 0.260	Loss 0.314	Prec@1 89.7800	Prec@5 99.7000	
Best Prec@1: [90.800]	
Starting epoch number: 48 Learning rate: 0.1
Train: [48]	Time 109.721	Data 0.402	Loss 0.170	Prec@1 94.1020	Prec@5 99.9280	
Val: [48]	Time 6.604	Data 0.184	Loss 0.367	Prec@1 88.7500	Prec@5 99.6000	
Best Prec@1: [90.800]	
Starting epoch number: 49 Learning rate: 0.1
Train: [49]	Time 109.762	Data 0.418	Loss 0.169	Prec@1 94.0900	Prec@5 99.9400	
Val: [49]	Time 6.630	Data 0.207	Loss 0.416	Prec@1 88.1900	Prec@5 99.4300	
Best Prec@1: [90.800]	
Starting epoch number: 50 Learning rate: 0.1
Train: [50]	Time 109.799	Data 0.425	Loss 0.168	Prec@1 94.0700	Prec@5 99.9480	
Val: [50]	Time 6.571	Data 0.146	Loss 0.432	Prec@1 87.9900	Prec@5 99.2800	
Best Prec@1: [90.800]	
Starting epoch number: 51 Learning rate: 0.1
Train: [51]	Time 109.618	Data 0.384	Loss 0.167	Prec@1 94.2380	Prec@5 99.9280	
Val: [51]	Time 6.556	Data 0.119	Loss 0.324	Prec@1 90.0500	Prec@5 99.6800	
Best Prec@1: [90.800]	
Starting epoch number: 52 Learning rate: 0.1
Train: [52]	Time 109.522	Data 0.429	Loss 0.163	Prec@1 94.3720	Prec@5 99.9260	
Val: [52]	Time 6.578	Data 0.129	Loss 0.404	Prec@1 88.4400	Prec@5 99.5700	
Best Prec@1: [90.800]	
Starting epoch number: 53 Learning rate: 0.1
Train: [53]	Time 109.526	Data 0.445	Loss 0.165	Prec@1 94.2180	Prec@5 99.9480	
Val: [53]	Time 6.621	Data 0.130	Loss 0.310	Prec@1 90.5500	Prec@5 99.6700	
Best Prec@1: [90.800]	
Starting epoch number: 54 Learning rate: 0.1
Train: [54]	Time 109.297	Data 0.396	Loss 0.159	Prec@1 94.5380	Prec@5 99.9260	
Val: [54]	Time 6.520	Data 0.116	Loss 0.369	Prec@1 88.8700	Prec@5 99.5700	
Best Prec@1: [90.800]	
Starting epoch number: 55 Learning rate: 0.1
Train: [55]	Time 109.076	Data 0.256	Loss 0.161	Prec@1 94.4220	Prec@5 99.9320	
Val: [55]	Time 6.533	Data 0.119	Loss 0.343	Prec@1 89.3000	Prec@5 99.7600	
Best Prec@1: [90.800]	
Starting epoch number: 56 Learning rate: 0.1
Train: [56]	Time 109.150	Data 0.250	Loss 0.164	Prec@1 94.2060	Prec@5 99.9480	
Val: [56]	Time 6.493	Data 0.098	Loss 0.377	Prec@1 88.7800	Prec@5 99.6500	
Best Prec@1: [90.800]	
Starting epoch number: 57 Learning rate: 0.1
Train: [57]	Time 109.087	Data 0.246	Loss 0.164	Prec@1 94.2700	Prec@5 99.9240	
Val: [57]	Time 6.469	Data 0.123	Loss 0.411	Prec@1 88.1900	Prec@5 99.3100	
Best Prec@1: [90.800]	
Starting epoch number: 58 Learning rate: 0.1
Train: [58]	Time 109.281	Data 0.279	Loss 0.165	Prec@1 94.3140	Prec@5 99.9180	
Val: [58]	Time 6.509	Data 0.099	Loss 0.358	Prec@1 88.9200	Prec@5 99.6900	
Best Prec@1: [90.800]	
Starting epoch number: 59 Learning rate: 0.1
Train: [59]	Time 109.049	Data 0.274	Loss 0.157	Prec@1 94.5640	Prec@5 99.9320	
Val: [59]	Time 6.511	Data 0.101	Loss 0.401	Prec@1 88.7300	Prec@5 99.5400	
Best Prec@1: [90.800]	
Starting epoch number: 60 Learning rate: 0.1
Train: [60]	Time 109.032	Data 0.283	Loss 0.166	Prec@1 94.0620	Prec@5 99.9300	
Val: [60]	Time 6.626	Data 0.127	Loss 0.343	Prec@1 89.9500	Prec@5 99.7200	
Best Prec@1: [90.800]	
Starting epoch number: 61 Learning rate: 0.1
Train: [61]	Time 109.236	Data 0.305	Loss 0.155	Prec@1 94.5640	Prec@5 99.9240	
Val: [61]	Time 6.447	Data 0.119	Loss 0.394	Prec@1 87.6800	Prec@5 99.6500	
Best Prec@1: [90.800]	
Starting epoch number: 62 Learning rate: 0.1
Train: [62]	Time 109.178	Data 0.295	Loss 0.164	Prec@1 94.3920	Prec@5 99.9320	
Val: [62]	Time 6.592	Data 0.198	Loss 0.391	Prec@1 87.7700	Prec@5 99.6300	
Best Prec@1: [90.800]	
Starting epoch number: 63 Learning rate: 0.1
Train: [63]	Time 109.097	Data 0.297	Loss 0.154	Prec@1 94.6700	Prec@5 99.9300	
Val: [63]	Time 6.542	Data 0.128	Loss 0.375	Prec@1 88.9000	Prec@5 99.6200	
Best Prec@1: [90.800]	
Starting epoch number: 64 Learning rate: 0.1
Train: [64]	Time 109.024	Data 0.296	Loss 0.153	Prec@1 94.6720	Prec@5 99.9240	
Val: [64]	Time 6.698	Data 0.129	Loss 0.480	Prec@1 86.6500	Prec@5 99.5700	
Best Prec@1: [90.800]	
Starting epoch number: 65 Learning rate: 0.1
Train: [65]	Time 109.163	Data 0.294	Loss 0.159	Prec@1 94.5520	Prec@5 99.9340	
Val: [65]	Time 6.523	Data 0.109	Loss 0.456	Prec@1 86.5700	Prec@5 99.5200	
Best Prec@1: [90.800]	
Starting epoch number: 66 Learning rate: 0.1
Train: [66]	Time 109.315	Data 0.305	Loss 0.155	Prec@1 94.6300	Prec@5 99.9520	
Val: [66]	Time 6.540	Data 0.133	Loss 0.364	Prec@1 88.4300	Prec@5 99.6900	
Best Prec@1: [90.800]	
Starting epoch number: 67 Learning rate: 0.1
Train: [67]	Time 109.374	Data 0.287	Loss 0.157	Prec@1 94.5340	Prec@5 99.9280	
Val: [67]	Time 6.534	Data 0.121	Loss 0.323	Prec@1 90.7000	Prec@5 99.7900	
Best Prec@1: [90.800]	
Starting epoch number: 68 Learning rate: 0.1
Train: [68]	Time 109.421	Data 0.288	Loss 0.152	Prec@1 94.7360	Prec@5 99.9580	
Val: [68]	Time 6.556	Data 0.121	Loss 0.388	Prec@1 88.8400	Prec@5 99.7800	
Best Prec@1: [90.800]	
Starting epoch number: 69 Learning rate: 0.1
Train: [69]	Time 109.466	Data 0.310	Loss 0.157	Prec@1 94.5320	Prec@5 99.9440	
Val: [69]	Time 6.581	Data 0.147	Loss 0.359	Prec@1 89.2300	Prec@5 99.4900	
Best Prec@1: [90.800]	
Starting epoch number: 70 Learning rate: 0.1
Train: [70]	Time 109.402	Data 0.297	Loss 0.156	Prec@1 94.6400	Prec@5 99.9460	
Val: [70]	Time 6.545	Data 0.133	Loss 0.359	Prec@1 88.7700	Prec@5 99.7200	
Best Prec@1: [90.800]	
Starting epoch number: 71 Learning rate: 0.1
Train: [71]	Time 109.332	Data 0.303	Loss 0.154	Prec@1 94.5460	Prec@5 99.9400	
Val: [71]	Time 6.536	Data 0.127	Loss 0.345	Prec@1 89.6500	Prec@5 99.6400	
Best Prec@1: [90.800]	
Starting epoch number: 72 Learning rate: 0.1
Train: [72]	Time 109.429	Data 0.319	Loss 0.153	Prec@1 94.6560	Prec@5 99.9460	
Val: [72]	Time 6.526	Data 0.120	Loss 0.423	Prec@1 88.4600	Prec@5 99.6300	
Best Prec@1: [90.800]	
Starting epoch number: 73 Learning rate: 0.1
Train: [73]	Time 109.312	Data 0.292	Loss 0.153	Prec@1 94.6480	Prec@5 99.9420	
Val: [73]	Time 6.677	Data 0.165	Loss 0.299	Prec@1 90.9200	Prec@5 99.7200	
Best Prec@1: [90.920]	
Starting epoch number: 74 Learning rate: 0.1
Train: [74]	Time 109.187	Data 0.293	Loss 0.150	Prec@1 94.7800	Prec@5 99.9320	
Val: [74]	Time 6.580	Data 0.150	Loss 0.342	Prec@1 89.8100	Prec@5 99.6100	
Best Prec@1: [90.920]	
Starting epoch number: 75 Learning rate: 0.1
Train: [75]	Time 109.126	Data 0.305	Loss 0.150	Prec@1 94.7300	Prec@5 99.9540	
Val: [75]	Time 6.536	Data 0.125	Loss 0.327	Prec@1 89.9600	Prec@5 99.6700	
Best Prec@1: [90.920]	
Starting epoch number: 76 Learning rate: 0.1
Train: [76]	Time 109.192	Data 0.311	Loss 0.151	Prec@1 94.7980	Prec@5 99.9420	
Val: [76]	Time 6.538	Data 0.129	Loss 0.347	Prec@1 89.9300	Prec@5 99.6200	
Best Prec@1: [90.920]	
Starting epoch number: 77 Learning rate: 0.1
Train: [77]	Time 109.180	Data 0.297	Loss 0.151	Prec@1 94.8220	Prec@5 99.9320	
Val: [77]	Time 6.610	Data 0.141	Loss 0.291	Prec@1 90.9500	Prec@5 99.6900	
Best Prec@1: [90.950]	
Starting epoch number: 78 Learning rate: 0.1
Train: [78]	Time 109.330	Data 0.299	Loss 0.151	Prec@1 94.8200	Prec@5 99.9460	
Val: [78]	Time 6.568	Data 0.117	Loss 0.330	Prec@1 90.2000	Prec@5 99.6600	
Best Prec@1: [90.950]	
Starting epoch number: 79 Learning rate: 0.1
Train: [79]	Time 109.427	Data 0.300	Loss 0.151	Prec@1 94.7860	Prec@5 99.9520	
Val: [79]	Time 6.557	Data 0.125	Loss 0.318	Prec@1 90.3100	Prec@5 99.5200	
Best Prec@1: [90.950]	
Starting epoch number: 80 Learning rate: 0.1
Train: [80]	Time 109.436	Data 0.305	Loss 0.148	Prec@1 94.8760	Prec@5 99.9480	
Val: [80]	Time 6.551	Data 0.119	Loss 0.347	Prec@1 89.6400	Prec@5 99.6800	
Best Prec@1: [90.950]	
Starting epoch number: 81 Learning rate: 0.1
Train: [81]	Time 109.495	Data 0.286	Loss 0.149	Prec@1 94.7340	Prec@5 99.9580	
Val: [81]	Time 6.545	Data 0.123	Loss 0.290	Prec@1 91.1800	Prec@5 99.6500	
Best Prec@1: [91.180]	
Starting epoch number: 82 Learning rate: 0.1
Train: [82]	Time 109.444	Data 0.302	Loss 0.145	Prec@1 94.9060	Prec@5 99.9460	
Val: [82]	Time 6.598	Data 0.124	Loss 0.327	Prec@1 90.2200	Prec@5 99.6700	
Best Prec@1: [91.180]	
Starting epoch number: 83 Learning rate: 0.1
Train: [83]	Time 109.253	Data 0.301	Loss 0.149	Prec@1 94.8960	Prec@5 99.9480	
Val: [83]	Time 6.532	Data 0.115	Loss 0.421	Prec@1 88.3400	Prec@5 99.6900	
Best Prec@1: [91.180]	
Starting epoch number: 84 Learning rate: 0.1
Train: [84]	Time 109.350	Data 0.306	Loss 0.148	Prec@1 94.8000	Prec@5 99.9500	
Val: [84]	Time 6.689	Data 0.219	Loss 0.333	Prec@1 90.0800	Prec@5 99.6800	
Best Prec@1: [91.180]	
Starting epoch number: 85 Learning rate: 0.1
Train: [85]	Time 109.445	Data 0.310	Loss 0.144	Prec@1 95.0240	Prec@5 99.9380	
Val: [85]	Time 6.552	Data 0.118	Loss 0.332	Prec@1 89.8100	Prec@5 99.7000	
Best Prec@1: [91.180]	
Starting epoch number: 86 Learning rate: 0.1
Train: [86]	Time 109.199	Data 0.295	Loss 0.147	Prec@1 94.9160	Prec@5 99.9320	
Val: [86]	Time 6.543	Data 0.154	Loss 0.310	Prec@1 90.4900	Prec@5 99.6900	
Best Prec@1: [91.180]	
Starting epoch number: 87 Learning rate: 0.1
Train: [87]	Time 109.069	Data 0.289	Loss 0.144	Prec@1 94.9920	Prec@5 99.9440	
Val: [87]	Time 6.557	Data 0.131	Loss 0.298	Prec@1 90.8100	Prec@5 99.8100	
Best Prec@1: [91.180]	
Starting epoch number: 88 Learning rate: 0.1
Train: [88]	Time 109.109	Data 0.302	Loss 0.150	Prec@1 94.7600	Prec@5 99.9460	
Val: [88]	Time 6.539	Data 0.131	Loss 0.347	Prec@1 89.6600	Prec@5 99.6800	
Best Prec@1: [91.180]	
Starting epoch number: 89 Learning rate: 0.1
Train: [89]	Time 109.183	Data 0.288	Loss 0.148	Prec@1 94.8220	Prec@5 99.9320	
Val: [89]	Time 6.639	Data 0.118	Loss 0.335	Prec@1 89.9600	Prec@5 99.6900	
Best Prec@1: [91.180]	
Starting epoch number: 90 Learning rate: 0.1
Train: [90]	Time 109.309	Data 0.300	Loss 0.144	Prec@1 95.0180	Prec@5 99.9460	
Val: [90]	Time 6.510	Data 0.125	Loss 0.359	Prec@1 89.6100	Prec@5 99.7900	
Best Prec@1: [91.180]	
Starting epoch number: 91 Learning rate: 0.1
Train: [91]	Time 109.366	Data 0.306	Loss 0.149	Prec@1 94.7360	Prec@5 99.9500	
Val: [91]	Time 6.522	Data 0.114	Loss 0.376	Prec@1 89.1600	Prec@5 99.5100	
Best Prec@1: [91.180]	
Starting epoch number: 92 Learning rate: 0.1
Train: [92]	Time 109.420	Data 0.290	Loss 0.138	Prec@1 95.2180	Prec@5 99.9620	
Val: [92]	Time 6.561	Data 0.138	Loss 0.393	Prec@1 88.0800	Prec@5 99.4300	
Best Prec@1: [91.180]	
Starting epoch number: 93 Learning rate: 0.1
Train: [93]	Time 109.340	Data 0.290	Loss 0.146	Prec@1 94.8800	Prec@5 99.9620	
Val: [93]	Time 6.628	Data 0.128	Loss 0.346	Prec@1 89.9200	Prec@5 99.7400	
Best Prec@1: [91.180]	
Starting epoch number: 94 Learning rate: 0.1
Train: [94]	Time 109.508	Data 0.347	Loss 0.141	Prec@1 95.1120	Prec@5 99.9460	
Val: [94]	Time 6.533	Data 0.122	Loss 0.335	Prec@1 90.0800	Prec@5 99.7600	
Best Prec@1: [91.180]	
Starting epoch number: 95 Learning rate: 0.1
Train: [95]	Time 109.573	Data 0.288	Loss 0.150	Prec@1 94.7540	Prec@5 99.9280	
Val: [95]	Time 6.603	Data 0.199	Loss 0.323	Prec@1 90.1700	Prec@5 99.5200	
Best Prec@1: [91.180]	
Starting epoch number: 96 Learning rate: 0.1
Train: [96]	Time 109.403	Data 0.315	Loss 0.142	Prec@1 95.1460	Prec@5 99.9620	
Val: [96]	Time 6.638	Data 0.156	Loss 0.307	Prec@1 91.1200	Prec@5 99.7200	
Best Prec@1: [91.180]	
Starting epoch number: 97 Learning rate: 0.1
Train: [97]	Time 109.311	Data 0.294	Loss 0.138	Prec@1 95.1660	Prec@5 99.9520	
Val: [97]	Time 6.575	Data 0.170	Loss 0.372	Prec@1 89.2700	Prec@5 99.5900	
Best Prec@1: [91.180]	
Starting epoch number: 98 Learning rate: 0.1
Train: [98]	Time 109.361	Data 0.300	Loss 0.144	Prec@1 95.0920	Prec@5 99.9300	
Val: [98]	Time 6.572	Data 0.175	Loss 0.334	Prec@1 89.8500	Prec@5 99.6000	
Best Prec@1: [91.180]	
Starting epoch number: 99 Learning rate: 0.1
Train: [99]	Time 109.219	Data 0.299	Loss 0.145	Prec@1 94.9300	Prec@5 99.9360	
Val: [99]	Time 6.548	Data 0.142	Loss 0.265	Prec@1 91.7600	Prec@5 99.7600	
Best Prec@1: [91.760]	
Starting epoch number: 100 Learning rate: 0.1
Train: [100]	Time 109.158	Data 0.291	Loss 0.143	Prec@1 95.0180	Prec@5 99.9540	
Val: [100]	Time 6.577	Data 0.149	Loss 0.317	Prec@1 90.2900	Prec@5 99.7900	
Best Prec@1: [91.760]	
Starting epoch number: 101 Learning rate: 0.1
Train: [101]	Time 109.196	Data 0.293	Loss 0.140	Prec@1 95.0440	Prec@5 99.9480	
Val: [101]	Time 6.524	Data 0.118	Loss 0.282	Prec@1 91.0200	Prec@5 99.7600	
Best Prec@1: [91.760]	
Starting epoch number: 102 Learning rate: 0.1
Train: [102]	Time 109.150	Data 0.288	Loss 0.147	Prec@1 94.8040	Prec@5 99.9440	
Val: [102]	Time 6.548	Data 0.149	Loss 0.395	Prec@1 88.2600	Prec@5 99.6800	
Best Prec@1: [91.760]	
Starting epoch number: 103 Learning rate: 0.1
Train: [103]	Time 109.135	Data 0.297	Loss 0.140	Prec@1 95.0860	Prec@5 99.9520	
Val: [103]	Time 6.518	Data 0.191	Loss 0.282	Prec@1 91.2700	Prec@5 99.8000	
Best Prec@1: [91.760]	
Starting epoch number: 104 Learning rate: 0.1
Train: [104]	Time 109.353	Data 0.294	Loss 0.136	Prec@1 95.2560	Prec@5 99.9520	
Val: [104]	Time 6.547	Data 0.127	Loss 0.345	Prec@1 89.7900	Prec@5 99.6900	
Best Prec@1: [91.760]	
Starting epoch number: 105 Learning rate: 0.1
Train: [105]	Time 109.409	Data 0.297	Loss 0.145	Prec@1 94.8820	Prec@5 99.9520	
Val: [105]	Time 6.540	Data 0.120	Loss 0.303	Prec@1 90.7500	Prec@5 99.7200	
Best Prec@1: [91.760]	
Starting epoch number: 106 Learning rate: 0.1
Train: [106]	Time 109.420	Data 0.296	Loss 0.139	Prec@1 95.0900	Prec@5 99.9620	
Val: [106]	Time 6.530	Data 0.129	Loss 0.322	Prec@1 90.4200	Prec@5 99.7800	
Best Prec@1: [91.760]	
Starting epoch number: 107 Learning rate: 0.1
Train: [107]	Time 109.474	Data 0.307	Loss 0.143	Prec@1 95.0060	Prec@5 99.9520	
Val: [107]	Time 6.472	Data 0.135	Loss 0.470	Prec@1 86.8300	Prec@5 99.6100	
Best Prec@1: [91.760]	
Starting epoch number: 108 Learning rate: 0.1
Train: [108]	Time 109.470	Data 0.298	Loss 0.145	Prec@1 94.9340	Prec@5 99.9500	
Val: [108]	Time 6.578	Data 0.158	Loss 0.301	Prec@1 90.7500	Prec@5 99.7200	
Best Prec@1: [91.760]	
Starting epoch number: 109 Learning rate: 0.1
Train: [109]	Time 109.218	Data 0.298	Loss 0.141	Prec@1 95.0300	Prec@5 99.9420	
Val: [109]	Time 6.651	Data 0.116	Loss 0.313	Prec@1 90.2900	Prec@5 99.7400	
Best Prec@1: [91.760]	
Starting epoch number: 110 Learning rate: 0.1
Train: [110]	Time 109.376	Data 0.289	Loss 0.141	Prec@1 95.0740	Prec@5 99.9460	
Val: [110]	Time 6.501	Data 0.123	Loss 0.305	Prec@1 91.0900	Prec@5 99.8400	
Best Prec@1: [91.760]	
Starting epoch number: 111 Learning rate: 0.1
Train: [111]	Time 109.356	Data 0.293	Loss 0.142	Prec@1 95.0900	Prec@5 99.9420	
Val: [111]	Time 6.547	Data 0.137	Loss 0.329	Prec@1 90.0600	Prec@5 99.6800	
Best Prec@1: [91.760]	
Starting epoch number: 112 Learning rate: 0.1
Train: [112]	Time 109.224	Data 0.309	Loss 0.142	Prec@1 95.0700	Prec@5 99.9360	
Val: [112]	Time 6.513	Data 0.120	Loss 0.298	Prec@1 90.7400	Prec@5 99.7400	
Best Prec@1: [91.760]	
Starting epoch number: 113 Learning rate: 0.1
Train: [113]	Time 109.099	Data 0.289	Loss 0.137	Prec@1 95.2440	Prec@5 99.9540	
Val: [113]	Time 6.542	Data 0.129	Loss 0.304	Prec@1 90.7300	Prec@5 99.7600	
Best Prec@1: [91.760]	
Starting epoch number: 114 Learning rate: 0.1
Train: [114]	Time 109.108	Data 0.288	Loss 0.138	Prec@1 95.2060	Prec@5 99.9420	
Val: [114]	Time 6.552	Data 0.160	Loss 0.343	Prec@1 89.7200	Prec@5 99.6800	
Best Prec@1: [91.760]	
Starting epoch number: 115 Learning rate: 0.1
Train: [115]	Time 109.188	Data 0.298	Loss 0.141	Prec@1 95.0820	Prec@5 99.9480	
Val: [115]	Time 6.527	Data 0.133	Loss 0.377	Prec@1 88.8400	Prec@5 99.6500	
Best Prec@1: [91.760]	
Starting epoch number: 116 Learning rate: 0.1
Train: [116]	Time 109.159	Data 0.289	Loss 0.141	Prec@1 95.0360	Prec@5 99.9480	
Val: [116]	Time 6.603	Data 0.186	Loss 0.319	Prec@1 90.8400	Prec@5 99.7100	
Best Prec@1: [91.760]	
Starting epoch number: 117 Learning rate: 0.1
Train: [117]	Time 109.227	Data 0.288	Loss 0.142	Prec@1 95.1220	Prec@5 99.9440	
Val: [117]	Time 6.537	Data 0.129	Loss 0.376	Prec@1 88.5300	Prec@5 99.4800	
Best Prec@1: [91.760]	
Starting epoch number: 118 Learning rate: 0.1
Train: [118]	Time 109.278	Data 0.301	Loss 0.140	Prec@1 95.2260	Prec@5 99.9380	
Val: [118]	Time 6.567	Data 0.146	Loss 0.362	Prec@1 88.9200	Prec@5 99.7100	
Best Prec@1: [91.760]	
Starting epoch number: 119 Learning rate: 0.1
Train: [119]	Time 109.263	Data 0.301	Loss 0.138	Prec@1 95.2180	Prec@5 99.9640	
Val: [119]	Time 6.550	Data 0.125	Loss 0.375	Prec@1 89.5300	Prec@5 99.5300	
Best Prec@1: [91.760]	
Starting epoch number: 120 Learning rate: 0.1
Train: [120]	Time 109.278	Data 0.346	Loss 0.135	Prec@1 95.2900	Prec@5 99.9400	
Val: [120]	Time 6.536	Data 0.129	Loss 0.387	Prec@1 88.5600	Prec@5 99.5300	
Best Prec@1: [91.760]	
Starting epoch number: 121 Learning rate: 0.1
Train: [121]	Time 109.147	Data 0.343	Loss 0.136	Prec@1 95.2120	Prec@5 99.9480	
Val: [121]	Time 6.631	Data 0.225	Loss 0.611	Prec@1 84.4600	Prec@5 99.5100	
Best Prec@1: [91.760]	
Starting epoch number: 122 Learning rate: 0.1
Train: [122]	Time 109.076	Data 0.339	Loss 0.142	Prec@1 95.0820	Prec@5 99.9540	
Val: [122]	Time 6.651	Data 0.118	Loss 0.403	Prec@1 87.6800	Prec@5 99.5700	
Best Prec@1: [91.760]	
Starting epoch number: 123 Learning rate: 0.1
Train: [123]	Time 108.987	Data 0.320	Loss 0.141	Prec@1 95.0480	Prec@5 99.9480	
Val: [123]	Time 6.521	Data 0.108	Loss 0.316	Prec@1 90.6600	Prec@5 99.7400	
Best Prec@1: [91.760]	
Starting epoch number: 124 Learning rate: 0.1
Train: [124]	Time 108.930	Data 0.315	Loss 0.139	Prec@1 95.1380	Prec@5 99.9420	
Val: [124]	Time 6.508	Data 0.129	Loss 0.348	Prec@1 89.6200	Prec@5 99.7300	
Best Prec@1: [91.760]	
Starting epoch number: 125 Learning rate: 0.1
Train: [125]	Time 108.926	Data 0.329	Loss 0.137	Prec@1 95.1500	Prec@5 99.9580	
Val: [125]	Time 6.543	Data 0.105	Loss 0.381	Prec@1 89.1800	Prec@5 99.5500	
Best Prec@1: [91.760]	
Starting epoch number: 126 Learning rate: 0.1
Train: [126]	Time 109.060	Data 0.308	Loss 0.137	Prec@1 95.2580	Prec@5 99.9600	
Val: [126]	Time 6.497	Data 0.115	Loss 0.341	Prec@1 89.7500	Prec@5 99.6200	
Best Prec@1: [91.760]	
Starting epoch number: 127 Learning rate: 0.1
Train: [127]	Time 109.196	Data 0.330	Loss 0.137	Prec@1 95.2380	Prec@5 99.9660	
Val: [127]	Time 6.541	Data 0.122	Loss 0.295	Prec@1 90.7700	Prec@5 99.8200	
Best Prec@1: [91.760]	
Starting epoch number: 128 Learning rate: 0.1
Train: [128]	Time 109.267	Data 0.316	Loss 0.136	Prec@1 95.1900	Prec@5 99.9600	
Val: [128]	Time 6.513	Data 0.121	Loss 0.325	Prec@1 90.5400	Prec@5 99.7400	
Best Prec@1: [91.760]	
Starting epoch number: 129 Learning rate: 0.1
Train: [129]	Time 109.278	Data 0.329	Loss 0.137	Prec@1 95.2080	Prec@5 99.9580	
Val: [129]	Time 6.516	Data 0.105	Loss 0.317	Prec@1 90.5900	Prec@5 99.6100	
Best Prec@1: [91.760]	
Starting epoch number: 130 Learning rate: 0.1
Train: [130]	Time 109.169	Data 0.282	Loss 0.139	Prec@1 95.1420	Prec@5 99.9500	
Val: [130]	Time 6.538	Data 0.132	Loss 0.322	Prec@1 90.4400	Prec@5 99.5800	
Best Prec@1: [91.760]	
Starting epoch number: 131 Learning rate: 0.1
Train: [131]	Time 109.084	Data 0.271	Loss 0.136	Prec@1 95.3420	Prec@5 99.9560	
Val: [131]	Time 6.544	Data 0.126	Loss 0.320	Prec@1 90.8700	Prec@5 99.7500	
Best Prec@1: [91.760]	
Starting epoch number: 132 Learning rate: 0.1
Train: [132]	Time 108.979	Data 0.287	Loss 0.141	Prec@1 95.1080	Prec@5 99.9420	
Val: [132]	Time 6.537	Data 0.161	Loss 0.292	Prec@1 90.6800	Prec@5 99.7600	
Best Prec@1: [91.760]	
Starting epoch number: 133 Learning rate: 0.1
Train: [133]	Time 108.886	Data 0.271	Loss 0.134	Prec@1 95.3360	Prec@5 99.9600	
Val: [133]	Time 6.509	Data 0.097	Loss 0.315	Prec@1 90.2300	Prec@5 99.7100	
Best Prec@1: [91.760]	
Starting epoch number: 134 Learning rate: 0.1
Train: [134]	Time 108.839	Data 0.282	Loss 0.137	Prec@1 95.1040	Prec@5 99.9520	
Val: [134]	Time 6.586	Data 0.099	Loss 0.378	Prec@1 89.0900	Prec@5 99.7100	
Best Prec@1: [91.760]	
Starting epoch number: 135 Learning rate: 0.1
Train: [135]	Time 108.822	Data 0.270	Loss 0.135	Prec@1 95.3780	Prec@5 99.9600	
Val: [135]	Time 6.473	Data 0.093	Loss 0.322	Prec@1 90.5200	Prec@5 99.7000	
Best Prec@1: [91.760]	
Starting epoch number: 136 Learning rate: 0.1
Train: [136]	Time 108.664	Data 0.266	Loss 0.135	Prec@1 95.3760	Prec@5 99.9400	
Val: [136]	Time 6.463	Data 0.115	Loss 0.353	Prec@1 89.1300	Prec@5 99.7800	
Best Prec@1: [91.760]	
Starting epoch number: 137 Learning rate: 0.1
Train: [137]	Time 108.644	Data 0.269	Loss 0.139	Prec@1 95.0760	Prec@5 99.9580	
Val: [137]	Time 6.486	Data 0.093	Loss 0.442	Prec@1 88.0000	Prec@5 99.6200	
Best Prec@1: [91.760]	
Starting epoch number: 138 Learning rate: 0.1
Train: [138]	Time 108.674	Data 0.274	Loss 0.131	Prec@1 95.5180	Prec@5 99.9620	
Val: [138]	Time 6.479	Data 0.114	Loss 0.314	Prec@1 90.5900	Prec@5 99.7100	
Best Prec@1: [91.760]	
Starting epoch number: 139 Learning rate: 0.1
Train: [139]	Time 108.763	Data 0.281	Loss 0.135	Prec@1 95.2980	Prec@5 99.9780	
Val: [139]	Time 6.481	Data 0.146	Loss 0.432	Prec@1 88.1200	Prec@5 99.5900	
Best Prec@1: [91.760]	
Starting epoch number: 140 Learning rate: 0.1
Train: [140]	Time 108.836	Data 0.277	Loss 0.136	Prec@1 95.2620	Prec@5 99.9580	
Val: [140]	Time 6.491	Data 0.101	Loss 0.339	Prec@1 90.2100	Prec@5 99.5500	
Best Prec@1: [91.760]	
Starting epoch number: 141 Learning rate: 0.1
Train: [141]	Time 108.877	Data 0.265	Loss 0.134	Prec@1 95.3560	Prec@5 99.9420	
Val: [141]	Time 6.488	Data 0.098	Loss 0.388	Prec@1 89.2800	Prec@5 99.6300	
Best Prec@1: [91.760]	
Starting epoch number: 142 Learning rate: 0.1
Train: [142]	Time 108.931	Data 0.279	Loss 0.139	Prec@1 95.3000	Prec@5 99.9480	
Val: [142]	Time 6.509	Data 0.103	Loss 0.316	Prec@1 90.3700	Prec@5 99.6800	
Best Prec@1: [91.760]	
Starting epoch number: 143 Learning rate: 0.1
Train: [143]	Time 108.980	Data 0.264	Loss 0.134	Prec@1 95.4320	Prec@5 99.9520	
Val: [143]	Time 6.537	Data 0.130	Loss 0.379	Prec@1 88.6700	Prec@5 99.5800	
Best Prec@1: [91.760]	
Starting epoch number: 144 Learning rate: 0.1
Train: [144]	Time 108.939	Data 0.257	Loss 0.131	Prec@1 95.4380	Prec@5 99.9640	
Val: [144]	Time 6.497	Data 0.097	Loss 0.351	Prec@1 90.0400	Prec@5 99.7200	
Best Prec@1: [91.760]	
Starting epoch number: 145 Learning rate: 0.1
Train: [145]	Time 108.836	Data 0.268	Loss 0.134	Prec@1 95.3740	Prec@5 99.9660	
Val: [145]	Time 6.528	Data 0.100	Loss 0.442	Prec@1 87.5200	Prec@5 99.5800	
Best Prec@1: [91.760]	
Starting epoch number: 146 Learning rate: 0.1
Train: [146]	Time 108.940	Data 0.266	Loss 0.133	Prec@1 95.4180	Prec@5 99.9540	
Val: [146]	Time 6.486	Data 0.106	Loss 0.366	Prec@1 89.1600	Prec@5 99.8200	
Best Prec@1: [91.760]	
Starting epoch number: 147 Learning rate: 0.1
Train: [147]	Time 108.968	Data 0.267	Loss 0.136	Prec@1 95.2140	Prec@5 99.9640	
Val: [147]	Time 6.516	Data 0.098	Loss 0.336	Prec@1 89.9900	Prec@5 99.6200	
Best Prec@1: [91.760]	
Starting epoch number: 148 Learning rate: 0.1
Train: [148]	Time 108.774	Data 0.266	Loss 0.136	Prec@1 95.2180	Prec@5 99.9620	
Val: [148]	Time 6.565	Data 0.102	Loss 0.353	Prec@1 89.8100	Prec@5 99.8000	
Best Prec@1: [91.760]	
Starting epoch number: 149 Learning rate: 0.1
Train: [149]	Time 108.723	Data 0.272	Loss 0.134	Prec@1 95.3260	Prec@5 99.9620	
Val: [149]	Time 6.482	Data 0.101	Loss 0.332	Prec@1 89.8500	Prec@5 99.6700	
Best Prec@1: [91.760]	
Starting epoch number: 150 Learning rate: 0.010000000000000002
Train: [150]	Time 108.749	Data 0.280	Loss 0.050	Prec@1 98.4360	Prec@5 99.9960	
Val: [150]	Time 6.491	Data 0.101	Loss 0.170	Prec@1 94.6300	Prec@5 99.8900	
Best Prec@1: [94.630]	
Starting epoch number: 151 Learning rate: 0.010000000000000002
Train: [151]	Time 108.793	Data 0.271	Loss 0.027	Prec@1 99.2560	Prec@5 100.0000	
Val: [151]	Time 6.539	Data 0.151	Loss 0.161	Prec@1 94.9700	Prec@5 99.9500	
Best Prec@1: [94.970]	
Starting epoch number: 152 Learning rate: 0.010000000000000002
Train: [152]	Time 108.884	Data 0.264	Loss 0.019	Prec@1 99.5420	Prec@5 100.0000	
Val: [152]	Time 6.499	Data 0.112	Loss 0.160	Prec@1 95.1800	Prec@5 99.9500	
Best Prec@1: [95.180]	
Starting epoch number: 153 Learning rate: 0.010000000000000002
Train: [153]	Time 108.876	Data 0.267	Loss 0.016	Prec@1 99.5980	Prec@5 100.0000	
Val: [153]	Time 6.560	Data 0.100	Loss 0.157	Prec@1 95.4000	Prec@5 99.9400	
Best Prec@1: [95.400]	
Starting epoch number: 154 Learning rate: 0.010000000000000002
Train: [154]	Time 108.896	Data 0.259	Loss 0.013	Prec@1 99.7420	Prec@5 100.0000	
Val: [154]	Time 6.542	Data 0.105	Loss 0.161	Prec@1 95.4300	Prec@5 99.9100	
Best Prec@1: [95.430]	
Starting epoch number: 155 Learning rate: 0.010000000000000002
Train: [155]	Time 108.986	Data 0.275	Loss 0.012	Prec@1 99.7280	Prec@5 100.0000	
Val: [155]	Time 6.503	Data 0.094	Loss 0.162	Prec@1 95.4100	Prec@5 99.9300	
Best Prec@1: [95.430]	
Starting epoch number: 156 Learning rate: 0.010000000000000002
Train: [156]	Time 108.868	Data 0.273	Loss 0.010	Prec@1 99.8060	Prec@5 100.0000	
Val: [156]	Time 6.648	Data 0.138	Loss 0.159	Prec@1 95.7200	Prec@5 99.9100	
Best Prec@1: [95.720]	
Starting epoch number: 157 Learning rate: 0.010000000000000002
Train: [157]	Time 108.736	Data 0.268	Loss 0.009	Prec@1 99.8280	Prec@5 100.0000	
Val: [157]	Time 6.574	Data 0.100	Loss 0.162	Prec@1 95.6100	Prec@5 99.9100	
Best Prec@1: [95.720]	
Starting epoch number: 158 Learning rate: 0.010000000000000002
Train: [158]	Time 108.892	Data 0.259	Loss 0.008	Prec@1 99.8560	Prec@5 100.0000	
Val: [158]	Time 6.506	Data 0.109	Loss 0.162	Prec@1 95.5700	Prec@5 99.9200	
Best Prec@1: [95.720]	
Starting epoch number: 159 Learning rate: 0.010000000000000002
Train: [159]	Time 108.897	Data 0.275	Loss 0.007	Prec@1 99.9020	Prec@5 100.0000	
Val: [159]	Time 6.491	Data 0.097	Loss 0.161	Prec@1 95.6700	Prec@5 99.9000	
Best Prec@1: [95.720]	
Starting epoch number: 160 Learning rate: 0.010000000000000002
Train: [160]	Time 108.675	Data 0.270	Loss 0.006	Prec@1 99.9280	Prec@5 100.0000	
Val: [160]	Time 6.496	Data 0.092	Loss 0.167	Prec@1 95.5400	Prec@5 99.9100	
Best Prec@1: [95.720]	
Starting epoch number: 161 Learning rate: 0.010000000000000002
Train: [161]	Time 108.652	Data 0.273	Loss 0.007	Prec@1 99.8920	Prec@5 100.0000	
Val: [161]	Time 6.506	Data 0.121	Loss 0.168	Prec@1 95.4900	Prec@5 99.9300	
Best Prec@1: [95.720]	
Starting epoch number: 162 Learning rate: 0.010000000000000002
Train: [162]	Time 108.689	Data 0.263	Loss 0.006	Prec@1 99.8960	Prec@5 100.0000	
Val: [162]	Time 6.496	Data 0.099	Loss 0.166	Prec@1 95.5400	Prec@5 99.9100	
Best Prec@1: [95.720]	
Starting epoch number: 163 Learning rate: 0.010000000000000002
Train: [163]	Time 108.704	Data 0.266	Loss 0.005	Prec@1 99.9180	Prec@5 100.0000	
Val: [163]	Time 6.546	Data 0.113	Loss 0.167	Prec@1 95.7000	Prec@5 99.9300	
Best Prec@1: [95.720]	
Starting epoch number: 164 Learning rate: 0.010000000000000002
Train: [164]	Time 108.741	Data 0.268	Loss 0.005	Prec@1 99.9620	Prec@5 100.0000	
Val: [164]	Time 6.538	Data 0.099	Loss 0.167	Prec@1 95.6700	Prec@5 99.9100	
Best Prec@1: [95.720]	
Starting epoch number: 165 Learning rate: 0.010000000000000002
Train: [165]	Time 108.871	Data 0.263	Loss 0.005	Prec@1 99.9280	Prec@5 100.0000	
Val: [165]	Time 6.482	Data 0.095	Loss 0.166	Prec@1 95.6700	Prec@5 99.9100	
Best Prec@1: [95.720]	
Starting epoch number: 166 Learning rate: 0.010000000000000002
Train: [166]	Time 108.930	Data 0.269	Loss 0.005	Prec@1 99.9340	Prec@5 100.0000	
Val: [166]	Time 6.515	Data 0.148	Loss 0.167	Prec@1 95.6000	Prec@5 99.9100	
Best Prec@1: [95.720]	
Starting epoch number: 167 Learning rate: 0.010000000000000002
Train: [167]	Time 108.946	Data 0.271	Loss 0.005	Prec@1 99.9140	Prec@5 100.0000	
Val: [167]	Time 6.518	Data 0.098	Loss 0.172	Prec@1 95.6300	Prec@5 99.9100	
Best Prec@1: [95.720]	
Starting epoch number: 168 Learning rate: 0.010000000000000002
Train: [168]	Time 108.855	Data 0.269	Loss 0.005	Prec@1 99.9060	Prec@5 100.0000	
Val: [168]	Time 6.585	Data 0.096	Loss 0.165	Prec@1 95.7500	Prec@5 99.9000	
Best Prec@1: [95.750]	
Starting epoch number: 169 Learning rate: 0.010000000000000002
Train: [169]	Time 108.822	Data 0.270	Loss 0.004	Prec@1 99.9580	Prec@5 100.0000	
Val: [169]	Time 6.489	Data 0.096	Loss 0.167	Prec@1 95.7300	Prec@5 99.9100	
Best Prec@1: [95.750]	
Starting epoch number: 170 Learning rate: 0.010000000000000002
Train: [170]	Time 108.771	Data 0.268	Loss 0.004	Prec@1 99.9480	Prec@5 100.0000	
Val: [170]	Time 6.515	Data 0.104	Loss 0.166	Prec@1 95.7400	Prec@5 99.8700	
Best Prec@1: [95.750]	
Starting epoch number: 171 Learning rate: 0.010000000000000002
Train: [171]	Time 108.717	Data 0.264	Loss 0.003	Prec@1 99.9640	Prec@5 100.0000	
Val: [171]	Time 6.492	Data 0.097	Loss 0.169	Prec@1 95.6700	Prec@5 99.8900	
Best Prec@1: [95.750]	
Starting epoch number: 172 Learning rate: 0.010000000000000002
Train: [172]	Time 108.625	Data 0.265	Loss 0.004	Prec@1 99.9540	Prec@5 100.0000	
Val: [172]	Time 6.498	Data 0.126	Loss 0.166	Prec@1 95.8100	Prec@5 99.9000	
Best Prec@1: [95.810]	
Starting epoch number: 173 Learning rate: 0.010000000000000002
Train: [173]	Time 108.527	Data 0.264	Loss 0.004	Prec@1 99.9540	Prec@5 100.0000	
Val: [173]	Time 6.515	Data 0.129	Loss 0.167	Prec@1 95.8500	Prec@5 99.8800	
Best Prec@1: [95.850]	
Starting epoch number: 174 Learning rate: 0.010000000000000002
Train: [174]	Time 108.519	Data 0.269	Loss 0.003	Prec@1 99.9560	Prec@5 100.0000	
Val: [174]	Time 6.545	Data 0.118	Loss 0.168	Prec@1 95.7500	Prec@5 99.8800	
Best Prec@1: [95.850]	
Starting epoch number: 175 Learning rate: 0.010000000000000002
Train: [175]	Time 108.603	Data 0.266	Loss 0.003	Prec@1 99.9700	Prec@5 100.0000	
Val: [175]	Time 6.468	Data 0.097	Loss 0.173	Prec@1 95.7400	Prec@5 99.8600	
Best Prec@1: [95.850]	
Starting epoch number: 176 Learning rate: 0.010000000000000002
Train: [176]	Time 108.577	Data 0.265	Loss 0.003	Prec@1 99.9680	Prec@5 100.0000	
Val: [176]	Time 6.493	Data 0.094	Loss 0.173	Prec@1 95.5900	Prec@5 99.8900	
Best Prec@1: [95.850]	
Starting epoch number: 177 Learning rate: 0.010000000000000002
Train: [177]	Time 108.724	Data 0.273	Loss 0.003	Prec@1 99.9680	Prec@5 100.0000	
Val: [177]	Time 6.507	Data 0.098	Loss 0.173	Prec@1 95.6300	Prec@5 99.8900	
Best Prec@1: [95.850]	
Starting epoch number: 178 Learning rate: 0.010000000000000002
Train: [178]	Time 108.775	Data 0.266	Loss 0.003	Prec@1 99.9680	Prec@5 100.0000	
Val: [178]	Time 6.515	Data 0.096	Loss 0.173	Prec@1 95.7000	Prec@5 99.9200	
Best Prec@1: [95.850]	
Starting epoch number: 179 Learning rate: 0.010000000000000002
Train: [179]	Time 108.887	Data 0.281	Loss 0.003	Prec@1 99.9640	Prec@5 100.0000	
Val: [179]	Time 6.505	Data 0.118	Loss 0.169	Prec@1 95.8400	Prec@5 99.8800	
Best Prec@1: [95.850]	
Starting epoch number: 180 Learning rate: 0.010000000000000002
Train: [180]	Time 108.909	Data 0.271	Loss 0.003	Prec@1 99.9500	Prec@5 100.0000	
Val: [180]	Time 6.504	Data 0.101	Loss 0.171	Prec@1 95.7700	Prec@5 99.8800	
Best Prec@1: [95.850]	
Starting epoch number: 181 Learning rate: 0.010000000000000002
Train: [181]	Time 108.846	Data 0.262	Loss 0.003	Prec@1 99.9860	Prec@5 100.0000	
Val: [181]	Time 6.504	Data 0.106	Loss 0.167	Prec@1 95.910	Prec@5 99.8700	
Best Prec@1: [95.910]	
Starting epoch number: 182 Learning rate: 0.010000000000000002
Train: [182]	Time 108.784	Data 0.293	Loss 0.002	Prec@1 99.9780	Prec@5 100.0000	
Val: [182]	Time 6.610	Data 0.099	Loss 0.172	Prec@1 95.8400	Prec@5 99.8600	
Best Prec@1: [95.910]	
Starting epoch number: 183 Learning rate: 0.010000000000000002
Train: [183]	Time 108.682	Data 0.273	Loss 0.002	Prec@1 99.9920	Prec@5 100.0000	
Val: [183]	Time 6.581	Data 0.175	Loss 0.168	Prec@1 95.8800	Prec@5 99.8700	
Best Prec@1: [95.910]	
Starting epoch number: 184 Learning rate: 0.010000000000000002
Train: [184]	Time 108.697	Data 0.275	Loss 0.003	Prec@1 99.9820	Prec@5 100.0000	
Val: [184]	Time 6.521	Data 0.098	Loss 0.169	Prec@1 95.8700	Prec@5 99.9000	
Best Prec@1: [95.910]	
Starting epoch number: 185 Learning rate: 0.010000000000000002
Train: [185]	Time 108.676	Data 0.275	Loss 0.002	Prec@1 99.9780	Prec@5 100.0000	
Val: [185]	Time 6.500	Data 0.101	Loss 0.172	Prec@1 95.9000	Prec@5 99.8700	
Best Prec@1: [95.910]	
Starting epoch number: 186 Learning rate: 0.010000000000000002
Train: [186]	Time 108.599	Data 0.269	Loss 0.002	Prec@1 99.9820	Prec@5 100.0000	
Val: [186]	Time 6.590	Data 0.126	Loss 0.177	Prec@1 95.7000	Prec@5 99.8800	
Best Prec@1: [95.910]	
Starting epoch number: 187 Learning rate: 0.010000000000000002
Train: [187]	Time 108.481	Data 0.280	Loss 0.002	Prec@1 99.9840	Prec@5 100.0000	
Val: [187]	Time 6.528	Data 0.103	Loss 0.171	Prec@1 95.6700	Prec@5 99.8900	
Best Prec@1: [95.910]	
Starting epoch number: 188 Learning rate: 0.010000000000000002
Train: [188]	Time 108.582	Data 0.275	Loss 0.003	Prec@1 99.9700	Prec@5 100.0000	
Val: [188]	Time 6.534	Data 0.140	Loss 0.170	Prec@1 95.7500	Prec@5 99.9200	
Best Prec@1: [95.910]	
Starting epoch number: 189 Learning rate: 0.010000000000000002
Train: [189]	Time 108.652	Data 0.274	Loss 0.002	Prec@1 99.9760	Prec@5 100.0000	
Val: [189]	Time 6.464	Data 0.106	Loss 0.171	Prec@1 95.7100	Prec@5 99.9000	
Best Prec@1: [95.910]	
Starting epoch number: 190 Learning rate: 0.010000000000000002
Train: [190]	Time 108.764	Data 0.266	Loss 0.002	Prec@1 99.9820	Prec@5 100.0000	
Val: [190]	Time 6.525	Data 0.112	Loss 0.168	Prec@1 95.8000	Prec@5 99.9000	
Best Prec@1: [95.910]	
Starting epoch number: 191 Learning rate: 0.010000000000000002
Train: [191]	Time 108.826	Data 0.274	Loss 0.002	Prec@1 99.9880	Prec@5 100.0000	
Val: [191]	Time 6.515	Data 0.111	Loss 0.167	Prec@1 95.8300	Prec@5 99.8800	
Best Prec@1: [95.910]	
Starting epoch number: 192 Learning rate: 0.010000000000000002
Train: [192]	Time 108.884	Data 0.263	Loss 0.002	Prec@1 99.9800	Prec@5 100.0000	
Val: [192]	Time 6.508	Data 0.112	Loss 0.165	Prec@1 95.8000	Prec@5 99.8700	
Best Prec@1: [95.910]	
Starting epoch number: 193 Learning rate: 0.010000000000000002
Train: [193]	Time 108.813	Data 0.275	Loss 0.002	Prec@1 99.9760	Prec@5 100.0000	
Val: [193]	Time 6.500	Data 0.104	Loss 0.167	Prec@1 95.8400	Prec@5 99.8900	
Best Prec@1: [95.910]	
Starting epoch number: 194 Learning rate: 0.010000000000000002
Train: [194]	Time 108.744	Data 0.275	Loss 0.002	Prec@1 99.9780	Prec@5 100.0000	
Val: [194]	Time 6.509	Data 0.119	Loss 0.171	Prec@1 95.9000	Prec@5 99.8700	
Best Prec@1: [95.910]	
Starting epoch number: 195 Learning rate: 0.010000000000000002
Train: [195]	Time 108.806	Data 0.287	Loss 0.002	Prec@1 99.9840	Prec@5 100.0000	
Val: [195]	Time 6.542	Data 0.141	Loss 0.167	Prec@1 95.8600	Prec@5 99.8700	
Best Prec@1: [95.910]	
Starting epoch number: 196 Learning rate: 0.010000000000000002
Train: [196]	Time 108.833	Data 0.285	Loss 0.002	Prec@1 99.9760	Prec@5 100.0000	
Val: [196]	Time 6.687	Data 0.186	Loss 0.167	Prec@1 95.9000	Prec@5 99.8700	
Best Prec@1: [95.910]	
Starting epoch number: 197 Learning rate: 0.010000000000000002
Train: [197]	Time 108.731	Data 0.284	Loss 0.002	Prec@1 99.9860	Prec@5 100.0000	
Val: [197]	Time 6.475	Data 0.092	Loss 0.171	Prec@1 95.7100	Prec@5 99.9100	
Best Prec@1: [95.910]	
Starting epoch number: 198 Learning rate: 0.010000000000000002
Train: [198]	Time 108.640	Data 0.265	Loss 0.002	Prec@1 99.9880	Prec@5 100.0000	
Val: [198]	Time 6.484	Data 0.097	Loss 0.171	Prec@1 95.8000	Prec@5 99.9100	
Best Prec@1: [95.910]	
Starting epoch number: 199 Learning rate: 0.010000000000000002
Train: [199]	Time 108.578	Data 0.268	Loss 0.002	Prec@1 99.9900	Prec@5 100.0000	
Val: [199]	Time 6.517	Data 0.130	Loss 0.170	Prec@1 95.7400	Prec@5 99.8700	
Best Prec@1: [95.910]	
Starting epoch number: 200 Learning rate: 0.010000000000000002
Train: [200]	Time 108.492	Data 0.272	Loss 0.002	Prec@1 99.9960	Prec@5 100.0000	
Val: [200]	Time 6.484	Data 0.115	Loss 0.170	Prec@1 95.8100	Prec@5 99.8800	
Best Prec@1: [95.910]	