Namespace(acc_type='class', augment=True, batch_size=64, bottleneck=True, cachemode=True, cardinality=8, criterion='crossentropy', cuda=True, data_dir='../data', dataset='cifar100', decayinterval=None, decaylevel=None, droprate=0, epochs=300, evaluate=False, expandConfig=None, expandSize=16, from_modelzoo=False, growth=100, layers=40, learningratescheduler='cifarschedular', logdir='../logs/densenetexpanderbc2wide_cifar100_40_100_expandSize16', lr=0.1, manualSeed=123, maxlr=0.1, minlr=0.0005, model_def='densenetexpander2_cifar', momentum=0.9, name='densenetexpanderbc2wide_cifar100_40_100_expandSize16', nclasses=100, nesterov=True, ngpus=1, optimType='sgd', pretrained=False, pretrained_file='', printfreq=200, reduce=0.5, resume='', start_epoch=0, store='', tenCrop=False, tensorboard=True, testOnly=False, verbose=False, weightDecay=0.0001, weight_init=False, widen_factor=4, workers=2)
DenseNet3 (
  (conv1): Conv2d(3, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): DenseBlock (
    (layer): Sequential (
      (0): BottleneckBlock (
        (bn1): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (1): BottleneckBlock (
        (bn1): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (2): BottleneckBlock (
        (bn1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (3): BottleneckBlock (
        (bn1): BatchNorm2d(500, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (4): BottleneckBlock (
        (bn1): BatchNorm2d(600, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (5): BottleneckBlock (
        (bn1): BatchNorm2d(700, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
    )
  )
  (trans1): TransitionBlock (
    (bn1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)
    (relu): ReLU (inplace)
    (conv1): ExpanderConv2d (
    )
  )
  (block2): DenseBlock (
    (layer): Sequential (
      (0): BottleneckBlock (
        (bn1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (1): BottleneckBlock (
        (bn1): BatchNorm2d(500, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (2): BottleneckBlock (
        (bn1): BatchNorm2d(600, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (3): BottleneckBlock (
        (bn1): BatchNorm2d(700, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (4): BottleneckBlock (
        (bn1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (5): BottleneckBlock (
        (bn1): BatchNorm2d(900, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
    )
  )
  (trans2): TransitionBlock (
    (bn1): BatchNorm2d(1000, eps=1e-05, momentum=0.1, affine=True)
    (relu): ReLU (inplace)
    (conv1): ExpanderConv2d (
    )
  )
  (block3): DenseBlock (
    (layer): Sequential (
      (0): BottleneckBlock (
        (bn1): BatchNorm2d(500, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (1): BottleneckBlock (
        (bn1): BatchNorm2d(600, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (2): BottleneckBlock (
        (bn1): BatchNorm2d(700, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (3): BottleneckBlock (
        (bn1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (4): BottleneckBlock (
        (bn1): BatchNorm2d(900, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (5): BottleneckBlock (
        (bn1): BatchNorm2d(1000, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
    )
  )
  (bn1): BatchNorm2d(1100, eps=1e-05, momentum=0.1, affine=True)
  (relu): ReLU (inplace)
  (fc): Linear (1100 -> 100)
)
Files already downloaded and verified
Starting epoch number: 0 Learning rate: 0.1
Train: [0]	Time 234.617	Data 0.336	Loss 3.740	Prec@1 13.1420	Prec@5 35.8620	
Val: [0]	Time 15.138	Data 0.103	Loss 3.613	Prec@1 18.4200	Prec@5 45.1700	
Best Prec@1: [18.420]	
Starting epoch number: 1 Learning rate: 0.1
Train: [1]	Time 237.953	Data 0.279	Loss 2.777	Prec@1 29.5360	Prec@5 61.0080	
Val: [1]	Time 15.487	Data 0.106	Loss 2.512	Prec@1 35.8300	Prec@5 68.4000	
Best Prec@1: [35.830]	
Starting epoch number: 2 Learning rate: 0.1
Train: [2]	Time 239.435	Data 0.275	Loss 2.161	Prec@1 42.1680	Prec@5 74.6020	
Val: [2]	Time 15.500	Data 0.095	Loss 2.126	Prec@1 43.9600	Prec@5 76.1400	
Best Prec@1: [43.960]	
Starting epoch number: 3 Learning rate: 0.1
Train: [3]	Time 239.370	Data 0.291	Loss 1.841	Prec@1 49.3060	Prec@5 80.9020	
Val: [3]	Time 15.455	Data 0.119	Loss 1.932	Prec@1 48.1000	Prec@5 79.5600	
Best Prec@1: [48.100]	
Starting epoch number: 4 Learning rate: 0.1
Train: [4]	Time 238.739	Data 0.264	Loss 1.634	Prec@1 54.3940	Prec@5 84.3900	
Val: [4]	Time 15.454	Data 0.108	Loss 1.746	Prec@1 53.2000	Prec@5 83.0000	
Best Prec@1: [53.200]	
Starting epoch number: 5 Learning rate: 0.1
Train: [5]	Time 238.522	Data 0.274	Loss 1.492	Prec@1 57.7900	Prec@5 86.6220	
Val: [5]	Time 15.453	Data 0.113	Loss 1.737	Prec@1 54.1800	Prec@5 83.7100	
Best Prec@1: [54.180]	
Starting epoch number: 6 Learning rate: 0.1
Train: [6]	Time 238.495	Data 0.281	Loss 1.388	Prec@1 60.2980	Prec@5 88.2200	
Val: [6]	Time 15.422	Data 0.104	Loss 1.599	Prec@1 56.8500	Prec@5 85.2800	
Best Prec@1: [56.850]	
Starting epoch number: 7 Learning rate: 0.1
Train: [7]	Time 238.512	Data 0.284	Loss 1.306	Prec@1 62.4700	Prec@5 89.3500	
Val: [7]	Time 15.468	Data 0.091	Loss 1.546	Prec@1 57.4000	Prec@5 86.4400	
Best Prec@1: [57.400]	
Starting epoch number: 8 Learning rate: 0.1
Train: [8]	Time 238.732	Data 0.284	Loss 1.249	Prec@1 64.0100	Prec@5 90.1180	
Val: [8]	Time 15.395	Data 0.088	Loss 1.524	Prec@1 59.4300	Prec@5 86.4800	
Best Prec@1: [59.430]	
Starting epoch number: 9 Learning rate: 0.1
Train: [9]	Time 238.626	Data 0.269	Loss 1.199	Prec@1 65.3120	Prec@5 90.7300	
Val: [9]	Time 15.457	Data 0.095	Loss 1.513	Prec@1 58.4400	Prec@5 86.9600	
Best Prec@1: [59.430]	
Starting epoch number: 10 Learning rate: 0.1
Train: [10]	Time 238.619	Data 0.273	Loss 1.157	Prec@1 66.1720	Prec@5 91.3400	
Val: [10]	Time 15.462	Data 0.103	Loss 1.483	Prec@1 60.4400	Prec@5 87.0400	
Best Prec@1: [60.440]	
Starting epoch number: 11 Learning rate: 0.1
Train: [11]	Time 238.593	Data 0.286	Loss 1.126	Prec@1 67.0160	Prec@5 91.8340	
Val: [11]	Time 15.390	Data 0.090	Loss 1.453	Prec@1 60.5100	Prec@5 87.4000	
Best Prec@1: [60.510]	
Starting epoch number: 12 Learning rate: 0.1
Train: [12]	Time 238.527	Data 0.267	Loss 1.099	Prec@1 67.8580	Prec@5 92.2300	
Val: [12]	Time 15.484	Data 0.100	Loss 1.429	Prec@1 61.2000	Prec@5 87.5500	
Best Prec@1: [61.200]	
Starting epoch number: 13 Learning rate: 0.1
Train: [13]	Time 240.792	Data 0.278	Loss 1.076	Prec@1 68.3180	Prec@5 92.4640	
Val: [13]	Time 15.464	Data 0.102	Loss 1.430	Prec@1 61.9900	Prec@5 88.8400	
Best Prec@1: [61.990]	
Starting epoch number: 14 Learning rate: 0.1
Train: [14]	Time 240.492	Data 0.277	Loss 1.045	Prec@1 68.9980	Prec@5 92.7980	
Val: [14]	Time 15.820	Data 0.095	Loss 1.579	Prec@1 59.0700	Prec@5 86.7700	
Best Prec@1: [61.990]	
Starting epoch number: 15 Learning rate: 0.1
Train: [15]	Time 239.981	Data 0.271	Loss 1.036	Prec@1 69.6340	Prec@5 92.9700	
Val: [15]	Time 15.514	Data 0.092	Loss 1.529	Prec@1 59.8400	Prec@5 87.7000	
Best Prec@1: [61.990]	
Starting epoch number: 16 Learning rate: 0.1
Train: [16]	Time 239.999	Data 0.295	Loss 1.021	Prec@1 69.9280	Prec@5 93.2440	
Val: [16]	Time 15.520	Data 0.119	Loss 1.424	Prec@1 62.1400	Prec@5 88.3300	
Best Prec@1: [62.140]	
Starting epoch number: 17 Learning rate: 0.1
Train: [17]	Time 239.709	Data 0.279	Loss 1.008	Prec@1 70.2240	Prec@5 93.3740	
Val: [17]	Time 15.501	Data 0.100	Loss 1.392	Prec@1 62.7800	Prec@5 88.7900	
Best Prec@1: [62.780]	
Starting epoch number: 18 Learning rate: 0.1
Train: [18]	Time 239.441	Data 0.279	Loss 0.996	Prec@1 70.3900	Prec@5 93.5280	
Val: [18]	Time 15.540	Data 0.097	Loss 1.517	Prec@1 60.1200	Prec@5 87.4200	
Best Prec@1: [62.780]	
Starting epoch number: 19 Learning rate: 0.1
Train: [19]	Time 239.816	Data 0.263	Loss 0.977	Prec@1 70.8940	Prec@5 93.6100	
Val: [19]	Time 15.566	Data 0.105	Loss 1.548	Prec@1 59.9700	Prec@5 87.6600	
Best Prec@1: [62.780]	
Starting epoch number: 20 Learning rate: 0.1
Train: [20]	Time 239.662	Data 0.293	Loss 0.969	Prec@1 71.0020	Prec@5 93.8460	
Val: [20]	Time 15.386	Data 0.093	Loss 1.454	Prec@1 61.8000	Prec@5 88.6900	
Best Prec@1: [62.780]	
Starting epoch number: 21 Learning rate: 0.1
Train: [21]	Time 239.389	Data 0.277	Loss 0.957	Prec@1 71.6220	Prec@5 93.9120	
Val: [21]	Time 15.477	Data 0.102	Loss 1.573	Prec@1 59.5300	Prec@5 86.2700	
Best Prec@1: [62.780]	
Starting epoch number: 22 Learning rate: 0.1
Train: [22]	Time 239.344	Data 0.262	Loss 0.947	Prec@1 71.8140	Prec@5 93.9940	
Val: [22]	Time 15.471	Data 0.108	Loss 1.464	Prec@1 61.6300	Prec@5 88.3700	
Best Prec@1: [62.780]	
Starting epoch number: 23 Learning rate: 0.1
Train: [23]	Time 239.312	Data 0.279	Loss 0.937	Prec@1 72.0000	Prec@5 94.1460	
Val: [23]	Time 15.522	Data 0.098	Loss 1.458	Prec@1 61.5800	Prec@5 88.2800	
Best Prec@1: [62.780]	
Starting epoch number: 24 Learning rate: 0.1
Train: [24]	Time 238.772	Data 0.268	Loss 0.929	Prec@1 72.1660	Prec@5 94.2640	
Val: [24]	Time 15.452	Data 0.088	Loss 1.447	Prec@1 62.4400	Prec@5 87.9000	
Best Prec@1: [62.780]	
Starting epoch number: 25 Learning rate: 0.1
Train: [25]	Time 239.128	Data 0.277	Loss 0.912	Prec@1 72.6680	Prec@5 94.5380	
Val: [25]	Time 15.452	Data 0.097	Loss 1.450	Prec@1 62.5000	Prec@5 88.6400	
Best Prec@1: [62.780]	
Starting epoch number: 26 Learning rate: 0.1
Train: [26]	Time 239.278	Data 0.296	Loss 0.919	Prec@1 72.4860	Prec@5 94.4200	
Val: [26]	Time 15.489	Data 0.101	Loss 1.369	Prec@1 63.4700	Prec@5 89.6300	
Best Prec@1: [63.470]	
Starting epoch number: 27 Learning rate: 0.1
Train: [27]	Time 238.875	Data 0.267	Loss 0.904	Prec@1 73.0340	Prec@5 94.3940	
Val: [27]	Time 15.418	Data 0.100	Loss 1.396	Prec@1 63.4500	Prec@5 89.3900	
Best Prec@1: [63.470]	
Starting epoch number: 28 Learning rate: 0.1
Train: [28]	Time 238.874	Data 0.268	Loss 0.894	Prec@1 73.2460	Prec@5 94.7100	
Val: [28]	Time 15.459	Data 0.106	Loss 1.392	Prec@1 62.7900	Prec@5 89.6000	
Best Prec@1: [63.470]	
Starting epoch number: 29 Learning rate: 0.1
Train: [29]	Time 239.002	Data 0.268	Loss 0.884	Prec@1 73.4580	Prec@5 94.7260	
Val: [29]	Time 15.469	Data 0.110	Loss 1.417	Prec@1 61.7100	Prec@5 88.8100	
Best Prec@1: [63.470]	
Starting epoch number: 30 Learning rate: 0.1
Train: [30]	Time 238.895	Data 0.270	Loss 0.888	Prec@1 73.4740	Prec@5 94.5820	
Val: [30]	Time 15.443	Data 0.088	Loss 1.394	Prec@1 62.7000	Prec@5 89.3200	
Best Prec@1: [63.470]	
Starting epoch number: 31 Learning rate: 0.1
Train: [31]	Time 239.020	Data 0.272	Loss 0.875	Prec@1 73.7920	Prec@5 94.8800	
Val: [31]	Time 15.410	Data 0.111	Loss 1.393	Prec@1 63.5600	Prec@5 89.0700	
Best Prec@1: [63.560]	
Starting epoch number: 32 Learning rate: 0.1
Train: [32]	Time 238.433	Data 0.282	Loss 0.864	Prec@1 73.9760	Prec@5 95.0360	
Val: [32]	Time 15.533	Data 0.114	Loss 1.336	Prec@1 64.5600	Prec@5 89.6200	
Best Prec@1: [64.560]	
Starting epoch number: 33 Learning rate: 0.1
Train: [33]	Time 238.586	Data 0.281	Loss 0.866	Prec@1 74.0000	Prec@5 95.0740	
Val: [33]	Time 15.577	Data 0.108	Loss 1.422	Prec@1 62.8200	Prec@5 88.6500	
Best Prec@1: [64.560]	
Starting epoch number: 34 Learning rate: 0.1
Train: [34]	Time 238.896	Data 0.282	Loss 0.856	Prec@1 74.1760	Prec@5 95.1560	
Val: [34]	Time 15.421	Data 0.115	Loss 1.349	Prec@1 64.7000	Prec@5 89.4400	
Best Prec@1: [64.700]	
Starting epoch number: 35 Learning rate: 0.1
Train: [35]	Time 238.805	Data 0.289	Loss 0.852	Prec@1 74.3720	Prec@5 95.0940	
Val: [35]	Time 15.484	Data 0.094	Loss 1.317	Prec@1 65.9400	Prec@5 90.2000	
Best Prec@1: [65.940]	
Starting epoch number: 36 Learning rate: 0.1
Train: [36]	Time 238.756	Data 0.273	Loss 0.846	Prec@1 74.4660	Prec@5 95.2340	
Val: [36]	Time 15.494	Data 0.091	Loss 1.480	Prec@1 62.2300	Prec@5 88.2700	
Best Prec@1: [65.940]	
Starting epoch number: 37 Learning rate: 0.1
Train: [37]	Time 238.768	Data 0.271	Loss 0.838	Prec@1 74.4520	Prec@5 95.3500	
Val: [37]	Time 15.452	Data 0.104	Loss 1.382	Prec@1 64.5400	Prec@5 89.2300	
Best Prec@1: [65.940]	
Starting epoch number: 38 Learning rate: 0.1
Train: [38]	Time 238.570	Data 0.281	Loss 0.844	Prec@1 74.4480	Prec@5 95.2020	
Val: [38]	Time 15.421	Data 0.096	Loss 1.401	Prec@1 64.3700	Prec@5 89.4700	
Best Prec@1: [65.940]	
Starting epoch number: 39 Learning rate: 0.1
Train: [39]	Time 238.161	Data 0.269	Loss 0.832	Prec@1 74.8640	Prec@5 95.2820	
Val: [39]	Time 15.389	Data 0.089	Loss 1.496	Prec@1 62.5500	Prec@5 88.2700	
Best Prec@1: [65.940]	
Starting epoch number: 40 Learning rate: 0.1
Train: [40]	Time 238.337	Data 0.269	Loss 0.829	Prec@1 74.7460	Prec@5 95.4840	
Val: [40]	Time 15.408	Data 0.091	Loss 1.418	Prec@1 63.4700	Prec@5 89.0800	
Best Prec@1: [65.940]	
Starting epoch number: 41 Learning rate: 0.1
Train: [41]	Time 238.288	Data 0.258	Loss 0.828	Prec@1 74.9520	Prec@5 95.3780	
Val: [41]	Time 15.442	Data 0.099	Loss 1.404	Prec@1 63.1700	Prec@5 89.9200	
Best Prec@1: [65.940]	
Starting epoch number: 42 Learning rate: 0.1
Train: [42]	Time 238.484	Data 0.276	Loss 0.825	Prec@1 75.1440	Prec@5 95.3500	
Val: [42]	Time 15.455	Data 0.102	Loss 1.339	Prec@1 65.0100	Prec@5 89.3000	
Best Prec@1: [65.940]	
Starting epoch number: 43 Learning rate: 0.1
Train: [43]	Time 238.539	Data 0.281	Loss 0.817	Prec@1 75.3300	Prec@5 95.5260	
Val: [43]	Time 15.460	Data 0.098	Loss 1.374	Prec@1 64.1900	Prec@5 89.0500	
Best Prec@1: [65.940]	
Starting epoch number: 44 Learning rate: 0.1
Train: [44]	Time 238.575	Data 0.261	Loss 0.822	Prec@1 75.0080	Prec@5 95.5320	
Val: [44]	Time 15.489	Data 0.104	Loss 1.351	Prec@1 64.2800	Prec@5 89.9700	
Best Prec@1: [65.940]	
Starting epoch number: 45 Learning rate: 0.1
Train: [45]	Time 238.705	Data 0.269	Loss 0.806	Prec@1 75.6080	Prec@5 95.5600	
Val: [45]	Time 15.486	Data 0.117	Loss 1.530	Prec@1 62.5600	Prec@5 89.0800	
Best Prec@1: [65.940]	
Starting epoch number: 46 Learning rate: 0.1
Train: [46]	Time 238.505	Data 0.295	Loss 0.808	Prec@1 75.5380	Prec@5 95.5860	
Val: [46]	Time 15.457	Data 0.120	Loss 1.420	Prec@1 62.6900	Prec@5 88.8700	
Best Prec@1: [65.940]	
Starting epoch number: 47 Learning rate: 0.1
Train: [47]	Time 238.436	Data 0.270	Loss 0.803	Prec@1 75.6420	Prec@5 95.5540	
Val: [47]	Time 15.393	Data 0.088	Loss 1.382	Prec@1 64.4100	Prec@5 89.1800	
Best Prec@1: [65.940]	
Starting epoch number: 48 Learning rate: 0.1
Train: [48]	Time 238.413	Data 0.272	Loss 0.801	Prec@1 75.5000	Prec@5 95.6220	
Val: [48]	Time 15.433	Data 0.111	Loss 1.358	Prec@1 65.4700	Prec@5 89.8000	
Best Prec@1: [65.940]	
Starting epoch number: 49 Learning rate: 0.1
Train: [49]	Time 238.401	Data 0.275	Loss 0.800	Prec@1 75.5220	Prec@5 95.7740	
Val: [49]	Time 15.428	Data 0.100	Loss 1.414	Prec@1 64.7900	Prec@5 89.4500	
Best Prec@1: [65.940]	
Starting epoch number: 50 Learning rate: 0.1
Train: [50]	Time 238.280	Data 0.267	Loss 0.797	Prec@1 75.6460	Prec@5 95.7280	
Val: [50]	Time 15.419	Data 0.093	Loss 1.512	Prec@1 62.6400	Prec@5 88.7200	
Best Prec@1: [65.940]	
Starting epoch number: 51 Learning rate: 0.1
Train: [51]	Time 238.264	Data 0.279	Loss 0.796	Prec@1 75.9360	Prec@5 95.7260	
Val: [51]	Time 15.377	Data 0.089	Loss 1.392	Prec@1 63.8800	Prec@5 89.7300	
Best Prec@1: [65.940]	
Starting epoch number: 52 Learning rate: 0.1
Train: [52]	Time 237.929	Data 0.271	Loss 0.787	Prec@1 75.9800	Prec@5 95.7840	
Val: [52]	Time 15.424	Data 0.101	Loss 1.435	Prec@1 63.6000	Prec@5 89.2900	
Best Prec@1: [65.940]	
Starting epoch number: 53 Learning rate: 0.1
Train: [53]	Time 237.940	Data 0.267	Loss 0.790	Prec@1 76.0400	Prec@5 95.7620	
Val: [53]	Time 15.393	Data 0.098	Loss 1.467	Prec@1 62.7600	Prec@5 88.5500	
Best Prec@1: [65.940]	
Starting epoch number: 54 Learning rate: 0.1
Train: [54]	Time 237.321	Data 0.270	Loss 0.778	Prec@1 76.4500	Prec@5 95.8760	
Val: [54]	Time 15.370	Data 0.095	Loss 1.413	Prec@1 63.7600	Prec@5 89.5200	
Best Prec@1: [65.940]	
Starting epoch number: 55 Learning rate: 0.1
Train: [55]	Time 238.003	Data 0.290	Loss 0.781	Prec@1 76.1860	Prec@5 95.9380	
Val: [55]	Time 15.462	Data 0.106	Loss 1.461	Prec@1 63.3700	Prec@5 88.5500	
Best Prec@1: [65.940]	
Starting epoch number: 56 Learning rate: 0.1
Train: [56]	Time 237.928	Data 0.272	Loss 0.777	Prec@1 76.4620	Prec@5 95.9300	
Val: [56]	Time 15.432	Data 0.113	Loss 1.402	Prec@1 63.6800	Prec@5 89.3300	
Best Prec@1: [65.940]	
Starting epoch number: 57 Learning rate: 0.1
Train: [57]	Time 238.073	Data 0.259	Loss 0.780	Prec@1 76.1460	Prec@5 95.9740	
Val: [57]	Time 15.479	Data 0.103	Loss 1.541	Prec@1 61.4100	Prec@5 87.6400	
Best Prec@1: [65.940]	
Starting epoch number: 58 Learning rate: 0.1
Train: [58]	Time 237.936	Data 0.265	Loss 0.775	Prec@1 76.4340	Prec@5 95.7940	
Val: [58]	Time 15.410	Data 0.093	Loss 1.546	Prec@1 62.5700	Prec@5 88.3900	
Best Prec@1: [65.940]	
Starting epoch number: 59 Learning rate: 0.1
Train: [59]	Time 237.939	Data 0.281	Loss 0.763	Prec@1 76.7780	Prec@5 95.9660	
Val: [59]	Time 15.384	Data 0.095	Loss 1.562	Prec@1 60.9600	Prec@5 88.6600	
Best Prec@1: [65.940]	
Starting epoch number: 60 Learning rate: 0.1
Train: [60]	Time 237.886	Data 0.270	Loss 0.769	Prec@1 76.4160	Prec@5 96.0200	
Val: [60]	Time 15.393	Data 0.097	Loss 1.341	Prec@1 65.3300	Prec@5 90.3700	
Best Prec@1: [65.940]	
Starting epoch number: 61 Learning rate: 0.1
Train: [61]	Time 237.784	Data 0.274	Loss 0.771	Prec@1 76.5060	Prec@5 96.0620	
Val: [61]	Time 15.392	Data 0.089	Loss 1.414	Prec@1 64.0000	Prec@5 89.9600	
Best Prec@1: [65.940]	
Starting epoch number: 62 Learning rate: 0.1
Train: [62]	Time 237.843	Data 0.288	Loss 0.766	Prec@1 76.5460	Prec@5 96.0220	
Val: [62]	Time 15.391	Data 0.096	Loss 1.289	Prec@1 66.1600	Prec@5 90.6900	
Best Prec@1: [66.160]	
Starting epoch number: 63 Learning rate: 0.1
Train: [63]	Time 237.670	Data 0.285	Loss 0.765	Prec@1 76.5340	Prec@5 96.0860	
Val: [63]	Time 15.309	Data 0.094	Loss 1.588	Prec@1 62.4700	Prec@5 87.5800	
Best Prec@1: [66.160]	
Starting epoch number: 64 Learning rate: 0.1
Train: [64]	Time 237.735	Data 0.276	Loss 0.761	Prec@1 76.7980	Prec@5 96.1040	
Val: [64]	Time 15.410	Data 0.103	Loss 1.404	Prec@1 63.8100	Prec@5 89.4300	
Best Prec@1: [66.160]	
Starting epoch number: 65 Learning rate: 0.1
Train: [65]	Time 237.772	Data 0.273	Loss 0.754	Prec@1 77.0020	Prec@5 96.1180	
Val: [65]	Time 15.442	Data 0.094	Loss 1.428	Prec@1 63.6900	Prec@5 89.3700	
Best Prec@1: [66.160]	
Starting epoch number: 66 Learning rate: 0.1
Train: [66]	Time 237.913	Data 0.266	Loss 0.757	Prec@1 76.9080	Prec@5 96.0800	
Val: [66]	Time 15.408	Data 0.091	Loss 1.498	Prec@1 63.2300	Prec@5 88.8100	
Best Prec@1: [66.160]	
Starting epoch number: 67 Learning rate: 0.1
Train: [67]	Time 237.571	Data 0.264	Loss 0.753	Prec@1 77.0400	Prec@5 96.1520	
Val: [67]	Time 15.315	Data 0.092	Loss 1.354	Prec@1 64.6600	Prec@5 90.0000	
Best Prec@1: [66.160]	
Starting epoch number: 68 Learning rate: 0.1
Train: [68]	Time 237.493	Data 0.285	Loss 0.755	Prec@1 77.0240	Prec@5 96.1160	
Val: [68]	Time 15.432	Data 0.095	Loss 1.468	Prec@1 62.4200	Prec@5 88.2100	
Best Prec@1: [66.160]	
Starting epoch number: 69 Learning rate: 0.1
Train: [69]	Time 237.299	Data 0.279	Loss 0.746	Prec@1 77.3880	Prec@5 96.1060	
Val: [69]	Time 15.513	Data 0.104	Loss 1.421	Prec@1 65.2300	Prec@5 89.5100	
Best Prec@1: [66.160]	
Starting epoch number: 70 Learning rate: 0.1
Train: [70]	Time 237.176	Data 0.277	Loss 0.751	Prec@1 77.0440	Prec@5 96.1880	
Val: [70]	Time 15.351	Data 0.095	Loss 1.401	Prec@1 64.3000	Prec@5 89.4500	
Best Prec@1: [66.160]	
Starting epoch number: 71 Learning rate: 0.1
Train: [71]	Time 237.219	Data 0.281	Loss 0.746	Prec@1 77.3220	Prec@5 96.2560	
Val: [71]	Time 15.388	Data 0.117	Loss 1.312	Prec@1 65.6500	Prec@5 90.6200	
Best Prec@1: [66.160]	
Starting epoch number: 72 Learning rate: 0.1
Train: [72]	Time 237.135	Data 0.274	Loss 0.746	Prec@1 77.0240	Prec@5 96.2700	
Val: [72]	Time 15.370	Data 0.091	Loss 1.494	Prec@1 63.7500	Prec@5 88.6200	
Best Prec@1: [66.160]	
Starting epoch number: 73 Learning rate: 0.1
Train: [73]	Time 237.128	Data 0.286	Loss 0.741	Prec@1 77.4800	Prec@5 96.2320	
Val: [73]	Time 15.383	Data 0.107	Loss 1.557	Prec@1 61.9400	Prec@5 87.7500	
Best Prec@1: [66.160]	
Starting epoch number: 74 Learning rate: 0.1
Train: [74]	Time 237.130	Data 0.278	Loss 0.747	Prec@1 77.0800	Prec@5 96.1360	
Val: [74]	Time 15.361	Data 0.105	Loss 1.484	Prec@1 63.7400	Prec@5 89.4300	
Best Prec@1: [66.160]	
Starting epoch number: 75 Learning rate: 0.1
Train: [75]	Time 237.064	Data 0.272	Loss 0.738	Prec@1 77.4220	Prec@5 96.3340	
Val: [75]	Time 15.320	Data 0.085	Loss 1.427	Prec@1 64.3000	Prec@5 89.2600	
Best Prec@1: [66.160]	
Starting epoch number: 76 Learning rate: 0.1
Train: [76]	Time 236.925	Data 0.257	Loss 0.737	Prec@1 77.1640	Prec@5 96.2960	
Val: [76]	Time 15.360	Data 0.092	Loss 1.388	Prec@1 64.8700	Prec@5 89.9700	
Best Prec@1: [66.160]	
Starting epoch number: 77 Learning rate: 0.1
Train: [77]	Time 237.143	Data 0.263	Loss 0.738	Prec@1 77.4100	Prec@5 96.4220	
Val: [77]	Time 15.395	Data 0.104	Loss 1.380	Prec@1 64.4600	Prec@5 89.7100	
Best Prec@1: [66.160]	
Starting epoch number: 78 Learning rate: 0.1
Train: [78]	Time 237.028	Data 0.288	Loss 0.750	Prec@1 77.0300	Prec@5 96.2480	
Val: [78]	Time 15.333	Data 0.101	Loss 1.367	Prec@1 65.5400	Prec@5 89.7600	
Best Prec@1: [66.160]	
Starting epoch number: 79 Learning rate: 0.1
Train: [79]	Time 236.934	Data 0.268	Loss 0.734	Prec@1 77.4860	Prec@5 96.4120	
Val: [79]	Time 15.335	Data 0.112	Loss 1.329	Prec@1 66.0900	Prec@5 90.1000	
Best Prec@1: [66.160]	
Starting epoch number: 80 Learning rate: 0.1
Train: [80]	Time 236.763	Data 0.266	Loss 0.730	Prec@1 77.5320	Prec@5 96.4380	
Val: [80]	Time 15.339	Data 0.096	Loss 1.321	Prec@1 65.8500	Prec@5 90.1000	
Best Prec@1: [66.160]	
Starting epoch number: 81 Learning rate: 0.1
Train: [81]	Time 237.096	Data 0.301	Loss 0.726	Prec@1 77.8820	Prec@5 96.3140	
Val: [81]	Time 15.349	Data 0.091	Loss 1.367	Prec@1 65.2300	Prec@5 89.6000	
Best Prec@1: [66.160]	
Starting epoch number: 82 Learning rate: 0.1
Train: [82]	Time 236.893	Data 0.265	Loss 0.726	Prec@1 77.7380	Prec@5 96.4380	
Val: [82]	Time 15.350	Data 0.089	Loss 1.504	Prec@1 63.5400	Prec@5 88.8800	
Best Prec@1: [66.160]	
Starting epoch number: 83 Learning rate: 0.1
Train: [83]	Time 237.007	Data 0.275	Loss 0.727	Prec@1 77.6680	Prec@5 96.3920	
Val: [83]	Time 15.311	Data 0.099	Loss 1.510	Prec@1 62.5600	Prec@5 88.5900	
Best Prec@1: [66.160]	
Starting epoch number: 84 Learning rate: 0.1
Train: [84]	Time 236.798	Data 0.281	Loss 0.721	Prec@1 78.0660	Prec@5 96.4960	
Val: [84]	Time 15.358	Data 0.101	Loss 1.345	Prec@1 65.2400	Prec@5 90.9400	
Best Prec@1: [66.160]	
Starting epoch number: 85 Learning rate: 0.1
Train: [85]	Time 236.966	Data 0.260	Loss 0.730	Prec@1 77.7080	Prec@5 96.3080	
Val: [85]	Time 15.358	Data 0.106	Loss 1.427	Prec@1 64.2600	Prec@5 89.0100	
Best Prec@1: [66.160]	
Starting epoch number: 86 Learning rate: 0.1
Train: [86]	Time 236.980	Data 0.267	Loss 0.722	Prec@1 77.7460	Prec@5 96.4640	
Val: [86]	Time 15.399	Data 0.089	Loss 1.467	Prec@1 64.3600	Prec@5 88.8200	
Best Prec@1: [66.160]	
Starting epoch number: 87 Learning rate: 0.1
Train: [87]	Time 237.136	Data 0.252	Loss 0.721	Prec@1 77.7240	Prec@5 96.3380	
Val: [87]	Time 15.395	Data 0.114	Loss 1.429	Prec@1 63.3400	Prec@5 89.3500	
Best Prec@1: [66.160]	
Starting epoch number: 88 Learning rate: 0.1
Train: [88]	Time 237.328	Data 0.267	Loss 0.718	Prec@1 77.8220	Prec@5 96.5480	
Val: [88]	Time 15.363	Data 0.106	Loss 1.646	Prec@1 61.8900	Prec@5 87.5900	
Best Prec@1: [66.160]	
Starting epoch number: 89 Learning rate: 0.1
Train: [89]	Time 237.078	Data 0.259	Loss 0.718	Prec@1 78.0860	Prec@5 96.4720	
Val: [89]	Time 15.494	Data 0.091	Loss 1.424	Prec@1 64.6300	Prec@5 89.7000	
Best Prec@1: [66.160]	
Starting epoch number: 90 Learning rate: 0.1
Train: [90]	Time 236.945	Data 0.283	Loss 0.719	Prec@1 77.8180	Prec@5 96.5260	
Val: [90]	Time 15.404	Data 0.092	Loss 1.403	Prec@1 64.3400	Prec@5 90.1600	
Best Prec@1: [66.160]	
Starting epoch number: 91 Learning rate: 0.1
Train: [91]	Time 236.693	Data 0.265	Loss 0.716	Prec@1 77.9040	Prec@5 96.5560	
Val: [91]	Time 15.308	Data 0.098	Loss 1.401	Prec@1 65.3600	Prec@5 90.1400	
Best Prec@1: [66.160]	
Starting epoch number: 92 Learning rate: 0.1
Train: [92]	Time 236.791	Data 0.273	Loss 0.724	Prec@1 77.7260	Prec@5 96.5320	
Val: [92]	Time 15.315	Data 0.093	Loss 1.522	Prec@1 62.5500	Prec@5 88.6600	
Best Prec@1: [66.160]	
Starting epoch number: 93 Learning rate: 0.1
Train: [93]	Time 236.707	Data 0.280	Loss 0.720	Prec@1 77.8440	Prec@5 96.4800	
Val: [93]	Time 15.316	Data 0.094	Loss 1.409	Prec@1 64.9700	Prec@5 89.5100	
Best Prec@1: [66.160]	
Starting epoch number: 94 Learning rate: 0.1
Train: [94]	Time 236.503	Data 0.271	Loss 0.717	Prec@1 77.9400	Prec@5 96.5120	
Val: [94]	Time 15.303	Data 0.110	Loss 1.446	Prec@1 64.4800	Prec@5 89.5000	
Best Prec@1: [66.160]	
Starting epoch number: 95 Learning rate: 0.1
Train: [95]	Time 236.534	Data 0.282	Loss 0.707	Prec@1 78.0640	Prec@5 96.5500	
Val: [95]	Time 15.376	Data 0.101	Loss 1.388	Prec@1 65.0100	Prec@5 89.5700	
Best Prec@1: [66.160]	
Starting epoch number: 96 Learning rate: 0.1
Train: [96]	Time 238.530	Data 0.273	Loss 0.708	Prec@1 78.1720	Prec@5 96.5700	
Val: [96]	Time 15.526	Data 0.091	Loss 1.377	Prec@1 65.3800	Prec@5 90.2800	
Best Prec@1: [66.160]	
Starting epoch number: 97 Learning rate: 0.1
Train: [97]	Time 238.790	Data 0.275	Loss 0.709	Prec@1 78.3100	Prec@5 96.6080	
Val: [97]	Time 15.538	Data 0.099	Loss 1.386	Prec@1 65.0200	Prec@5 89.6800	
Best Prec@1: [66.160]	
Starting epoch number: 98 Learning rate: 0.1
Train: [98]	Time 239.424	Data 0.273	Loss 0.708	Prec@1 78.1460	Prec@5 96.6580	
Val: [98]	Time 15.500	Data 0.112	Loss 1.507	Prec@1 63.3900	Prec@5 89.1900	
Best Prec@1: [66.160]	
Starting epoch number: 99 Learning rate: 0.1
Train: [99]	Time 239.969	Data 0.279	Loss 0.704	Prec@1 78.3640	Prec@5 96.7240	
Val: [99]	Time 15.545	Data 0.095	Loss 1.563	Prec@1 62.3200	Prec@5 88.5100	
Best Prec@1: [66.160]	
Starting epoch number: 100 Learning rate: 0.1
Train: [100]	Time 239.458	Data 0.296	Loss 0.709	Prec@1 78.1140	Prec@5 96.6620	
Val: [100]	Time 15.467	Data 0.114	Loss 1.464	Prec@1 64.6000	Prec@5 89.0800	
Best Prec@1: [66.160]	
Starting epoch number: 101 Learning rate: 0.1
Train: [101]	Time 238.861	Data 0.279	Loss 0.707	Prec@1 78.3720	Prec@5 96.6660	
Val: [101]	Time 15.437	Data 0.093	Loss 1.453	Prec@1 64.0800	Prec@5 88.9600	
Best Prec@1: [66.160]	
Starting epoch number: 102 Learning rate: 0.1
Train: [102]	Time 238.820	Data 0.260	Loss 0.708	Prec@1 78.1340	Prec@5 96.5120	
Val: [102]	Time 15.439	Data 0.101	Loss 1.273	Prec@1 66.7800	Prec@5 90.2900	
Best Prec@1: [66.780]	
Starting epoch number: 103 Learning rate: 0.1
Train: [103]	Time 238.575	Data 0.257	Loss 0.701	Prec@1 78.3980	Prec@5 96.6800	
Val: [103]	Time 15.488	Data 0.106	Loss 1.441	Prec@1 64.4700	Prec@5 89.2900	
Best Prec@1: [66.780]	
Starting epoch number: 104 Learning rate: 0.1
Train: [104]	Time 238.537	Data 0.272	Loss 0.710	Prec@1 77.8800	Prec@5 96.6460	
Val: [104]	Time 15.537	Data 0.103	Loss 1.367	Prec@1 65.5100	Prec@5 90.1900	
Best Prec@1: [66.780]	
Starting epoch number: 105 Learning rate: 0.1
Train: [105]	Time 238.293	Data 0.270	Loss 0.705	Prec@1 78.3280	Prec@5 96.6620	
Val: [105]	Time 15.387	Data 0.096	Loss 1.391	Prec@1 65.6200	Prec@5 90.1700	
Best Prec@1: [66.780]	
Starting epoch number: 106 Learning rate: 0.1
Train: [106]	Time 237.926	Data 0.278	Loss 0.704	Prec@1 78.3580	Prec@5 96.6180	
Val: [106]	Time 15.395	Data 0.097	Loss 1.444	Prec@1 64.6000	Prec@5 89.1900	
Best Prec@1: [66.780]	
Starting epoch number: 107 Learning rate: 0.1
Train: [107]	Time 238.131	Data 0.282	Loss 0.700	Prec@1 78.4160	Prec@5 96.7060	
Val: [107]	Time 15.462	Data 0.112	Loss 1.455	Prec@1 63.6300	Prec@5 88.8200	
Best Prec@1: [66.780]	
Starting epoch number: 108 Learning rate: 0.1
Train: [108]	Time 237.965	Data 0.318	Loss 0.697	Prec@1 78.5740	Prec@5 96.8040	
Val: [108]	Time 15.409	Data 0.098	Loss 1.471	Prec@1 63.8500	Prec@5 89.6500	
Best Prec@1: [66.780]	
Starting epoch number: 109 Learning rate: 0.1
Train: [109]	Time 237.836	Data 0.264	Loss 0.704	Prec@1 78.3800	Prec@5 96.6300	
Val: [109]	Time 15.397	Data 0.099	Loss 1.553	Prec@1 62.6900	Prec@5 88.5400	
Best Prec@1: [66.780]	
Starting epoch number: 110 Learning rate: 0.1
Train: [110]	Time 237.608	Data 0.266	Loss 0.700	Prec@1 78.5780	Prec@5 96.5760	
Val: [110]	Time 15.381	Data 0.090	Loss 1.366	Prec@1 65.9100	Prec@5 89.6700	
Best Prec@1: [66.780]	
Starting epoch number: 111 Learning rate: 0.1
Train: [111]	Time 237.639	Data 0.280	Loss 0.696	Prec@1 78.5020	Prec@5 96.7220	
Val: [111]	Time 15.383	Data 0.088	Loss 1.465	Prec@1 64.3600	Prec@5 89.1500	
Best Prec@1: [66.780]	
Starting epoch number: 112 Learning rate: 0.1
Train: [112]	Time 237.159	Data 0.257	Loss 0.699	Prec@1 78.6120	Prec@5 96.6280	
Val: [112]	Time 15.374	Data 0.105	Loss 1.363	Prec@1 66.2100	Prec@5 90.0300	
Best Prec@1: [66.780]	
Starting epoch number: 113 Learning rate: 0.1
Train: [113]	Time 237.494	Data 0.272	Loss 0.694	Prec@1 78.5520	Prec@5 96.8060	
Val: [113]	Time 15.381	Data 0.102	Loss 1.432	Prec@1 64.2200	Prec@5 88.7600	
Best Prec@1: [66.780]	
Starting epoch number: 114 Learning rate: 0.1
Train: [114]	Time 237.162	Data 0.267	Loss 0.695	Prec@1 78.5260	Prec@5 96.7800	
Val: [114]	Time 15.415	Data 0.102	Loss 1.415	Prec@1 64.4700	Prec@5 89.8100	
Best Prec@1: [66.780]	
Starting epoch number: 115 Learning rate: 0.1
Train: [115]	Time 237.304	Data 0.290	Loss 0.693	Prec@1 78.7240	Prec@5 96.7200	
Val: [115]	Time 15.319	Data 0.091	Loss 1.500	Prec@1 63.0600	Prec@5 88.2800	
Best Prec@1: [66.780]	
Starting epoch number: 116 Learning rate: 0.1
Train: [116]	Time 237.335	Data 0.269	Loss 0.694	Prec@1 78.6600	Prec@5 96.6360	
Val: [116]	Time 15.373	Data 0.105	Loss 1.391	Prec@1 64.9800	Prec@5 89.6700	
Best Prec@1: [66.780]	
Starting epoch number: 117 Learning rate: 0.1
Train: [117]	Time 237.384	Data 0.271	Loss 0.690	Prec@1 78.4760	Prec@5 96.9100	
Val: [117]	Time 15.356	Data 0.106	Loss 1.418	Prec@1 65.0000	Prec@5 89.6600	
Best Prec@1: [66.780]	
Starting epoch number: 118 Learning rate: 0.1
Train: [118]	Time 237.273	Data 0.274	Loss 0.695	Prec@1 78.4020	Prec@5 96.6420	
Val: [118]	Time 15.301	Data 0.089	Loss 1.359	Prec@1 65.3400	Prec@5 89.5900	
Best Prec@1: [66.780]	
Starting epoch number: 119 Learning rate: 0.1
Train: [119]	Time 236.792	Data 0.284	Loss 0.694	Prec@1 78.5380	Prec@5 96.8320	
Val: [119]	Time 15.410	Data 0.092	Loss 1.422	Prec@1 64.3100	Prec@5 89.0300	
Best Prec@1: [66.780]	
Starting epoch number: 120 Learning rate: 0.1
Train: [120]	Time 236.574	Data 0.325	Loss 0.694	Prec@1 78.5580	Prec@5 96.7960	
Val: [120]	Time 15.313	Data 0.088	Loss 1.385	Prec@1 65.2400	Prec@5 89.7900	
Best Prec@1: [66.780]	
Starting epoch number: 121 Learning rate: 0.1
Train: [121]	Time 236.610	Data 0.339	Loss 0.681	Prec@1 79.0740	Prec@5 96.8400	
Val: [121]	Time 15.388	Data 0.102	Loss 1.439	Prec@1 65.3200	Prec@5 89.8000	
Best Prec@1: [66.780]	
Starting epoch number: 122 Learning rate: 0.1
Train: [122]	Time 236.751	Data 0.309	Loss 0.687	Prec@1 78.6800	Prec@5 96.7380	
Val: [122]	Time 15.388	Data 0.102	Loss 1.409	Prec@1 65.1600	Prec@5 90.0900	
Best Prec@1: [66.780]	
Starting epoch number: 123 Learning rate: 0.1
Train: [123]	Time 236.888	Data 0.269	Loss 0.690	Prec@1 78.7020	Prec@5 96.8040	
Val: [123]	Time 15.391	Data 0.094	Loss 1.388	Prec@1 65.9200	Prec@5 89.8600	
Best Prec@1: [66.780]	
Starting epoch number: 124 Learning rate: 0.1
Train: [124]	Time 236.792	Data 0.277	Loss 0.685	Prec@1 78.7660	Prec@5 96.9180	
Val: [124]	Time 15.344	Data 0.094	Loss 1.485	Prec@1 64.0800	Prec@5 89.4600	
Best Prec@1: [66.780]	
Starting epoch number: 125 Learning rate: 0.1
Train: [125]	Time 237.007	Data 0.274	Loss 0.690	Prec@1 78.7440	Prec@5 96.7400	
Val: [125]	Time 15.371	Data 0.094	Loss 1.380	Prec@1 65.2200	Prec@5 89.6500	
Best Prec@1: [66.780]	
Starting epoch number: 126 Learning rate: 0.1
Train: [126]	Time 237.041	Data 0.252	Loss 0.689	Prec@1 78.7140	Prec@5 96.9720	
Val: [126]	Time 15.380	Data 0.101	Loss 1.361	Prec@1 66.2500	Prec@5 89.8900	
Best Prec@1: [66.780]	
Starting epoch number: 127 Learning rate: 0.1
Train: [127]	Time 237.105	Data 0.262	Loss 0.688	Prec@1 78.7100	Prec@5 96.7960	
Val: [127]	Time 15.341	Data 0.098	Loss 1.530	Prec@1 63.5200	Prec@5 88.9500	
Best Prec@1: [66.780]	
Starting epoch number: 128 Learning rate: 0.1
Train: [128]	Time 237.076	Data 0.277	Loss 0.688	Prec@1 78.7640	Prec@5 96.7800	
Val: [128]	Time 15.374	Data 0.111	Loss 1.393	Prec@1 65.3600	Prec@5 89.8000	
Best Prec@1: [66.780]	
Starting epoch number: 129 Learning rate: 0.1
Train: [129]	Time 236.746	Data 0.267	Loss 0.683	Prec@1 78.8300	Prec@5 96.8880	
Val: [129]	Time 15.566	Data 0.110	Loss 1.471	Prec@1 62.9000	Prec@5 88.5100	
Best Prec@1: [66.780]	
Starting epoch number: 130 Learning rate: 0.1
Train: [130]	Time 236.889	Data 0.282	Loss 0.681	Prec@1 78.9980	Prec@5 96.8340	
Val: [130]	Time 15.331	Data 0.097	Loss 1.441	Prec@1 64.3300	Prec@5 89.5300	
Best Prec@1: [66.780]	
Starting epoch number: 131 Learning rate: 0.1
Train: [131]	Time 236.905	Data 0.260	Loss 0.683	Prec@1 78.9100	Prec@5 96.8220	
Val: [131]	Time 15.366	Data 0.091	Loss 1.430	Prec@1 64.3600	Prec@5 89.6400	
Best Prec@1: [66.780]	
Starting epoch number: 132 Learning rate: 0.1
Train: [132]	Time 236.988	Data 0.276	Loss 0.687	Prec@1 78.7340	Prec@5 96.8460	
Val: [132]	Time 15.363	Data 0.105	Loss 1.408	Prec@1 64.1100	Prec@5 89.2200	
Best Prec@1: [66.780]	
Starting epoch number: 133 Learning rate: 0.1
Train: [133]	Time 236.856	Data 0.256	Loss 0.683	Prec@1 78.7420	Prec@5 96.9000	
Val: [133]	Time 15.282	Data 0.096	Loss 1.566	Prec@1 62.9600	Prec@5 89.0900	
Best Prec@1: [66.780]	
Starting epoch number: 134 Learning rate: 0.1
Train: [134]	Time 236.577	Data 0.263	Loss 0.684	Prec@1 78.7100	Prec@5 96.9460	
Val: [134]	Time 15.352	Data 0.100	Loss 1.372	Prec@1 65.3800	Prec@5 89.7400	
Best Prec@1: [66.780]	
Starting epoch number: 135 Learning rate: 0.1
Train: [135]	Time 236.769	Data 0.257	Loss 0.682	Prec@1 78.8980	Prec@5 96.9420	
Val: [135]	Time 15.340	Data 0.091	Loss 1.339	Prec@1 65.8200	Prec@5 90.0900	
Best Prec@1: [66.780]	
Starting epoch number: 136 Learning rate: 0.1
Train: [136]	Time 236.754	Data 0.273	Loss 0.685	Prec@1 78.9180	Prec@5 96.7880	
Val: [136]	Time 15.297	Data 0.092	Loss 1.527	Prec@1 63.6100	Prec@5 88.6800	
Best Prec@1: [66.780]	
Starting epoch number: 137 Learning rate: 0.1
Train: [137]	Time 236.852	Data 0.278	Loss 0.676	Prec@1 79.2480	Prec@5 96.8920	
Val: [137]	Time 15.378	Data 0.096	Loss 1.383	Prec@1 64.9600	Prec@5 89.7600	
Best Prec@1: [66.780]	
Starting epoch number: 138 Learning rate: 0.1
Train: [138]	Time 236.362	Data 0.257	Loss 0.670	Prec@1 79.3780	Prec@5 96.9960	
Val: [138]	Time 15.317	Data 0.088	Loss 1.449	Prec@1 64.9100	Prec@5 89.0600	
Best Prec@1: [66.780]	
Starting epoch number: 139 Learning rate: 0.1
Train: [139]	Time 236.333	Data 0.280	Loss 0.678	Prec@1 79.0020	Prec@5 96.8660	
Val: [139]	Time 15.410	Data 0.092	Loss 1.360	Prec@1 65.9200	Prec@5 90.1100	
Best Prec@1: [66.780]	
Starting epoch number: 140 Learning rate: 0.1
Train: [140]	Time 236.365	Data 0.272	Loss 0.676	Prec@1 79.0380	Prec@5 96.8260	
Val: [140]	Time 15.341	Data 0.098	Loss 1.473	Prec@1 63.7500	Prec@5 89.0400	
Best Prec@1: [66.780]	
Starting epoch number: 141 Learning rate: 0.1
Train: [141]	Time 236.567	Data 0.259	Loss 0.686	Prec@1 78.5980	Prec@5 96.8800	
Val: [141]	Time 15.343	Data 0.120	Loss 1.332	Prec@1 66.0100	Prec@5 90.2700	
Best Prec@1: [66.780]	
Starting epoch number: 142 Learning rate: 0.1
Train: [142]	Time 236.659	Data 0.275	Loss 0.672	Prec@1 79.4160	Prec@5 96.9160	
Val: [142]	Time 15.356	Data 0.105	Loss 1.354	Prec@1 65.1200	Prec@5 90.2400	
Best Prec@1: [66.780]	
Starting epoch number: 143 Learning rate: 0.1
Train: [143]	Time 236.782	Data 0.253	Loss 0.677	Prec@1 79.0960	Prec@5 96.8700	
Val: [143]	Time 15.373	Data 0.087	Loss 1.391	Prec@1 65.6500	Prec@5 90.2700	
Best Prec@1: [66.780]	
Starting epoch number: 144 Learning rate: 0.1
Train: [144]	Time 236.853	Data 0.268	Loss 0.672	Prec@1 79.3140	Prec@5 97.1040	
Val: [144]	Time 15.355	Data 0.095	Loss 1.376	Prec@1 65.4300	Prec@5 90.0900	
Best Prec@1: [66.780]	
Starting epoch number: 145 Learning rate: 0.1
Train: [145]	Time 236.554	Data 0.282	Loss 0.665	Prec@1 79.3780	Prec@5 96.9840	
Val: [145]	Time 15.407	Data 0.103	Loss 1.498	Prec@1 64.2500	Prec@5 88.6200	
Best Prec@1: [66.780]	
Starting epoch number: 146 Learning rate: 0.1
Train: [146]	Time 236.519	Data 0.260	Loss 0.681	Prec@1 78.8880	Prec@5 96.8860	
Val: [146]	Time 15.323	Data 0.093	Loss 1.398	Prec@1 64.9700	Prec@5 89.9000	
Best Prec@1: [66.780]	
Starting epoch number: 147 Learning rate: 0.1
Train: [147]	Time 236.528	Data 0.269	Loss 0.672	Prec@1 79.3000	Prec@5 96.9300	
Val: [147]	Time 15.340	Data 0.099	Loss 1.466	Prec@1 63.6800	Prec@5 88.8600	
Best Prec@1: [66.780]	
Starting epoch number: 148 Learning rate: 0.1
Train: [148]	Time 236.468	Data 0.269	Loss 0.674	Prec@1 79.0280	Prec@5 97.0400	
Val: [148]	Time 15.372	Data 0.096	Loss 1.343	Prec@1 66.2400	Prec@5 90.3400	
Best Prec@1: [66.780]	
Starting epoch number: 149 Learning rate: 0.1
Train: [149]	Time 236.615	Data 0.280	Loss 0.668	Prec@1 79.4440	Prec@5 97.0860	
Val: [149]	Time 15.325	Data 0.101	Loss 1.433	Prec@1 64.0500	Prec@5 88.9800	
Best Prec@1: [66.780]	
Starting epoch number: 150 Learning rate: 0.010000000000000002
Train: [150]	Time 236.877	Data 0.270	Loss 0.377	Prec@1 88.9460	Prec@5 98.9000	
Val: [150]	Time 15.404	Data 0.103	Loss 0.932	Prec@1 75.3800	Prec@5 93.9000	
Best Prec@1: [75.380]	
Starting epoch number: 151 Learning rate: 0.010000000000000002
Train: [151]	Time 236.916	Data 0.277	Loss 0.273	Prec@1 92.2840	Prec@5 99.4920	
Val: [151]	Time 15.387	Data 0.093	Loss 0.928	Prec@1 76.0100	Prec@5 94.0500	
Best Prec@1: [76.010]	
Starting epoch number: 152 Learning rate: 0.010000000000000002
Train: [152]	Time 236.843	Data 0.268	Loss 0.229	Prec@1 93.8040	Prec@5 99.6180	
Val: [152]	Time 15.403	Data 0.084	Loss 0.928	Prec@1 76.1100	Prec@5 94.2700	
Best Prec@1: [76.110]	
Starting epoch number: 153 Learning rate: 0.010000000000000002
Train: [153]	Time 236.868	Data 0.263	Loss 0.202	Prec@1 94.5060	Prec@5 99.7400	
Val: [153]	Time 15.289	Data 0.082	Loss 0.936	Prec@1 75.9300	Prec@5 94.3200	
Best Prec@1: [76.110]	
Starting epoch number: 154 Learning rate: 0.010000000000000002
Train: [154]	Time 236.437	Data 0.264	Loss 0.184	Prec@1 95.1980	Prec@5 99.7740	
Val: [154]	Time 15.362	Data 0.109	Loss 0.952	Prec@1 75.9400	Prec@5 94.2400	
Best Prec@1: [76.110]	
Starting epoch number: 155 Learning rate: 0.010000000000000002
Train: [155]	Time 236.193	Data 0.285	Loss 0.168	Prec@1 95.6640	Prec@5 99.8160	
Val: [155]	Time 15.303	Data 0.103	Loss 0.955	Prec@1 76.1900	Prec@5 94.3700	
Best Prec@1: [76.190]	
Starting epoch number: 156 Learning rate: 0.010000000000000002
Train: [156]	Time 236.634	Data 0.274	Loss 0.154	Prec@1 96.1440	Prec@5 99.8480	
Val: [156]	Time 15.388	Data 0.090	Loss 0.971	Prec@1 76.2000	Prec@5 94.2700	
Best Prec@1: [76.200]	
Starting epoch number: 157 Learning rate: 0.010000000000000002
Train: [157]	Time 236.438	Data 0.274	Loss 0.143	Prec@1 96.3920	Prec@5 99.8960	
Val: [157]	Time 15.334	Data 0.102	Loss 0.975	Prec@1 75.8700	Prec@5 94.2300	
Best Prec@1: [76.200]	
Starting epoch number: 158 Learning rate: 0.010000000000000002
Train: [158]	Time 236.609	Data 0.284	Loss 0.134	Prec@1 96.7040	Prec@5 99.9040	
Val: [158]	Time 15.398	Data 0.115	Loss 0.990	Prec@1 75.7500	Prec@5 94.2700	
Best Prec@1: [76.200]	
Starting epoch number: 159 Learning rate: 0.010000000000000002
Train: [159]	Time 236.787	Data 0.263	Loss 0.124	Prec@1 97.0260	Prec@5 99.9160	
Val: [159]	Time 15.363	Data 0.097	Loss 0.997	Prec@1 75.9000	Prec@5 94.1900	
Best Prec@1: [76.200]	
Starting epoch number: 160 Learning rate: 0.010000000000000002
Train: [160]	Time 236.525	Data 0.286	Loss 0.116	Prec@1 97.2740	Prec@5 99.9480	
Val: [160]	Time 15.385	Data 0.094	Loss 1.008	Prec@1 75.8400	Prec@5 94.1400	
Best Prec@1: [76.200]	
Starting epoch number: 161 Learning rate: 0.010000000000000002
Train: [161]	Time 236.477	Data 0.263	Loss 0.109	Prec@1 97.5960	Prec@5 99.9440	
Val: [161]	Time 15.322	Data 0.087	Loss 1.008	Prec@1 75.7700	Prec@5 94.2600	
Best Prec@1: [76.200]	
Starting epoch number: 162 Learning rate: 0.010000000000000002
Train: [162]	Time 236.421	Data 0.252	Loss 0.104	Prec@1 97.6440	Prec@5 99.9540	
Val: [162]	Time 15.366	Data 0.084	Loss 1.009	Prec@1 76.0900	Prec@5 94.4000	
Best Prec@1: [76.200]	
Starting epoch number: 163 Learning rate: 0.010000000000000002
Train: [163]	Time 236.479	Data 0.278	Loss 0.095	Prec@1 97.9700	Prec@5 99.9720	
Val: [163]	Time 15.479	Data 0.095	Loss 1.039	Prec@1 75.8200	Prec@5 94.2400	
Best Prec@1: [76.200]	
Starting epoch number: 164 Learning rate: 0.010000000000000002
Train: [164]	Time 236.551	Data 0.276	Loss 0.094	Prec@1 97.9920	Prec@5 99.9860	
Val: [164]	Time 15.323	Data 0.108	Loss 1.034	Prec@1 75.8000	Prec@5 94.1200	
Best Prec@1: [76.200]	
Starting epoch number: 165 Learning rate: 0.010000000000000002
Train: [165]	Time 236.417	Data 0.284	Loss 0.088	Prec@1 98.1100	Prec@5 99.9800	
Val: [165]	Time 15.355	Data 0.110	Loss 1.048	Prec@1 75.6200	Prec@5 94.0200	
Best Prec@1: [76.200]	
Starting epoch number: 166 Learning rate: 0.010000000000000002
Train: [166]	Time 236.065	Data 0.283	Loss 0.084	Prec@1 98.2900	Prec@5 99.9700	
Val: [166]	Time 15.385	Data 0.099	Loss 1.058	Prec@1 75.9700	Prec@5 94.2100	
Best Prec@1: [76.200]	
Starting epoch number: 167 Learning rate: 0.010000000000000002
Train: [167]	Time 236.397	Data 0.266	Loss 0.080	Prec@1 98.3500	Prec@5 99.9900	
Val: [167]	Time 15.284	Data 0.104	Loss 1.058	Prec@1 75.9300	Prec@5 94.1300	
Best Prec@1: [76.200]	
Starting epoch number: 168 Learning rate: 0.010000000000000002
Train: [168]	Time 235.592	Data 0.264	Loss 0.077	Prec@1 98.4720	Prec@5 99.9900	
Val: [168]	Time 15.275	Data 0.094	Loss 1.066	Prec@1 75.8000	Prec@5 94.1300	
Best Prec@1: [76.200]	
Starting epoch number: 169 Learning rate: 0.010000000000000002
Train: [169]	Time 235.973	Data 0.262	Loss 0.073	Prec@1 98.5800	Prec@5 99.9940	
Val: [169]	Time 15.328	Data 0.103	Loss 1.063	Prec@1 76.1400	Prec@5 93.8100	
Best Prec@1: [76.200]	
Starting epoch number: 170 Learning rate: 0.010000000000000002
Train: [170]	Time 236.176	Data 0.265	Loss 0.071	Prec@1 98.6660	Prec@5 99.9920	
Val: [170]	Time 15.441	Data 0.108	Loss 1.074	Prec@1 75.8700	Prec@5 94.1300	
Best Prec@1: [76.200]	
Starting epoch number: 171 Learning rate: 0.010000000000000002
Train: [171]	Time 236.263	Data 0.272	Loss 0.068	Prec@1 98.7200	Prec@5 99.9860	
Val: [171]	Time 15.365	Data 0.101	Loss 1.086	Prec@1 75.4700	Prec@5 93.9700	
Best Prec@1: [76.200]	
Starting epoch number: 172 Learning rate: 0.010000000000000002
Train: [172]	Time 236.160	Data 0.274	Loss 0.068	Prec@1 98.6860	Prec@5 99.9920	
Val: [172]	Time 15.261	Data 0.104	Loss 1.112	Prec@1 75.2500	Prec@5 93.9500	
Best Prec@1: [76.200]	
Starting epoch number: 173 Learning rate: 0.010000000000000002
Train: [173]	Time 236.126	Data 0.274	Loss 0.065	Prec@1 98.8020	Prec@5 99.9940	
Val: [173]	Time 15.331	Data 0.111	Loss 1.083	Prec@1 75.9000	Prec@5 94.1100	
Best Prec@1: [76.200]	
Starting epoch number: 174 Learning rate: 0.010000000000000002
Train: [174]	Time 236.241	Data 0.269	Loss 0.065	Prec@1 98.8260	Prec@5 99.9860	
Val: [174]	Time 15.425	Data 0.110	Loss 1.096	Prec@1 75.9600	Prec@5 94.0000	
Best Prec@1: [76.200]	
Starting epoch number: 175 Learning rate: 0.010000000000000002
Train: [175]	Time 236.167	Data 0.268	Loss 0.061	Prec@1 98.9080	Prec@5 99.9960	
Val: [175]	Time 15.357	Data 0.092	Loss 1.101	Prec@1 75.6100	Prec@5 93.8500	
Best Prec@1: [76.200]	
Starting epoch number: 176 Learning rate: 0.010000000000000002
Train: [176]	Time 236.002	Data 0.267	Loss 0.059	Prec@1 98.9760	Prec@5 99.9980	
Val: [176]	Time 15.407	Data 0.101	Loss 1.100	Prec@1 75.7900	Prec@5 93.9600	
Best Prec@1: [76.200]	
Starting epoch number: 177 Learning rate: 0.010000000000000002
Train: [177]	Time 235.960	Data 0.257	Loss 0.056	Prec@1 99.0440	Prec@5 99.9960	
Val: [177]	Time 15.295	Data 0.109	Loss 1.111	Prec@1 75.2400	Prec@5 93.7700	
Best Prec@1: [76.200]	
Starting epoch number: 178 Learning rate: 0.010000000000000002
Train: [178]	Time 235.972	Data 0.276	Loss 0.056	Prec@1 99.0520	Prec@5 99.9900	
Val: [178]	Time 15.286	Data 0.099	Loss 1.110	Prec@1 75.4400	Prec@5 93.9500	
Best Prec@1: [76.200]	
Starting epoch number: 179 Learning rate: 0.010000000000000002
Train: [179]	Time 236.090	Data 0.262	Loss 0.056	Prec@1 99.0320	Prec@5 99.9980	
Val: [179]	Time 15.324	Data 0.112	Loss 1.122	Prec@1 75.1300	Prec@5 93.8400	
Best Prec@1: [76.200]	
Starting epoch number: 180 Learning rate: 0.010000000000000002
Train: [180]	Time 236.326	Data 0.270	Loss 0.052	Prec@1 99.1280	Prec@5 99.9980	
Val: [180]	Time 15.304	Data 0.095	Loss 1.109	Prec@1 75.2100	Prec@5 93.6100	
Best Prec@1: [76.200]	
Starting epoch number: 181 Learning rate: 0.010000000000000002
Train: [181]	Time 236.303	Data 0.270	Loss 0.051	Prec@1 99.1960	Prec@5 99.9960	
Val: [181]	Time 15.325	Data 0.092	Loss 1.115	Prec@1 75.5700	Prec@5 93.9100	
Best Prec@1: [76.200]	
Starting epoch number: 182 Learning rate: 0.010000000000000002
Train: [182]	Time 235.765	Data 0.264	Loss 0.049	Prec@1 99.2960	Prec@5 100.0000	
Val: [182]	Time 15.251	Data 0.099	Loss 1.120	Prec@1 75.5500	Prec@5 93.7900	
Best Prec@1: [76.200]	
Starting epoch number: 183 Learning rate: 0.010000000000000002
Train: [183]	Time 236.058	Data 0.252	Loss 0.050	Prec@1 99.1540	Prec@5 99.9980	
Val: [183]	Time 15.257	Data 0.107	Loss 1.128	Prec@1 75.1200	Prec@5 93.6800	
Best Prec@1: [76.200]	
Starting epoch number: 184 Learning rate: 0.010000000000000002
Train: [184]	Time 235.927	Data 0.278	Loss 0.049	Prec@1 99.2660	Prec@5 99.9960	
Val: [184]	Time 15.343	Data 0.111	Loss 1.129	Prec@1 75.6400	Prec@5 93.7800	
Best Prec@1: [76.200]	
Starting epoch number: 185 Learning rate: 0.010000000000000002
Train: [185]	Time 235.851	Data 0.271	Loss 0.048	Prec@1 99.2160	Prec@5 99.9940	
Val: [185]	Time 15.257	Data 0.095	Loss 1.127	Prec@1 75.6800	Prec@5 93.7800	
Best Prec@1: [76.200]	
Starting epoch number: 186 Learning rate: 0.010000000000000002
Train: [186]	Time 235.904	Data 0.262	Loss 0.050	Prec@1 99.1800	Prec@5 99.9920	
Val: [186]	Time 15.299	Data 0.098	Loss 1.134	Prec@1 75.4000	Prec@5 93.7500	
Best Prec@1: [76.200]	
Starting epoch number: 187 Learning rate: 0.010000000000000002
Train: [187]	Time 235.684	Data 0.263	Loss 0.048	Prec@1 99.2300	Prec@5 99.9980	
Val: [187]	Time 15.292	Data 0.108	Loss 1.125	Prec@1 75.4200	Prec@5 93.6500	
Best Prec@1: [76.200]	
Starting epoch number: 188 Learning rate: 0.010000000000000002
Train: [188]	Time 235.241	Data 0.263	Loss 0.047	Prec@1 99.1860	Prec@5 99.9980	
Val: [188]	Time 15.305	Data 0.100	Loss 1.144	Prec@1 75.0300	Prec@5 93.6200	
Best Prec@1: [76.200]	
Starting epoch number: 189 Learning rate: 0.010000000000000002
Train: [189]	Time 236.074	Data 0.263	Loss 0.047	Prec@1 99.2680	Prec@5 100.0000	
Val: [189]	Time 15.440	Data 0.181	Loss 1.118	Prec@1 75.5100	Prec@5 93.7300	
Best Prec@1: [76.200]	
Starting epoch number: 190 Learning rate: 0.010000000000000002
Train: [190]	Time 236.503	Data 0.273	Loss 0.045	Prec@1 99.3360	Prec@5 100.0000	
Val: [190]	Time 15.370	Data 0.109	Loss 1.126	Prec@1 75.6100	Prec@5 93.7600	
Best Prec@1: [76.200]	
Starting epoch number: 191 Learning rate: 0.010000000000000002
Train: [191]	Time 235.966	Data 0.282	Loss 0.045	Prec@1 99.2820	Prec@5 99.9980	
Val: [191]	Time 15.278	Data 0.101	Loss 1.147	Prec@1 75.2400	Prec@5 93.7600	
Best Prec@1: [76.200]	
Starting epoch number: 192 Learning rate: 0.010000000000000002
Train: [192]	Time 235.647	Data 0.261	Loss 0.046	Prec@1 99.3420	Prec@5 99.9960	
Val: [192]	Time 15.316	Data 0.098	Loss 1.131	Prec@1 75.4500	Prec@5 93.6800	
Best Prec@1: [76.200]	
Starting epoch number: 193 Learning rate: 0.010000000000000002
Train: [193]	Time 235.973	Data 0.259	Loss 0.044	Prec@1 99.3700	Prec@5 99.9980	
Val: [193]	Time 15.302	Data 0.110	Loss 1.129	Prec@1 75.3400	Prec@5 93.8400	
Best Prec@1: [76.200]	
Starting epoch number: 194 Learning rate: 0.010000000000000002
Train: [194]	Time 235.695	Data 0.273	Loss 0.043	Prec@1 99.3640	Prec@5 100.0000	
Val: [194]	Time 15.302	Data 0.091	Loss 1.125	Prec@1 75.4500	Prec@5 93.6200	
Best Prec@1: [76.200]	
Starting epoch number: 195 Learning rate: 0.010000000000000002
Train: [195]	Time 235.600	Data 0.259	Loss 0.043	Prec@1 99.3520	Prec@5 99.9960	
Val: [195]	Time 15.262	Data 0.098	Loss 1.151	Prec@1 75.3100	Prec@5 93.7900	
Best Prec@1: [76.200]	
Starting epoch number: 196 Learning rate: 0.010000000000000002
Train: [196]	Time 235.498	Data 0.258	Loss 0.043	Prec@1 99.3760	Prec@5 100.0000	
Val: [196]	Time 15.294	Data 0.102	Loss 1.139	Prec@1 75.1900	Prec@5 93.6400	
Best Prec@1: [76.200]	
Starting epoch number: 197 Learning rate: 0.010000000000000002
Train: [197]	Time 235.514	Data 0.271	Loss 0.043	Prec@1 99.3520	Prec@5 99.9960	
Val: [197]	Time 15.277	Data 0.099	Loss 1.141	Prec@1 75.5200	Prec@5 93.5800	
Best Prec@1: [76.200]	
Starting epoch number: 198 Learning rate: 0.010000000000000002
Train: [198]	Time 235.676	Data 0.268	Loss 0.044	Prec@1 99.3560	Prec@5 100.0000	
Val: [198]	Time 15.310	Data 0.108	Loss 1.145	Prec@1 75.2900	Prec@5 93.4900	
Best Prec@1: [76.200]	
Starting epoch number: 199 Learning rate: 0.010000000000000002
Train: [199]	Time 235.520	Data 0.285	Loss 0.042	Prec@1 99.3440	Prec@5 99.9980	
Val: [199]	Time 15.305	Data 0.094	Loss 1.133	Prec@1 75.0600	Prec@5 93.6800	
Best Prec@1: [76.200]	
Starting epoch number: 200 Learning rate: 0.010000000000000002
Train: [200]	Time 235.335	Data 0.270	Loss 0.042	Prec@1 99.4140	Prec@5 99.9980	
Val: [200]	Time 15.229	Data 0.112	Loss 1.137	Prec@1 75.5100	Prec@5 93.6800	
Best Prec@1: [76.200]	
Starting epoch number: 201 Learning rate: 0.010000000000000002
Train: [201]	Time 235.211	Data 0.276	Loss 0.040	Prec@1 99.4280	Prec@5 99.9960	
Val: [201]	Time 15.309	Data 0.103	Loss 1.121	Prec@1 75.6600	Prec@5 93.6900	
Best Prec@1: [76.200]	
Starting epoch number: 202 Learning rate: 0.010000000000000002
Train: [202]	Time 235.382	Data 0.275	Loss 0.041	Prec@1 99.3900	Prec@5 99.9980	
Val: [202]	Time 15.336	Data 0.096	Loss 1.139	Prec@1 74.9300	Prec@5 93.6800	
Best Prec@1: [76.200]	
Starting epoch number: 203 Learning rate: 0.010000000000000002
Train: [203]	Time 235.520	Data 0.263	Loss 0.040	Prec@1 99.4600	Prec@5 99.9980	
Val: [203]	Time 15.287	Data 0.101	Loss 1.125	Prec@1 74.9900	Prec@5 93.6500	
Best Prec@1: [76.200]	
Starting epoch number: 204 Learning rate: 0.010000000000000002
Train: [204]	Time 235.358	Data 0.282	Loss 0.042	Prec@1 99.3340	Prec@5 99.9980	
Val: [204]	Time 15.279	Data 0.102	Loss 1.169	Prec@1 74.8400	Prec@5 93.4200	
Best Prec@1: [76.200]	
Starting epoch number: 205 Learning rate: 0.010000000000000002
Train: [205]	Time 235.138	Data 0.279	Loss 0.043	Prec@1 99.3360	Prec@5 100.0000	
Val: [205]	Time 15.245	Data 0.096	Loss 1.139	Prec@1 75.2300	Prec@5 93.4400	
Best Prec@1: [76.200]	
Starting epoch number: 206 Learning rate: 0.010000000000000002
Train: [206]	Time 235.233	Data 0.263	Loss 0.041	Prec@1 99.3780	Prec@5 100.0000	
Val: [206]	Time 15.261	Data 0.093	Loss 1.136	Prec@1 75.0300	Prec@5 93.6100	
Best Prec@1: [76.200]	
Starting epoch number: 207 Learning rate: 0.010000000000000002
Train: [207]	Time 235.309	Data 0.275	Loss 0.042	Prec@1 99.3800	Prec@5 99.9940	
Val: [207]	Time 15.230	Data 0.106	Loss 1.142	Prec@1 75.3400	Prec@5 93.4000	
Best Prec@1: [76.200]	
Starting epoch number: 208 Learning rate: 0.010000000000000002
Train: [208]	Time 234.940	Data 0.270	Loss 0.039	Prec@1 99.4660	Prec@5 99.9980	
Val: [208]	Time 15.204	Data 0.093	Loss 1.130	Prec@1 75.2100	Prec@5 93.6300	
Best Prec@1: [76.200]	
Starting epoch number: 209 Learning rate: 0.010000000000000002
Train: [209]	Time 235.038	Data 0.271	Loss 0.040	Prec@1 99.3980	Prec@5 99.9980	
Val: [209]	Time 15.256	Data 0.116	Loss 1.142	Prec@1 75.3600	Prec@5 93.4200	
Best Prec@1: [76.200]	
Starting epoch number: 210 Learning rate: 0.010000000000000002
Train: [210]	Time 235.017	Data 0.282	Loss 0.040	Prec@1 99.3840	Prec@5 99.9960	
Val: [210]	Time 15.322	Data 0.092	Loss 1.141	Prec@1 75.3500	Prec@5 93.5700	
Best Prec@1: [76.200]	
Starting epoch number: 211 Learning rate: 0.010000000000000002
Train: [211]	Time 235.009	Data 0.279	Loss 0.040	Prec@1 99.4600	Prec@5 99.9940	
Val: [211]	Time 15.205	Data 0.099	Loss 1.143	Prec@1 75.0500	Prec@5 93.6300	
Best Prec@1: [76.200]	
Starting epoch number: 212 Learning rate: 0.010000000000000002
Train: [212]	Time 235.069	Data 0.266	Loss 0.042	Prec@1 99.3640	Prec@5 99.9960	
Val: [212]	Time 15.298	Data 0.115	Loss 1.136	Prec@1 75.4500	Prec@5 93.4300	
Best Prec@1: [76.200]	
Starting epoch number: 213 Learning rate: 0.010000000000000002
Train: [213]	Time 235.169	Data 0.264	Loss 0.042	Prec@1 99.3560	Prec@5 99.9980	
Val: [213]	Time 15.214	Data 0.106	Loss 1.138	Prec@1 75.2300	Prec@5 93.5900	
Best Prec@1: [76.200]	
Starting epoch number: 214 Learning rate: 0.010000000000000002
Train: [214]	Time 235.088	Data 0.266	Loss 0.042	Prec@1 99.3500	Prec@5 100.0000	
Val: [214]	Time 15.256	Data 0.102	Loss 1.157	Prec@1 74.9700	Prec@5 93.3900	
Best Prec@1: [76.200]	
Starting epoch number: 215 Learning rate: 0.010000000000000002
Train: [215]	Time 235.028	Data 0.261	Loss 0.043	Prec@1 99.3500	Prec@5 99.9980	
Val: [215]	Time 15.268	Data 0.090	Loss 1.145	Prec@1 75.4600	Prec@5 93.3200	
Best Prec@1: [76.200]	
Starting epoch number: 216 Learning rate: 0.010000000000000002
Train: [216]	Time 235.027	Data 0.260	Loss 0.041	Prec@1 99.3380	Prec@5 100.0000	
Val: [216]	Time 15.238	Data 0.115	Loss 1.154	Prec@1 75.0300	Prec@5 93.3500	
Best Prec@1: [76.200]	
Starting epoch number: 217 Learning rate: 0.010000000000000002
Train: [217]	Time 234.821	Data 0.256	Loss 0.044	Prec@1 99.2840	Prec@5 99.9960	
Val: [217]	Time 15.269	Data 0.104	Loss 1.147	Prec@1 74.7600	Prec@5 93.4800	
Best Prec@1: [76.200]	
Starting epoch number: 218 Learning rate: 0.010000000000000002
Train: [218]	Time 235.072	Data 0.290	Loss 0.044	Prec@1 99.3420	Prec@5 99.9980	
Val: [218]	Time 15.285	Data 0.092	Loss 1.139	Prec@1 75.0500	Prec@5 93.5100	
Best Prec@1: [76.200]	
Starting epoch number: 219 Learning rate: 0.010000000000000002
Train: [219]	Time 235.414	Data 0.328	Loss 0.042	Prec@1 99.3800	Prec@5 100.0000	
Val: [219]	Time 15.179	Data 0.101	Loss 1.163	Prec@1 74.7000	Prec@5 93.1800	
Best Prec@1: [76.200]	
Starting epoch number: 220 Learning rate: 0.010000000000000002
Train: [220]	Time 235.429	Data 0.318	Loss 0.042	Prec@1 99.3760	Prec@5 99.9960	
Val: [220]	Time 15.239	Data 0.102	Loss 1.134	Prec@1 74.8600	Prec@5 93.4800	
Best Prec@1: [76.200]	
Starting epoch number: 221 Learning rate: 0.010000000000000002
Train: [221]	Time 235.284	Data 0.318	Loss 0.044	Prec@1 99.2920	Prec@5 99.9960	
Val: [221]	Time 15.215	Data 0.096	Loss 1.149	Prec@1 74.5200	Prec@5 93.4800	
Best Prec@1: [76.200]	
Starting epoch number: 222 Learning rate: 0.010000000000000002
Train: [222]	Time 235.051	Data 0.325	Loss 0.044	Prec@1 99.3240	Prec@5 100.0000	
Val: [222]	Time 15.302	Data 0.115	Loss 1.139	Prec@1 75.0100	Prec@5 93.2900	
Best Prec@1: [76.200]	
Starting epoch number: 223 Learning rate: 0.010000000000000002
Train: [223]	Time 235.537	Data 0.341	Loss 0.043	Prec@1 99.3140	Prec@5 99.9960	
Val: [223]	Time 15.260	Data 0.099	Loss 1.163	Prec@1 74.5000	Prec@5 93.4200	
Best Prec@1: [76.200]	
Starting epoch number: 224 Learning rate: 0.010000000000000002
Train: [224]	Time 235.206	Data 0.334	Loss 0.043	Prec@1 99.3860	Prec@5 99.9960	
Val: [224]	Time 15.276	Data 0.117	Loss 1.145	Prec@1 74.8900	Prec@5 93.3500	
Best Prec@1: [76.200]	
Starting epoch number: 225 Learning rate: 0.0010000000000000002
Train: [225]	Time 235.025	Data 0.328	Loss 0.034	Prec@1 99.5580	Prec@5 100.0000	
Val: [225]	Time 15.237	Data 0.104	Loss 1.116	Prec@1 75.6300	Prec@5 93.6200	
Best Prec@1: [76.200]	
Starting epoch number: 226 Learning rate: 0.0010000000000000002
Train: [226]	Time 235.075	Data 0.321	Loss 0.027	Prec@1 99.7660	Prec@5 100.0000	
Val: [226]	Time 15.223	Data 0.096	Loss 1.097	Prec@1 75.7800	Prec@5 93.7600	
Best Prec@1: [76.200]	
Starting epoch number: 227 Learning rate: 0.0010000000000000002
Train: [227]	Time 235.236	Data 0.330	Loss 0.025	Prec@1 99.7880	Prec@5 100.0000	
Val: [227]	Time 15.292	Data 0.116	Loss 1.108	Prec@1 75.7000	Prec@5 93.4700	
Best Prec@1: [76.200]	
Starting epoch number: 228 Learning rate: 0.0010000000000000002
Train: [228]	Time 234.775	Data 0.335	Loss 0.025	Prec@1 99.7680	Prec@5 99.9980	
Val: [228]	Time 15.319	Data 0.111	Loss 1.107	Prec@1 75.6100	Prec@5 93.6100	
Best Prec@1: [76.200]	
Starting epoch number: 229 Learning rate: 0.0010000000000000002
Train: [229]	Time 235.144	Data 0.314	Loss 0.023	Prec@1 99.8200	Prec@5 100.0000	
Val: [229]	Time 15.246	Data 0.104	Loss 1.112	Prec@1 75.7700	Prec@5 93.6700	
Best Prec@1: [76.200]	
Starting epoch number: 230 Learning rate: 0.0010000000000000002
Train: [230]	Time 235.298	Data 0.264	Loss 0.022	Prec@1 99.8120	Prec@5 100.0000	
Val: [230]	Time 15.277	Data 0.107	Loss 1.112	Prec@1 75.6300	Prec@5 93.6800	
Best Prec@1: [76.200]	
Starting epoch number: 231 Learning rate: 0.0010000000000000002
Train: [231]	Time 234.989	Data 0.285	Loss 0.022	Prec@1 99.8360	Prec@5 100.0000	
Val: [231]	Time 15.235	Data 0.106	Loss 1.114	Prec@1 75.6300	Prec@5 93.5200	
Best Prec@1: [76.200]	
Starting epoch number: 232 Learning rate: 0.0010000000000000002
Train: [232]	Time 234.706	Data 0.282	Loss 0.022	Prec@1 99.8560	Prec@5 100.0000	
Val: [232]	Time 15.266	Data 0.090	Loss 1.117	Prec@1 75.5200	Prec@5 93.5800	
Best Prec@1: [76.200]	
Starting epoch number: 233 Learning rate: 0.0010000000000000002
Train: [233]	Time 235.316	Data 0.265	Loss 0.021	Prec@1 99.8560	Prec@5 100.0000	
Val: [233]	Time 15.308	Data 0.119	Loss 1.107	Prec@1 75.6900	Prec@5 93.6300	
Best Prec@1: [76.200]	
Starting epoch number: 234 Learning rate: 0.0010000000000000002
Train: [234]	Time 235.636	Data 0.282	Loss 0.021	Prec@1 99.8460	Prec@5 100.0000	
Val: [234]	Time 15.281	Data 0.102	Loss 1.115	Prec@1 75.7600	Prec@5 93.5600	
Best Prec@1: [76.200]	
Starting epoch number: 235 Learning rate: 0.0010000000000000002
Train: [235]	Time 235.522	Data 0.279	Loss 0.020	Prec@1 99.8860	Prec@5 100.0000	
Val: [235]	Time 15.284	Data 0.096	Loss 1.111	Prec@1 75.6700	Prec@5 93.5500	
Best Prec@1: [76.200]	
Starting epoch number: 236 Learning rate: 0.0010000000000000002
Train: [236]	Time 235.239	Data 0.263	Loss 0.020	Prec@1 99.8740	Prec@5 100.0000	
Val: [236]	Time 15.222	Data 0.089	Loss 1.110	Prec@1 75.7500	Prec@5 93.6200	
Best Prec@1: [76.200]	
Starting epoch number: 237 Learning rate: 0.0010000000000000002
Train: [237]	Time 235.535	Data 0.272	Loss 0.019	Prec@1 99.8880	Prec@5 100.0000	
Val: [237]	Time 15.242	Data 0.093	Loss 1.111	Prec@1 75.6400	Prec@5 93.6200	
Best Prec@1: [76.200]	
Starting epoch number: 238 Learning rate: 0.0010000000000000002
Train: [238]	Time 235.422	Data 0.281	Loss 0.019	Prec@1 99.8960	Prec@5 100.0000	
Val: [238]	Time 15.288	Data 0.102	Loss 1.111	Prec@1 75.6100	Prec@5 93.5800	
Best Prec@1: [76.200]	
Starting epoch number: 239 Learning rate: 0.0010000000000000002
Train: [239]	Time 235.418	Data 0.280	Loss 0.019	Prec@1 99.8840	Prec@5 100.0000	
Val: [239]	Time 15.257	Data 0.099	Loss 1.107	Prec@1 75.9000	Prec@5 93.5900	
Best Prec@1: [76.200]	
Starting epoch number: 240 Learning rate: 0.0010000000000000002
Train: [240]	Time 235.640	Data 0.292	Loss 0.019	Prec@1 99.8540	Prec@5 100.0000	
Val: [240]	Time 15.269	Data 0.086	Loss 1.104	Prec@1 75.9400	Prec@5 93.5700	
Best Prec@1: [76.200]	
Starting epoch number: 241 Learning rate: 0.0010000000000000002
Train: [241]	Time 235.504	Data 0.281	Loss 0.019	Prec@1 99.8980	Prec@5 100.0000	
Val: [241]	Time 15.261	Data 0.106	Loss 1.108	Prec@1 76.0700	Prec@5 93.6900	
Best Prec@1: [76.200]	
Starting epoch number: 242 Learning rate: 0.0010000000000000002
Train: [242]	Time 235.405	Data 0.267	Loss 0.019	Prec@1 99.8980	Prec@5 100.0000	
Val: [242]	Time 15.330	Data 0.091	Loss 1.112	Prec@1 75.8900	Prec@5 93.5800	
Best Prec@1: [76.200]	
Starting epoch number: 243 Learning rate: 0.0010000000000000002
Train: [243]	Time 235.724	Data 0.280	Loss 0.019	Prec@1 99.8640	Prec@5 100.0000	
Val: [243]	Time 15.377	Data 0.099	Loss 1.115	Prec@1 75.8800	Prec@5 93.6500	
Best Prec@1: [76.200]	
Starting epoch number: 244 Learning rate: 0.0010000000000000002
Train: [244]	Time 235.827	Data 0.256	Loss 0.018	Prec@1 99.9020	Prec@5 100.0000	
Val: [244]	Time 15.273	Data 0.096	Loss 1.109	Prec@1 75.9900	Prec@5 93.6300	
Best Prec@1: [76.200]	
Starting epoch number: 245 Learning rate: 0.0010000000000000002
Train: [245]	Time 236.045	Data 0.256	Loss 0.019	Prec@1 99.8800	Prec@5 100.0000	
Val: [245]	Time 15.326	Data 0.111	Loss 1.115	Prec@1 75.7700	Prec@5 93.5900	
Best Prec@1: [76.200]	
Starting epoch number: 246 Learning rate: 0.0010000000000000002
Train: [246]	Time 235.479	Data 0.266	Loss 0.018	Prec@1 99.9000	Prec@5 100.0000	
Val: [246]	Time 15.303	Data 0.099	Loss 1.107	Prec@1 75.7400	Prec@5 93.7200	
Best Prec@1: [76.200]	
Starting epoch number: 247 Learning rate: 0.0010000000000000002
Train: [247]	Time 235.606	Data 0.285	Loss 0.018	Prec@1 99.8960	Prec@5 100.0000	
Val: [247]	Time 15.353	Data 0.089	Loss 1.113	Prec@1 75.8700	Prec@5 93.5900	
Best Prec@1: [76.200]	
Starting epoch number: 248 Learning rate: 0.0010000000000000002
Train: [248]	Time 235.764	Data 0.264	Loss 0.018	Prec@1 99.8840	Prec@5 100.0000	
Val: [248]	Time 15.285	Data 0.106	Loss 1.110	Prec@1 76.0200	Prec@5 93.5200	
Best Prec@1: [76.200]	
Starting epoch number: 249 Learning rate: 0.0010000000000000002
Train: [249]	Time 235.920	Data 0.250	Loss 0.018	Prec@1 99.9020	Prec@5 100.0000	
Val: [249]	Time 15.352	Data 0.101	Loss 1.108	Prec@1 75.9800	Prec@5 93.5700	
Best Prec@1: [76.200]	
Starting epoch number: 250 Learning rate: 0.0010000000000000002
Train: [250]	Time 235.914	Data 0.281	Loss 0.018	Prec@1 99.8720	Prec@5 100.0000	
Val: [250]	Time 15.271	Data 0.101	Loss 1.109	Prec@1 76.0200	Prec@5 93.5500	
Best Prec@1: [76.200]	
Starting epoch number: 251 Learning rate: 0.0010000000000000002
Train: [251]	Time 235.804	Data 0.265	Loss 0.017	Prec@1 99.8900	Prec@5 100.0000	
Val: [251]	Time 15.292	Data 0.106	Loss 1.121	Prec@1 75.8300	Prec@5 93.4800	
Best Prec@1: [76.200]	
Starting epoch number: 252 Learning rate: 0.0010000000000000002
Train: [252]	Time 236.077	Data 0.274	Loss 0.017	Prec@1 99.9060	Prec@5 100.0000	
Val: [252]	Time 15.294	Data 0.102	Loss 1.112	Prec@1 75.9200	Prec@5 93.6400	
Best Prec@1: [76.200]	
Starting epoch number: 253 Learning rate: 0.0010000000000000002
Train: [253]	Time 235.893	Data 0.258	Loss 0.017	Prec@1 99.9140	Prec@5 100.0000	
Val: [253]	Time 15.270	Data 0.101	Loss 1.111	Prec@1 76.1200	Prec@5 93.6800	
Best Prec@1: [76.200]	
Starting epoch number: 254 Learning rate: 0.0010000000000000002
Train: [254]	Time 236.264	Data 0.267	Loss 0.017	Prec@1 99.9120	Prec@5 100.0000	
Val: [254]	Time 15.307	Data 0.095	Loss 1.109	Prec@1 75.7600	Prec@5 93.6400	
Best Prec@1: [76.200]	
Starting epoch number: 255 Learning rate: 0.0010000000000000002
Train: [255]	Time 236.515	Data 0.276	Loss 0.017	Prec@1 99.9100	Prec@5 100.0000	
Val: [255]	Time 15.304	Data 0.107	Loss 1.110	Prec@1 75.7000	Prec@5 93.5600	
Best Prec@1: [76.200]	
Starting epoch number: 256 Learning rate: 0.0010000000000000002
Train: [256]	Time 236.309	Data 0.272	Loss 0.017	Prec@1 99.9040	Prec@5 100.0000	
Val: [256]	Time 15.312	Data 0.106	Loss 1.116	Prec@1 76.1100	Prec@5 93.7100	
Best Prec@1: [76.200]	
Starting epoch number: 257 Learning rate: 0.0010000000000000002
Train: [257]	Time 236.715	Data 0.283	Loss 0.017	Prec@1 99.8940	Prec@5 100.0000	
Val: [257]	Time 15.379	Data 0.103	Loss 1.108	Prec@1 76.1200	Prec@5 93.6200	
Best Prec@1: [76.200]	
Starting epoch number: 258 Learning rate: 0.0010000000000000002
Train: [258]	Time 236.998	Data 0.291	Loss 0.017	Prec@1 99.9020	Prec@5 100.0000	
Val: [258]	Time 15.347	Data 0.104	Loss 1.115	Prec@1 75.9300	Prec@5 93.5800	
Best Prec@1: [76.200]	
Starting epoch number: 259 Learning rate: 0.0010000000000000002
Train: [259]	Time 237.093	Data 0.255	Loss 0.016	Prec@1 99.9280	Prec@5 100.0000	
Val: [259]	Time 15.380	Data 0.098	Loss 1.115	Prec@1 75.6100	Prec@5 93.5600	
Best Prec@1: [76.200]	
Starting epoch number: 260 Learning rate: 0.0010000000000000002
Train: [260]	Time 236.843	Data 0.267	Loss 0.017	Prec@1 99.9100	Prec@5 100.0000	
Val: [260]	Time 15.319	Data 0.103	Loss 1.111	Prec@1 75.9100	Prec@5 93.5800	
Best Prec@1: [76.200]	
Starting epoch number: 261 Learning rate: 0.0010000000000000002
Train: [261]	Time 237.029	Data 0.258	Loss 0.017	Prec@1 99.9160	Prec@5 100.0000	
Val: [261]	Time 15.402	Data 0.100	Loss 1.114	Prec@1 75.8500	Prec@5 93.4700	
Best Prec@1: [76.200]	
Starting epoch number: 262 Learning rate: 0.0010000000000000002
Train: [262]	Time 236.977	Data 0.288	Loss 0.017	Prec@1 99.9180	Prec@5 100.0000	
Val: [262]	Time 15.333	Data 0.101	Loss 1.110	Prec@1 75.7300	Prec@5 93.6100	
Best Prec@1: [76.200]	
Starting epoch number: 263 Learning rate: 0.0010000000000000002
Train: [263]	Time 236.987	Data 0.259	Loss 0.017	Prec@1 99.9180	Prec@5 100.0000	
Val: [263]	Time 15.408	Data 0.112	Loss 1.114	Prec@1 75.8600	Prec@5 93.4600	
Best Prec@1: [76.200]	
Starting epoch number: 264 Learning rate: 0.0010000000000000002
Train: [264]	Time 236.786	Data 0.285	Loss 0.017	Prec@1 99.9180	Prec@5 100.0000	
Val: [264]	Time 15.331	Data 0.089	Loss 1.107	Prec@1 75.8500	Prec@5 93.5400	
Best Prec@1: [76.200]	
Starting epoch number: 265 Learning rate: 0.0010000000000000002
Train: [265]	Time 236.702	Data 0.272	Loss 0.017	Prec@1 99.9000	Prec@5 100.0000	
Val: [265]	Time 15.344	Data 0.103	Loss 1.105	Prec@1 75.7300	Prec@5 93.5000	
Best Prec@1: [76.200]	
Starting epoch number: 266 Learning rate: 0.0010000000000000002
Train: [266]	Time 236.564	Data 0.276	Loss 0.016	Prec@1 99.9240	Prec@5 100.0000	
Val: [266]	Time 15.324	Data 0.096	Loss 1.108	Prec@1 75.8300	Prec@5 93.5700	
Best Prec@1: [76.200]	
Starting epoch number: 267 Learning rate: 0.0010000000000000002
Train: [267]	Time 236.776	Data 0.255	Loss 0.017	Prec@1 99.9140	Prec@5 100.0000	
Val: [267]	Time 15.582	Data 0.097	Loss 1.107	Prec@1 76.0100	Prec@5 93.4900	
Best Prec@1: [76.200]	
Starting epoch number: 268 Learning rate: 0.0010000000000000002
Train: [268]	Time 237.206	Data 0.260	Loss 0.016	Prec@1 99.9180	Prec@5 100.0000	
Val: [268]	Time 15.414	Data 0.094	Loss 1.111	Prec@1 75.9200	Prec@5 93.5300	
Best Prec@1: [76.200]	
Starting epoch number: 269 Learning rate: 0.0010000000000000002
Train: [269]	Time 237.266	Data 0.263	Loss 0.017	Prec@1 99.9300	Prec@5 100.0000	
Val: [269]	Time 15.395	Data 0.106	Loss 1.110	Prec@1 75.9300	Prec@5 93.4000	
Best Prec@1: [76.200]	
Starting epoch number: 270 Learning rate: 0.0010000000000000002
Train: [270]	Time 237.410	Data 0.258	Loss 0.016	Prec@1 99.9200	Prec@5 100.0000	
Val: [270]	Time 15.414	Data 0.098	Loss 1.103	Prec@1 75.9200	Prec@5 93.5400	
Best Prec@1: [76.200]	
Starting epoch number: 271 Learning rate: 0.0010000000000000002
Train: [271]	Time 237.326	Data 0.274	Loss 0.017	Prec@1 99.8940	Prec@5 100.0000	
Val: [271]	Time 15.387	Data 0.102	Loss 1.103	Prec@1 76.0000	Prec@5 93.6200	
Best Prec@1: [76.200]	
Starting epoch number: 272 Learning rate: 0.0010000000000000002
Train: [272]	Time 237.427	Data 0.259	Loss 0.016	Prec@1 99.9400	Prec@5 100.0000	
Val: [272]	Time 15.395	Data 0.103	Loss 1.117	Prec@1 76.0700	Prec@5 93.5800	
Best Prec@1: [76.200]	
Starting epoch number: 273 Learning rate: 0.0010000000000000002
Train: [273]	Time 237.165	Data 0.282	Loss 0.017	Prec@1 99.8940	Prec@5 100.0000	
Val: [273]	Time 15.340	Data 0.099	Loss 1.113	Prec@1 75.8300	Prec@5 93.3900	
Best Prec@1: [76.200]	
Starting epoch number: 274 Learning rate: 0.0010000000000000002
Train: [274]	Time 236.912	Data 0.259	Loss 0.016	Prec@1 99.9200	Prec@5 100.0000	
Val: [274]	Time 15.426	Data 0.107	Loss 1.113	Prec@1 75.7600	Prec@5 93.5700	
Best Prec@1: [76.200]	
Starting epoch number: 275 Learning rate: 0.0010000000000000002
Train: [275]	Time 237.209	Data 0.276	Loss 0.016	Prec@1 99.9100	Prec@5 100.0000	
Val: [275]	Time 15.367	Data 0.104	Loss 1.103	Prec@1 75.9400	Prec@5 93.4500	
Best Prec@1: [76.200]	
Starting epoch number: 276 Learning rate: 0.0010000000000000002
Train: [276]	Time 237.318	Data 0.276	Loss 0.016	Prec@1 99.9080	Prec@5 100.0000	
Val: [276]	Time 15.339	Data 0.093	Loss 1.111	Prec@1 75.9300	Prec@5 93.5000	
Best Prec@1: [76.200]	
Starting epoch number: 277 Learning rate: 0.0010000000000000002
Train: [277]	Time 237.365	Data 0.276	Loss 0.016	Prec@1 99.9280	Prec@5 100.0000	
Val: [277]	Time 15.455	Data 0.085	Loss 1.108	Prec@1 75.9100	Prec@5 93.4800	
Best Prec@1: [76.200]	
Starting epoch number: 278 Learning rate: 0.0010000000000000002
Train: [278]	Time 237.135	Data 0.263	Loss 0.016	Prec@1 99.9160	Prec@5 100.0000	
Val: [278]	Time 15.389	Data 0.102	Loss 1.108	Prec@1 75.8400	Prec@5 93.5500	
Best Prec@1: [76.200]	
Starting epoch number: 279 Learning rate: 0.0010000000000000002
Train: [279]	Time 237.097	Data 0.284	Loss 0.016	Prec@1 99.9080	Prec@5 100.0000	
Val: [279]	Time 15.391	Data 0.096	Loss 1.113	Prec@1 76.0500	Prec@5 93.5200	
Best Prec@1: [76.200]	
Starting epoch number: 280 Learning rate: 0.0010000000000000002
Train: [280]	Time 237.635	Data 0.274	Loss 0.016	Prec@1 99.9180	Prec@5 100.0000	
Val: [280]	Time 15.430	Data 0.110	Loss 1.118	Prec@1 75.8300	Prec@5 93.5000	
Best Prec@1: [76.200]	
Starting epoch number: 281 Learning rate: 0.0010000000000000002
Train: [281]	Time 235.372	Data 0.291	Loss 0.016	Prec@1 99.9140	Prec@5 100.0000	
Val: [281]	Time 15.249	Data 0.112	Loss 1.102	Prec@1 76.1000	Prec@5 93.5000	
Best Prec@1: [76.200]	
Starting epoch number: 282 Learning rate: 0.0010000000000000002
Train: [282]	Time 236.291	Data 0.270	Loss 0.016	Prec@1 99.9120	Prec@5 100.0000	
Val: [282]	Time 15.359	Data 0.105	Loss 1.098	Prec@1 75.7700	Prec@5 93.6100	
Best Prec@1: [76.200]	
Starting epoch number: 283 Learning rate: 0.0010000000000000002
Train: [283]	Time 237.122	Data 0.279	Loss 0.016	Prec@1 99.9080	Prec@5 100.0000	
Val: [283]	Time 15.418	Data 0.104	Loss 1.104	Prec@1 75.8400	Prec@5 93.5500	
Best Prec@1: [76.200]	
Starting epoch number: 284 Learning rate: 0.0010000000000000002
Train: [284]	Time 236.824	Data 0.277	Loss 0.016	Prec@1 99.9260	Prec@5 100.0000	
Val: [284]	Time 15.514	Data 0.099	Loss 1.100	Prec@1 75.8800	Prec@5 93.5600	
Best Prec@1: [76.200]	
Starting epoch number: 285 Learning rate: 0.0010000000000000002
Train: [285]	Time 237.111	Data 0.260	Loss 0.016	Prec@1 99.9200	Prec@5 100.0000	
Val: [285]	Time 15.403	Data 0.102	Loss 1.110	Prec@1 75.9100	Prec@5 93.6100	
Best Prec@1: [76.200]	
Starting epoch number: 286 Learning rate: 0.0010000000000000002
Train: [286]	Time 237.514	Data 0.274	Loss 0.016	Prec@1 99.9060	Prec@5 100.0000	
Val: [286]	Time 15.442	Data 0.098	Loss 1.113	Prec@1 75.8400	Prec@5 93.6600	
Best Prec@1: [76.200]	
Starting epoch number: 287 Learning rate: 0.0010000000000000002
Train: [287]	Time 237.511	Data 0.264	Loss 0.016	Prec@1 99.9220	Prec@5 99.9980	
Val: [287]	Time 15.393	Data 0.089	Loss 1.107	Prec@1 75.8100	Prec@5 93.6000	
Best Prec@1: [76.200]	
Starting epoch number: 288 Learning rate: 0.0010000000000000002
Train: [288]	Time 237.460	Data 0.278	Loss 0.016	Prec@1 99.9440	Prec@5 100.0000	
Val: [288]	Time 15.418	Data 0.110	Loss 1.117	Prec@1 75.9000	Prec@5 93.4200	
Best Prec@1: [76.200]	
Starting epoch number: 289 Learning rate: 0.0010000000000000002
Train: [289]	Time 237.600	Data 0.285	Loss 0.016	Prec@1 99.9260	Prec@5 100.0000	
Val: [289]	Time 15.471	Data 0.116	Loss 1.102	Prec@1 75.9700	Prec@5 93.4700	
Best Prec@1: [76.200]	
Starting epoch number: 290 Learning rate: 0.0010000000000000002
Train: [290]	Time 237.747	Data 0.275	Loss 0.016	Prec@1 99.9220	Prec@5 100.0000	
Val: [290]	Time 15.475	Data 0.100	Loss 1.111	Prec@1 75.7200	Prec@5 93.6100	
Best Prec@1: [76.200]	
Starting epoch number: 291 Learning rate: 0.0010000000000000002
Train: [291]	Time 237.676	Data 0.264	Loss 0.015	Prec@1 99.9200	Prec@5 100.0000	
Val: [291]	Time 15.425	Data 0.094	Loss 1.107	Prec@1 75.8000	Prec@5 93.5500	
Best Prec@1: [76.200]	
Starting epoch number: 292 Learning rate: 0.0010000000000000002
Train: [292]	Time 237.303	Data 0.294	Loss 0.016	Prec@1 99.9460	Prec@5 100.0000	
Val: [292]	Time 15.394	Data 0.091	Loss 1.108	Prec@1 75.7800	Prec@5 93.5500	
Best Prec@1: [76.200]	
Starting epoch number: 293 Learning rate: 0.0010000000000000002
Train: [293]	Time 237.400	Data 0.266	Loss 0.016	Prec@1 99.9100	Prec@5 100.0000	
Val: [293]	Time 15.385	Data 0.092	Loss 1.115	Prec@1 76.0200	Prec@5 93.4600	
Best Prec@1: [76.200]	
Starting epoch number: 294 Learning rate: 0.0010000000000000002
Train: [294]	Time 237.257	Data 0.275	Loss 0.016	Prec@1 99.9240	Prec@5 100.0000	
Val: [294]	Time 15.400	Data 0.110	Loss 1.111	Prec@1 75.9100	Prec@5 93.4000	
Best Prec@1: [76.200]	
Starting epoch number: 295 Learning rate: 0.0010000000000000002
Train: [295]	Time 237.168	Data 0.278	Loss 0.016	Prec@1 99.9180	Prec@5 100.0000	
Val: [295]	Time 15.496	Data 0.110	Loss 1.101	Prec@1 76.0800	Prec@5 93.6000	
Best Prec@1: [76.200]	
Starting epoch number: 296 Learning rate: 0.0010000000000000002
Train: [296]	Time 237.310	Data 0.282	Loss 0.016	Prec@1 99.9080	Prec@5 100.0000	
Val: [296]	Time 15.430	Data 0.093	Loss 1.104	Prec@1 75.9300	Prec@5 93.4400	
Best Prec@1: [76.200]	
Starting epoch number: 297 Learning rate: 0.0010000000000000002
Train: [297]	Time 237.474	Data 0.272	Loss 0.015	Prec@1 99.9300	Prec@5 100.0000	
Val: [297]	Time 15.401	Data 0.111	Loss 1.103	Prec@1 75.8800	Prec@5 93.4700	
Best Prec@1: [76.200]	
Starting epoch number: 298 Learning rate: 0.0010000000000000002
Train: [298]	Time 237.264	Data 0.264	Loss 0.015	Prec@1 99.9320	Prec@5 100.0000	
Val: [298]	Time 15.508	Data 0.109	Loss 1.108	Prec@1 75.8200	Prec@5 93.3600	
Best Prec@1: [76.200]	
Starting epoch number: 299 Learning rate: 0.0010000000000000002
Train: [299]	Time 237.212	Data 0.287	Loss 0.015	Prec@1 99.9420	Prec@5 100.0000	
Val: [299]	Time 15.400	Data 0.118	Loss 1.101	Prec@1 75.8500	Prec@5 93.5200	
Best Prec@1: [76.200]	
