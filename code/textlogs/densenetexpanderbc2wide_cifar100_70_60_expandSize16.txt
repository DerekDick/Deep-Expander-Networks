Namespace(acc_type='class', augment=True, batch_size=64, bottleneck=True, cachemode=True, cardinality=8, criterion='crossentropy', cuda=True, data_dir='../data', dataset='cifar100', decayinterval=None, decaylevel=None, droprate=0, epochs=300, evaluate=False, expandConfig=None, expandSize=16, from_modelzoo=False, growth=60, layers=70, learningratescheduler='cifarschedular', logdir='../logs/densenetexpanderbc2wide_cifar100_70_60_expandSize16', lr=0.1, manualSeed=123, maxlr=0.1, minlr=0.0005, model_def='densenetexpander2_cifar', momentum=0.9, name='densenetexpanderbc2wide_cifar100_70_60_expandSize16', nclasses=100, nesterov=True, ngpus=1, optimType='sgd', pretrained=False, pretrained_file='', printfreq=200, reduce=0.5, resume='', start_epoch=0, store='', tenCrop=False, tensorboard=True, testOnly=False, verbose=False, weightDecay=0.0001, weight_init=False, widen_factor=4, workers=2)
DenseNet3 (
  (conv1): Conv2d(3, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): DenseBlock (
    (layer): Sequential (
      (0): BottleneckBlock (
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (1): BottleneckBlock (
        (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (2): BottleneckBlock (
        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (3): BottleneckBlock (
        (bn1): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (4): BottleneckBlock (
        (bn1): BatchNorm2d(360, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (5): BottleneckBlock (
        (bn1): BatchNorm2d(420, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (6): BottleneckBlock (
        (bn1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (7): BottleneckBlock (
        (bn1): BatchNorm2d(540, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (8): BottleneckBlock (
        (bn1): BatchNorm2d(600, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (9): BottleneckBlock (
        (bn1): BatchNorm2d(660, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (10): BottleneckBlock (
        (bn1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
    )
  )
  (trans1): TransitionBlock (
    (bn1): BatchNorm2d(780, eps=1e-05, momentum=0.1, affine=True)
    (relu): ReLU (inplace)
    (conv1): Conv2d(780, 390, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block2): DenseBlock (
    (layer): Sequential (
      (0): BottleneckBlock (
        (bn1): BatchNorm2d(390, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (1): BottleneckBlock (
        (bn1): BatchNorm2d(450, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (2): BottleneckBlock (
        (bn1): BatchNorm2d(510, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (3): BottleneckBlock (
        (bn1): BatchNorm2d(570, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (4): BottleneckBlock (
        (bn1): BatchNorm2d(630, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (5): BottleneckBlock (
        (bn1): BatchNorm2d(690, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (6): BottleneckBlock (
        (bn1): BatchNorm2d(750, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (7): BottleneckBlock (
        (bn1): BatchNorm2d(810, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (8): BottleneckBlock (
        (bn1): BatchNorm2d(870, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (9): BottleneckBlock (
        (bn1): BatchNorm2d(930, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (10): BottleneckBlock (
        (bn1): BatchNorm2d(990, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
    )
  )
  (trans2): TransitionBlock (
    (bn1): BatchNorm2d(1050, eps=1e-05, momentum=0.1, affine=True)
    (relu): ReLU (inplace)
    (conv1): Conv2d(1050, 525, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block3): DenseBlock (
    (layer): Sequential (
      (0): BottleneckBlock (
        (bn1): BatchNorm2d(525, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (1): BottleneckBlock (
        (bn1): BatchNorm2d(585, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (2): BottleneckBlock (
        (bn1): BatchNorm2d(645, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (3): BottleneckBlock (
        (bn1): BatchNorm2d(705, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (4): BottleneckBlock (
        (bn1): BatchNorm2d(765, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (5): BottleneckBlock (
        (bn1): BatchNorm2d(825, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (6): BottleneckBlock (
        (bn1): BatchNorm2d(885, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (7): BottleneckBlock (
        (bn1): BatchNorm2d(945, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (8): BottleneckBlock (
        (bn1): BatchNorm2d(1005, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (9): BottleneckBlock (
        (bn1): BatchNorm2d(1065, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (10): BottleneckBlock (
        (bn1): BatchNorm2d(1125, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
    )
  )
  (bn1): BatchNorm2d(1185, eps=1e-05, momentum=0.1, affine=True)
  (relu): ReLU (inplace)
  (fc): Linear (1185 -> 100)
)
Files already downloaded and verified
Starting epoch number: 0 Learning rate: 0.1
Train: [0]	Time 264.323	Data 3.770	Loss 3.751	Prec@1 12.7340	Prec@5 35.6560	
Val: [0]	Time 15.759	Data 0.487	Loss 3.314	Prec@1 20.5900	Prec@5 49.2100	
Best Prec@1: [20.590]	
Starting epoch number: 1 Learning rate: 0.1
Train: [1]	Time 263.060	Data 5.250	Loss 2.711	Prec@1 30.4060	Prec@5 62.6500	
Val: [1]	Time 16.189	Data 0.866	Loss 2.567	Prec@1 35.2500	Prec@5 68.4000	
Best Prec@1: [35.250]	
Starting epoch number: 2 Learning rate: 0.1
Train: [2]	Time 262.704	Data 5.564	Loss 2.136	Prec@1 42.5040	Prec@5 75.3280	
Val: [2]	Time 16.270	Data 0.967	Loss 2.173	Prec@1 43.0500	Prec@5 76.7200	
Best Prec@1: [43.050]	
Starting epoch number: 3 Learning rate: 0.1
Train: [3]	Time 263.090	Data 5.770	Loss 1.815	Prec@1 49.9300	Prec@5 81.2000	
Val: [3]	Time 16.116	Data 0.674	Loss 1.897	Prec@1 49.4100	Prec@5 80.6100	
Best Prec@1: [49.410]	
Starting epoch number: 4 Learning rate: 0.1
Train: [4]	Time 262.393	Data 5.379	Loss 1.609	Prec@1 55.0200	Prec@5 84.6280	
Val: [4]	Time 16.398	Data 1.089	Loss 1.674	Prec@1 54.0500	Prec@5 83.5200	
Best Prec@1: [54.050]	
Starting epoch number: 5 Learning rate: 0.1
Train: [5]	Time 262.493	Data 5.480	Loss 1.467	Prec@1 58.4880	Prec@5 87.0920	
Val: [5]	Time 16.076	Data 0.675	Loss 1.607	Prec@1 57.4800	Prec@5 84.6600	
Best Prec@1: [57.480]	
Starting epoch number: 6 Learning rate: 0.1
Train: [6]	Time 262.371	Data 5.501	Loss 1.351	Prec@1 61.4440	Prec@5 88.7040	
Val: [6]	Time 15.862	Data 0.517	Loss 1.631	Prec@1 55.5600	Prec@5 84.7200	
Best Prec@1: [57.480]	
Starting epoch number: 7 Learning rate: 0.1
Train: [7]	Time 263.160	Data 5.694	Loss 1.268	Prec@1 63.3840	Prec@5 89.9060	
Val: [7]	Time 16.393	Data 0.955	Loss 1.455	Prec@1 60.6800	Prec@5 87.1900	
Best Prec@1: [60.680]	
Starting epoch number: 8 Learning rate: 0.1
Train: [8]	Time 262.037	Data 4.139	Loss 1.198	Prec@1 65.2780	Prec@5 90.7380	
Val: [8]	Time 15.709	Data 0.129	Loss 1.460	Prec@1 61.0600	Prec@5 87.4100	
Best Prec@1: [61.060]	
Starting epoch number: 9 Learning rate: 0.1
Train: [9]	Time 258.188	Data 0.437	Loss 1.145	Prec@1 66.6820	Prec@5 91.3860	
Val: [9]	Time 15.581	Data 0.163	Loss 1.509	Prec@1 59.8400	Prec@5 87.0100	
Best Prec@1: [61.060]	
Starting epoch number: 10 Learning rate: 0.1
Train: [10]	Time 258.248	Data 0.448	Loss 1.100	Prec@1 67.7140	Prec@5 92.1920	
Val: [10]	Time 15.646	Data 0.202	Loss 1.393	Prec@1 61.8900	Prec@5 88.0200	
Best Prec@1: [61.890]	
Starting epoch number: 11 Learning rate: 0.1
Train: [11]	Time 257.787	Data 0.495	Loss 1.061	Prec@1 69.0100	Prec@5 92.5420	
Val: [11]	Time 15.611	Data 0.137	Loss 1.534	Prec@1 60.2700	Prec@5 87.1600	
Best Prec@1: [61.890]	
Starting epoch number: 12 Learning rate: 0.1
Train: [12]	Time 258.188	Data 0.441	Loss 1.030	Prec@1 69.4740	Prec@5 93.0660	
Val: [12]	Time 15.626	Data 0.151	Loss 1.458	Prec@1 62.0000	Prec@5 87.8400	
Best Prec@1: [62.000]	
Starting epoch number: 13 Learning rate: 0.1
Train: [13]	Time 258.528	Data 0.495	Loss 1.004	Prec@1 70.1860	Prec@5 93.3040	
Val: [13]	Time 15.651	Data 0.148	Loss 1.455	Prec@1 62.6500	Prec@5 87.6700	
Best Prec@1: [62.650]	
Starting epoch number: 14 Learning rate: 0.1
Train: [14]	Time 257.978	Data 0.465	Loss 0.974	Prec@1 71.2380	Prec@5 93.6600	
Val: [14]	Time 15.607	Data 0.147	Loss 1.461	Prec@1 61.6400	Prec@5 88.7600	
Best Prec@1: [62.650]	
Starting epoch number: 15 Learning rate: 0.1
Train: [15]	Time 258.077	Data 0.449	Loss 0.955	Prec@1 71.7040	Prec@5 93.9480	
Val: [15]	Time 15.620	Data 0.162	Loss 1.427	Prec@1 62.4000	Prec@5 88.5500	
Best Prec@1: [62.650]	
Starting epoch number: 16 Learning rate: 0.1
Train: [16]	Time 257.764	Data 0.438	Loss 0.937	Prec@1 72.3000	Prec@5 94.1080	
Val: [16]	Time 15.643	Data 0.178	Loss 1.313	Prec@1 64.8700	Prec@5 89.5100	
Best Prec@1: [64.870]	
Starting epoch number: 17 Learning rate: 0.1
Train: [17]	Time 258.389	Data 0.558	Loss 0.921	Prec@1 72.4920	Prec@5 94.3040	
Val: [17]	Time 15.677	Data 0.176	Loss 1.486	Prec@1 62.2500	Prec@5 88.0900	
Best Prec@1: [64.870]	
Starting epoch number: 18 Learning rate: 0.1
Train: [18]	Time 258.538	Data 0.513	Loss 0.911	Prec@1 72.6620	Prec@5 94.4620	
Val: [18]	Time 15.651	Data 0.128	Loss 1.426	Prec@1 63.3400	Prec@5 88.4500	
Best Prec@1: [64.870]	
Starting epoch number: 19 Learning rate: 0.1
Train: [19]	Time 257.977	Data 0.548	Loss 0.891	Prec@1 73.3360	Prec@5 94.5440	
Val: [19]	Time 15.612	Data 0.132	Loss 1.334	Prec@1 63.7700	Prec@5 89.4600	
Best Prec@1: [64.870]	
Starting epoch number: 20 Learning rate: 0.1
Train: [20]	Time 258.053	Data 0.502	Loss 0.873	Prec@1 73.8020	Prec@5 94.8840	
Val: [20]	Time 15.596	Data 0.149	Loss 1.395	Prec@1 64.3600	Prec@5 89.2400	
Best Prec@1: [64.870]	
Starting epoch number: 21 Learning rate: 0.1
Train: [21]	Time 257.827	Data 0.506	Loss 0.874	Prec@1 73.8040	Prec@5 94.7480	
Val: [21]	Time 15.629	Data 0.176	Loss 1.447	Prec@1 62.3900	Prec@5 88.6500	
Best Prec@1: [64.870]	
Starting epoch number: 22 Learning rate: 0.1
Train: [22]	Time 258.257	Data 0.454	Loss 0.851	Prec@1 74.5880	Prec@5 95.1380	
Val: [22]	Time 15.713	Data 0.125	Loss 1.359	Prec@1 64.3300	Prec@5 89.4600	
Best Prec@1: [64.870]	
Starting epoch number: 23 Learning rate: 0.1
Train: [23]	Time 258.591	Data 0.446	Loss 0.839	Prec@1 74.5860	Prec@5 95.2000	
Val: [23]	Time 15.720	Data 0.152	Loss 1.440	Prec@1 63.8500	Prec@5 88.8900	
Best Prec@1: [64.870]	
Starting epoch number: 24 Learning rate: 0.1
Train: [24]	Time 258.142	Data 0.537	Loss 0.833	Prec@1 74.5900	Prec@5 95.3640	
Val: [24]	Time 15.596	Data 0.136	Loss 1.358	Prec@1 64.2300	Prec@5 89.0300	
Best Prec@1: [64.870]	
Starting epoch number: 25 Learning rate: 0.1
Train: [25]	Time 258.088	Data 0.494	Loss 0.817	Prec@1 75.4460	Prec@5 95.4620	
Val: [25]	Time 15.641	Data 0.140	Loss 1.500	Prec@1 62.6100	Prec@5 89.0200	
Best Prec@1: [64.870]	
Starting epoch number: 26 Learning rate: 0.1
Train: [26]	Time 257.594	Data 0.561	Loss 0.812	Prec@1 75.3120	Prec@5 95.5640	
Val: [26]	Time 15.627	Data 0.157	Loss 1.415	Prec@1 64.2500	Prec@5 89.3700	
Best Prec@1: [64.870]	
Starting epoch number: 27 Learning rate: 0.1
Train: [27]	Time 258.169	Data 0.512	Loss 0.803	Prec@1 75.8300	Prec@5 95.5400	
Val: [27]	Time 15.625	Data 0.147	Loss 1.503	Prec@1 62.5400	Prec@5 88.6400	
Best Prec@1: [64.870]	
Starting epoch number: 28 Learning rate: 0.1
Train: [28]	Time 258.394	Data 0.589	Loss 0.791	Prec@1 76.0780	Prec@5 95.6380	
Val: [28]	Time 15.748	Data 0.142	Loss 1.416	Prec@1 64.6600	Prec@5 88.9500	
Best Prec@1: [64.870]	
Starting epoch number: 29 Learning rate: 0.1
Train: [29]	Time 258.470	Data 0.529	Loss 0.779	Prec@1 76.5820	Prec@5 95.7860	
Val: [29]	Time 15.644	Data 0.175	Loss 1.345	Prec@1 65.2100	Prec@5 90.2500	
Best Prec@1: [65.210]	
Starting epoch number: 30 Learning rate: 0.1
Train: [30]	Time 257.956	Data 0.560	Loss 0.773	Prec@1 76.5640	Prec@5 95.9080	
Val: [30]	Time 15.652	Data 0.159	Loss 1.476	Prec@1 62.6100	Prec@5 88.5900	
Best Prec@1: [65.210]	
Starting epoch number: 31 Learning rate: 0.1
Train: [31]	Time 257.872	Data 0.491	Loss 0.770	Prec@1 76.3420	Prec@5 96.0220	
Val: [31]	Time 15.600	Data 0.183	Loss 1.343	Prec@1 64.9800	Prec@5 90.1100	
Best Prec@1: [65.210]	
Starting epoch number: 32 Learning rate: 0.1
Train: [32]	Time 257.714	Data 0.498	Loss 0.758	Prec@1 76.9280	Prec@5 96.1620	
Val: [32]	Time 15.647	Data 0.147	Loss 1.433	Prec@1 63.6100	Prec@5 88.8600	
Best Prec@1: [65.210]	
Starting epoch number: 33 Learning rate: 0.1
Train: [33]	Time 258.268	Data 0.513	Loss 0.751	Prec@1 77.1360	Prec@5 96.1600	
Val: [33]	Time 15.694	Data 0.187	Loss 1.391	Prec@1 64.0400	Prec@5 89.6100	
Best Prec@1: [65.210]	
Starting epoch number: 34 Learning rate: 0.1
Train: [34]	Time 258.565	Data 0.482	Loss 0.744	Prec@1 77.3100	Prec@5 96.3000	
Val: [34]	Time 15.694	Data 0.174	Loss 1.414	Prec@1 64.7400	Prec@5 89.3600	
Best Prec@1: [65.210]	
Starting epoch number: 35 Learning rate: 0.1
Train: [35]	Time 258.208	Data 0.662	Loss 0.732	Prec@1 77.6280	Prec@5 96.2400	
Val: [35]	Time 15.725	Data 0.247	Loss 1.341	Prec@1 64.9700	Prec@5 89.3000	
Best Prec@1: [65.210]	
Starting epoch number: 36 Learning rate: 0.1
Train: [36]	Time 258.527	Data 0.823	Loss 0.728	Prec@1 77.9520	Prec@5 96.3720	
Val: [36]	Time 15.599	Data 0.134	Loss 1.422	Prec@1 64.4600	Prec@5 89.3900	
Best Prec@1: [65.210]	
Starting epoch number: 37 Learning rate: 0.1
Train: [37]	Time 257.858	Data 0.647	Loss 0.719	Prec@1 78.0500	Prec@5 96.4700	
Val: [37]	Time 15.728	Data 0.278	Loss 1.367	Prec@1 65.6900	Prec@5 89.2500	
Best Prec@1: [65.690]	
Starting epoch number: 38 Learning rate: 0.1
Train: [38]	Time 258.729	Data 0.668	Loss 0.718	Prec@1 78.0800	Prec@5 96.4560	
Val: [38]	Time 15.669	Data 0.143	Loss 1.358	Prec@1 64.7500	Prec@5 89.6900	
Best Prec@1: [65.690]	
Starting epoch number: 39 Learning rate: 0.1
Train: [39]	Time 258.722	Data 0.675	Loss 0.716	Prec@1 78.0740	Prec@5 96.5100	
Val: [39]	Time 15.706	Data 0.187	Loss 1.349	Prec@1 65.4300	Prec@5 90.1500	
Best Prec@1: [65.690]	
Starting epoch number: 40 Learning rate: 0.1
Train: [40]	Time 258.470	Data 0.880	Loss 0.694	Prec@1 78.7940	Prec@5 96.7680	
Val: [40]	Time 15.762	Data 0.262	Loss 1.381	Prec@1 65.5400	Prec@5 90.3400	
Best Prec@1: [65.690]	
Starting epoch number: 41 Learning rate: 0.1
Train: [41]	Time 258.319	Data 0.707	Loss 0.703	Prec@1 78.4200	Prec@5 96.5840	
Val: [41]	Time 15.755	Data 0.142	Loss 1.511	Prec@1 63.6700	Prec@5 88.8400	
Best Prec@1: [65.690]	
Starting epoch number: 42 Learning rate: 0.1
Train: [42]	Time 258.362	Data 0.955	Loss 0.694	Prec@1 78.5600	Prec@5 96.7460	
Val: [42]	Time 15.690	Data 0.172	Loss 1.367	Prec@1 65.3800	Prec@5 89.7900	
Best Prec@1: [65.690]	
Starting epoch number: 43 Learning rate: 0.1
Train: [43]	Time 258.918	Data 1.179	Loss 0.691	Prec@1 78.6460	Prec@5 96.8300	
Val: [43]	Time 15.945	Data 0.353	Loss 1.347	Prec@1 66.3100	Prec@5 90.0500	
Best Prec@1: [66.310]	
Starting epoch number: 44 Learning rate: 0.1
Train: [44]	Time 259.011	Data 1.051	Loss 0.682	Prec@1 79.0200	Prec@5 96.7440	
Val: [44]	Time 15.677	Data 0.133	Loss 1.402	Prec@1 64.6800	Prec@5 89.9800	
Best Prec@1: [66.310]	
Starting epoch number: 45 Learning rate: 0.1
Train: [45]	Time 258.348	Data 1.098	Loss 0.681	Prec@1 79.0860	Prec@5 96.8140	
Val: [45]	Time 15.628	Data 0.184	Loss 1.497	Prec@1 64.2700	Prec@5 88.7100	
Best Prec@1: [66.310]	
Starting epoch number: 46 Learning rate: 0.1
Train: [46]	Time 259.342	Data 1.873	Loss 0.671	Prec@1 79.4820	Prec@5 96.8880	
Val: [46]	Time 15.816	Data 0.384	Loss 1.522	Prec@1 63.0400	Prec@5 88.3600	
Best Prec@1: [66.310]	
Starting epoch number: 47 Learning rate: 0.1
Train: [47]	Time 261.664	Data 4.752	Loss 0.672	Prec@1 79.1860	Prec@5 97.0200	
Val: [47]	Time 16.354	Data 0.805	Loss 1.542	Prec@1 63.8200	Prec@5 88.3900	
Best Prec@1: [66.310]	
Starting epoch number: 48 Learning rate: 0.1
Train: [48]	Time 262.088	Data 4.712	Loss 0.662	Prec@1 79.3940	Prec@5 97.0240	
Val: [48]	Time 16.126	Data 0.658	Loss 1.417	Prec@1 65.2700	Prec@5 89.5500	
Best Prec@1: [66.310]	
Starting epoch number: 49 Learning rate: 0.1
Train: [49]	Time 261.855	Data 4.506	Loss 0.667	Prec@1 79.4020	Prec@5 96.8980	
Val: [49]	Time 16.139	Data 0.649	Loss 1.410	Prec@1 64.9200	Prec@5 89.9200	
Best Prec@1: [66.310]	
Starting epoch number: 50 Learning rate: 0.1
Train: [50]	Time 260.580	Data 3.174	Loss 0.655	Prec@1 79.6660	Prec@5 97.1660	
Val: [50]	Time 15.826	Data 0.355	Loss 1.461	Prec@1 64.1300	Prec@5 89.1600	
Best Prec@1: [66.310]	
Starting epoch number: 51 Learning rate: 0.1
Train: [51]	Time 262.062	Data 4.832	Loss 0.648	Prec@1 79.9100	Prec@5 97.1180	
Val: [51]	Time 15.846	Data 0.518	Loss 1.342	Prec@1 66.8400	Prec@5 89.9300	
Best Prec@1: [66.840]	
Starting epoch number: 52 Learning rate: 0.1
Train: [52]	Time 260.135	Data 3.239	Loss 0.652	Prec@1 79.7220	Prec@5 97.1120	
Val: [52]	Time 15.832	Data 0.421	Loss 1.438	Prec@1 65.1700	Prec@5 89.6800	
Best Prec@1: [66.840]	
Starting epoch number: 53 Learning rate: 0.1
Train: [53]	Time 261.830	Data 4.590	Loss 0.640	Prec@1 80.1940	Prec@5 97.2420	
Val: [53]	Time 16.552	Data 1.059	Loss 1.478	Prec@1 64.4400	Prec@5 89.3600	
Best Prec@1: [66.840]	
Starting epoch number: 54 Learning rate: 0.1
Train: [54]	Time 262.662	Data 5.155	Loss 0.641	Prec@1 80.1660	Prec@5 97.1100	
Val: [54]	Time 16.137	Data 0.732	Loss 1.500	Prec@1 64.5400	Prec@5 88.9300	
Best Prec@1: [66.840]	
Starting epoch number: 55 Learning rate: 0.1
Train: [55]	Time 261.329	Data 4.492	Loss 0.636	Prec@1 80.1640	Prec@5 97.2120	
Val: [55]	Time 15.920	Data 0.501	Loss 1.502	Prec@1 64.7300	Prec@5 89.1600	
Best Prec@1: [66.840]	
Starting epoch number: 56 Learning rate: 0.1
Train: [56]	Time 261.040	Data 3.651	Loss 0.634	Prec@1 80.3740	Prec@5 97.2560	
Val: [56]	Time 16.024	Data 0.558	Loss 1.421	Prec@1 65.1400	Prec@5 89.8500	
Best Prec@1: [66.840]	
Starting epoch number: 57 Learning rate: 0.1
Train: [57]	Time 261.090	Data 4.103	Loss 0.628	Prec@1 80.4780	Prec@5 97.3380	
Val: [57]	Time 15.705	Data 0.273	Loss 1.430	Prec@1 65.2600	Prec@5 89.4100	
Best Prec@1: [66.840]	
Starting epoch number: 58 Learning rate: 0.1
Train: [58]	Time 261.237	Data 3.697	Loss 0.639	Prec@1 80.0940	Prec@5 97.2660	
Val: [58]	Time 16.359	Data 0.691	Loss 1.568	Prec@1 62.9700	Prec@5 88.7300	
Best Prec@1: [66.840]	
Starting epoch number: 59 Learning rate: 0.1
Train: [59]	Time 262.070	Data 4.411	Loss 0.624	Prec@1 80.5660	Prec@5 97.3860	
Val: [59]	Time 16.144	Data 0.501	Loss 1.342	Prec@1 66.6700	Prec@5 90.2900	
Best Prec@1: [66.840]	
Starting epoch number: 60 Learning rate: 0.1
Train: [60]	Time 260.621	Data 3.470	Loss 0.619	Prec@1 80.8120	Prec@5 97.3260	
Val: [60]	Time 16.068	Data 0.581	Loss 1.485	Prec@1 64.4000	Prec@5 89.3900	
Best Prec@1: [66.840]	
Starting epoch number: 61 Learning rate: 0.1
Train: [61]	Time 261.625	Data 4.310	Loss 0.626	Prec@1 80.2800	Prec@5 97.4120	
Val: [61]	Time 15.968	Data 0.688	Loss 1.537	Prec@1 63.7100	Prec@5 88.9800	
Best Prec@1: [66.840]	
Starting epoch number: 62 Learning rate: 0.1
Train: [62]	Time 261.815	Data 4.865	Loss 0.618	Prec@1 80.7700	Prec@5 97.4560	
Val: [62]	Time 16.545	Data 1.036	Loss 1.580	Prec@1 63.9100	Prec@5 89.5700	
Best Prec@1: [66.840]	
Starting epoch number: 63 Learning rate: 0.1
Train: [63]	Time 258.816	Data 1.298	Loss 0.620	Prec@1 80.8220	Prec@5 97.5060	
Val: [63]	Time 15.702	Data 0.145	Loss 1.431	Prec@1 66.0700	Prec@5 90.0400	
Best Prec@1: [66.840]	
Starting epoch number: 64 Learning rate: 0.1
Train: [64]	Time 257.977	Data 0.495	Loss 0.617	Prec@1 80.8240	Prec@5 97.4080	
Val: [64]	Time 15.692	Data 0.128	Loss 1.458	Prec@1 65.6600	Prec@5 89.8100	
Best Prec@1: [66.840]	
Starting epoch number: 65 Learning rate: 0.1
Train: [65]	Time 257.791	Data 0.545	Loss 0.619	Prec@1 80.8060	Prec@5 97.5640	
Val: [65]	Time 15.587	Data 0.153	Loss 1.410	Prec@1 66.5700	Prec@5 89.3000	
Best Prec@1: [66.840]	
Starting epoch number: 66 Learning rate: 0.1
Train: [66]	Time 257.569	Data 0.503	Loss 0.614	Prec@1 80.7160	Prec@5 97.4800	
Val: [66]	Time 15.623	Data 0.131	Loss 1.570	Prec@1 64.0200	Prec@5 88.7100	
Best Prec@1: [66.840]	
Starting epoch number: 67 Learning rate: 0.1
Train: [67]	Time 257.304	Data 0.496	Loss 0.615	Prec@1 80.7400	Prec@5 97.3900	
Val: [67]	Time 15.588	Data 0.139	Loss 1.662	Prec@1 62.6700	Prec@5 88.3300	
Best Prec@1: [66.840]	
Starting epoch number: 68 Learning rate: 0.1
Train: [68]	Time 257.340	Data 0.539	Loss 0.603	Prec@1 81.2060	Prec@5 97.6000	
Val: [68]	Time 15.787	Data 0.326	Loss 1.437	Prec@1 65.0400	Prec@5 89.2500	
Best Prec@1: [66.840]	
Starting epoch number: 69 Learning rate: 0.1
Train: [69]	Time 257.960	Data 0.919	Loss 0.601	Prec@1 81.0540	Prec@5 97.5420	
Val: [69]	Time 15.752	Data 0.266	Loss 1.597	Prec@1 62.8400	Prec@5 88.5200	
Best Prec@1: [66.840]	
Starting epoch number: 70 Learning rate: 0.1
Train: [70]	Time 258.473	Data 1.088	Loss 0.599	Prec@1 81.2380	Prec@5 97.6860	
Val: [70]	Time 15.753	Data 0.359	Loss 1.433	Prec@1 65.4300	Prec@5 89.6900	
Best Prec@1: [66.840]	
Starting epoch number: 71 Learning rate: 0.1
Train: [71]	Time 258.202	Data 0.842	Loss 0.595	Prec@1 81.3760	Prec@5 97.6260	
Val: [71]	Time 15.728	Data 0.228	Loss 1.481	Prec@1 64.9600	Prec@5 89.0700	
Best Prec@1: [66.840]	
Starting epoch number: 72 Learning rate: 0.1
Train: [72]	Time 257.979	Data 0.965	Loss 0.599	Prec@1 81.3640	Prec@5 97.5320	
Val: [72]	Time 15.585	Data 0.133	Loss 1.424	Prec@1 65.3400	Prec@5 89.5600	
Best Prec@1: [66.840]	
Starting epoch number: 73 Learning rate: 0.1
Train: [73]	Time 258.088	Data 1.026	Loss 0.591	Prec@1 81.5340	Prec@5 97.7060	
Val: [73]	Time 15.669	Data 0.146	Loss 1.390	Prec@1 66.3000	Prec@5 90.2400	
Best Prec@1: [66.840]	
Starting epoch number: 74 Learning rate: 0.1
Train: [74]	Time 258.661	Data 1.029	Loss 0.594	Prec@1 81.4200	Prec@5 97.6860	
Val: [74]	Time 15.774	Data 0.225	Loss 1.383	Prec@1 67.0400	Prec@5 90.2600	
Best Prec@1: [67.040]	
Starting epoch number: 75 Learning rate: 0.1
Train: [75]	Time 258.154	Data 0.664	Loss 0.587	Prec@1 81.5900	Prec@5 97.6620	
Val: [75]	Time 15.729	Data 0.191	Loss 1.408	Prec@1 65.2100	Prec@5 89.5500	
Best Prec@1: [67.040]	
Starting epoch number: 76 Learning rate: 0.1
Train: [76]	Time 257.692	Data 0.722	Loss 0.582	Prec@1 81.7800	Prec@5 97.7460	
Val: [76]	Time 15.650	Data 0.162	Loss 1.393	Prec@1 66.5200	Prec@5 90.2300	
Best Prec@1: [67.040]	
Starting epoch number: 77 Learning rate: 0.1
Train: [77]	Time 257.983	Data 0.750	Loss 0.578	Prec@1 81.8380	Prec@5 97.8220	
Val: [77]	Time 15.637	Data 0.172	Loss 1.377	Prec@1 66.1100	Prec@5 90.0900	
Best Prec@1: [67.040]	
Starting epoch number: 78 Learning rate: 0.1
Train: [78]	Time 257.670	Data 0.784	Loss 0.583	Prec@1 81.8760	Prec@5 97.6800	
Val: [78]	Time 15.666	Data 0.193	Loss 1.444	Prec@1 66.2200	Prec@5 90.1400	
Best Prec@1: [67.040]	
Starting epoch number: 79 Learning rate: 0.1
Train: [79]	Time 258.232	Data 0.949	Loss 0.573	Prec@1 81.9840	Prec@5 97.7680	
Val: [79]	Time 15.761	Data 0.189	Loss 1.541	Prec@1 64.7400	Prec@5 88.9200	
Best Prec@1: [67.040]	
Starting epoch number: 80 Learning rate: 0.1
Train: [80]	Time 258.570	Data 0.931	Loss 0.589	Prec@1 81.5520	Prec@5 97.6780	
Val: [80]	Time 15.675	Data 0.152	Loss 1.562	Prec@1 64.3300	Prec@5 88.8500	
Best Prec@1: [67.040]	
Starting epoch number: 81 Learning rate: 0.1
Train: [81]	Time 258.834	Data 1.005	Loss 0.582	Prec@1 81.9980	Prec@5 97.7780	
Val: [81]	Time 15.691	Data 0.221	Loss 1.423	Prec@1 66.1200	Prec@5 89.9100	
Best Prec@1: [67.040]	
Starting epoch number: 82 Learning rate: 0.1
Train: [82]	Time 258.683	Data 1.459	Loss 0.570	Prec@1 82.1180	Prec@5 97.8120	
Val: [82]	Time 15.741	Data 0.267	Loss 1.455	Prec@1 64.9100	Prec@5 89.6800	
Best Prec@1: [67.040]	
Starting epoch number: 83 Learning rate: 0.1
Train: [83]	Time 258.026	Data 0.965	Loss 0.574	Prec@1 81.8480	Prec@5 97.8540	
Val: [83]	Time 15.611	Data 0.168	Loss 1.481	Prec@1 64.8300	Prec@5 89.1600	
Best Prec@1: [67.040]	
Starting epoch number: 84 Learning rate: 0.1
Train: [84]	Time 257.625	Data 0.844	Loss 0.561	Prec@1 82.4100	Prec@5 97.9560	
Val: [84]	Time 15.579	Data 0.151	Loss 1.363	Prec@1 66.5100	Prec@5 90.2500	
Best Prec@1: [67.040]	
Starting epoch number: 85 Learning rate: 0.1
Train: [85]	Time 258.007	Data 0.904	Loss 0.563	Prec@1 82.3980	Prec@5 97.9120	
Val: [85]	Time 15.680	Data 0.175	Loss 1.593	Prec@1 64.0700	Prec@5 89.2300	
Best Prec@1: [67.040]	
Starting epoch number: 86 Learning rate: 0.1
Train: [86]	Time 258.142	Data 0.647	Loss 0.569	Prec@1 81.9720	Prec@5 97.8100	
Val: [86]	Time 15.690	Data 0.137	Loss 1.491	Prec@1 64.8200	Prec@5 89.0600	
Best Prec@1: [67.040]	
Starting epoch number: 87 Learning rate: 0.1
Train: [87]	Time 257.880	Data 0.443	Loss 0.562	Prec@1 82.4380	Prec@5 97.8300	
Val: [87]	Time 15.652	Data 0.140	Loss 1.554	Prec@1 65.2600	Prec@5 88.9400	
Best Prec@1: [67.040]	
Starting epoch number: 88 Learning rate: 0.1
Train: [88]	Time 257.342	Data 0.543	Loss 0.564	Prec@1 82.2040	Prec@5 97.9340	
Val: [88]	Time 15.672	Data 0.187	Loss 1.437	Prec@1 65.6700	Prec@5 89.7600	
Best Prec@1: [67.040]	
Starting epoch number: 89 Learning rate: 0.1
Train: [89]	Time 257.324	Data 0.477	Loss 0.563	Prec@1 82.2840	Prec@5 97.9400	
Val: [89]	Time 15.581	Data 0.143	Loss 1.435	Prec@1 66.7800	Prec@5 90.0400	
Best Prec@1: [67.040]	
Starting epoch number: 90 Learning rate: 0.1
Train: [90]	Time 256.888	Data 0.427	Loss 0.552	Prec@1 82.6380	Prec@5 97.9740	
Val: [90]	Time 15.622	Data 0.151	Loss 1.409	Prec@1 66.7700	Prec@5 90.3000	
Best Prec@1: [67.040]	
Starting epoch number: 91 Learning rate: 0.1
Train: [91]	Time 257.506	Data 0.566	Loss 0.551	Prec@1 82.6600	Prec@5 98.0340	
Val: [91]	Time 15.736	Data 0.161	Loss 1.457	Prec@1 66.1400	Prec@5 89.6900	
Best Prec@1: [67.040]	
Starting epoch number: 92 Learning rate: 0.1
Train: [92]	Time 257.800	Data 0.534	Loss 0.559	Prec@1 82.3100	Prec@5 97.9440	
Val: [92]	Time 15.672	Data 0.170	Loss 1.394	Prec@1 66.9000	Prec@5 90.3900	
Best Prec@1: [67.040]	
Starting epoch number: 93 Learning rate: 0.1
Train: [93]	Time 257.827	Data 0.571	Loss 0.552	Prec@1 82.4900	Prec@5 98.0660	
Val: [93]	Time 15.702	Data 0.154	Loss 1.506	Prec@1 65.8000	Prec@5 89.8300	
Best Prec@1: [67.040]	
Starting epoch number: 94 Learning rate: 0.1
Train: [94]	Time 257.252	Data 0.549	Loss 0.557	Prec@1 82.4380	Prec@5 98.0240	
Val: [94]	Time 15.624	Data 0.150	Loss 1.418	Prec@1 66.8100	Prec@5 89.6200	
Best Prec@1: [67.040]	
Starting epoch number: 95 Learning rate: 0.1
Train: [95]	Time 257.046	Data 0.481	Loss 0.550	Prec@1 82.6500	Prec@5 97.9880	
Val: [95]	Time 15.572	Data 0.139	Loss 1.411	Prec@1 66.6800	Prec@5 90.2900	
Best Prec@1: [67.040]	
Starting epoch number: 96 Learning rate: 0.1
Train: [96]	Time 256.940	Data 0.489	Loss 0.551	Prec@1 82.8800	Prec@5 97.9860	
Val: [96]	Time 15.589	Data 0.133	Loss 1.403	Prec@1 66.1200	Prec@5 90.0900	
Best Prec@1: [67.040]	
Starting epoch number: 97 Learning rate: 0.1
Train: [97]	Time 257.202	Data 0.487	Loss 0.548	Prec@1 82.7440	Prec@5 97.9900	
Val: [97]	Time 15.708	Data 0.135	Loss 1.405	Prec@1 66.2300	Prec@5 89.5200	
Best Prec@1: [67.040]	
Starting epoch number: 98 Learning rate: 0.1
Train: [98]	Time 257.786	Data 0.530	Loss 0.551	Prec@1 82.6840	Prec@5 97.9480	
Val: [98]	Time 15.665	Data 0.162	Loss 1.507	Prec@1 65.9100	Prec@5 88.8100	
Best Prec@1: [67.040]	
Starting epoch number: 99 Learning rate: 0.1
Train: [99]	Time 257.855	Data 0.589	Loss 0.548	Prec@1 82.8800	Prec@5 97.9860	
Val: [99]	Time 15.710	Data 0.149	Loss 1.490	Prec@1 65.7000	Prec@5 89.3700	
Best Prec@1: [67.040]	
Starting epoch number: 100 Learning rate: 0.1
Train: [100]	Time 258.624	Data 0.564	Loss 0.545	Prec@1 82.9340	Prec@5 97.9680	
Val: [100]	Time 15.758	Data 0.135	Loss 1.401	Prec@1 66.3700	Prec@5 89.6800	
Best Prec@1: [67.040]	
Starting epoch number: 101 Learning rate: 0.1
Train: [101]	Time 258.181	Data 0.499	Loss 0.543	Prec@1 82.7180	Prec@5 97.9860	
Val: [101]	Time 15.707	Data 0.145	Loss 1.439	Prec@1 65.2900	Prec@5 89.8200	
Best Prec@1: [67.040]	
Starting epoch number: 102 Learning rate: 0.1
Train: [102]	Time 258.899	Data 0.592	Loss 0.539	Prec@1 82.9140	Prec@5 98.1080	
Val: [102]	Time 15.718	Data 0.134	Loss 1.480	Prec@1 64.7500	Prec@5 89.3400	
Best Prec@1: [67.040]	
Starting epoch number: 103 Learning rate: 0.1
Train: [103]	Time 258.170	Data 0.523	Loss 0.538	Prec@1 82.9420	Prec@5 98.0580	
Val: [103]	Time 15.694	Data 0.170	Loss 1.451	Prec@1 65.9100	Prec@5 90.1700	
Best Prec@1: [67.040]	
Starting epoch number: 104 Learning rate: 0.1
Train: [104]	Time 258.491	Data 0.459	Loss 0.539	Prec@1 83.1740	Prec@5 98.0260	
Val: [104]	Time 15.754	Data 0.140	Loss 1.385	Prec@1 66.5300	Prec@5 90.3500	
Best Prec@1: [67.040]	
Starting epoch number: 105 Learning rate: 0.1
Train: [105]	Time 258.814	Data 0.526	Loss 0.539	Prec@1 82.9260	Prec@5 98.0720	
Val: [105]	Time 15.787	Data 0.144	Loss 1.440	Prec@1 66.1600	Prec@5 90.1900	
Best Prec@1: [67.040]	
Starting epoch number: 106 Learning rate: 0.1
Train: [106]	Time 258.974	Data 0.538	Loss 0.536	Prec@1 82.9560	Prec@5 98.0860	
Val: [106]	Time 15.800	Data 0.127	Loss 1.465	Prec@1 66.1300	Prec@5 89.7800	
Best Prec@1: [67.040]	
Starting epoch number: 107 Learning rate: 0.1
Train: [107]	Time 259.270	Data 0.508	Loss 0.536	Prec@1 83.0640	Prec@5 98.1400	
Val: [107]	Time 15.818	Data 0.149	Loss 1.412	Prec@1 67.1100	Prec@5 90.2900	
Best Prec@1: [67.110]	
Starting epoch number: 108 Learning rate: 0.1
Train: [108]	Time 259.179	Data 0.498	Loss 0.535	Prec@1 83.0340	Prec@5 98.1300	
Val: [108]	Time 15.803	Data 0.173	Loss 1.389	Prec@1 67.5600	Prec@5 90.0500	
Best Prec@1: [67.560]	
Starting epoch number: 109 Learning rate: 0.1
Train: [109]	Time 258.827	Data 0.507	Loss 0.536	Prec@1 83.2660	Prec@5 97.9340	
Val: [109]	Time 15.692	Data 0.133	Loss 1.473	Prec@1 66.1300	Prec@5 90.2000	
Best Prec@1: [67.560]	
Starting epoch number: 110 Learning rate: 0.1
Train: [110]	Time 258.051	Data 0.569	Loss 0.540	Prec@1 83.0880	Prec@5 98.0740	
Val: [110]	Time 15.709	Data 0.166	Loss 1.459	Prec@1 66.3000	Prec@5 89.9500	
Best Prec@1: [67.560]	
Starting epoch number: 111 Learning rate: 0.1
Train: [111]	Time 258.240	Data 0.434	Loss 0.531	Prec@1 83.2140	Prec@5 98.0160	
Val: [111]	Time 15.815	Data 0.140	Loss 1.438	Prec@1 65.7300	Prec@5 89.4600	
Best Prec@1: [67.560]	
Starting epoch number: 112 Learning rate: 0.1
Train: [112]	Time 258.720	Data 0.531	Loss 0.536	Prec@1 83.1440	Prec@5 98.1520	
Val: [112]	Time 15.834	Data 0.186	Loss 1.592	Prec@1 63.4600	Prec@5 88.2100	
Best Prec@1: [67.560]	
Starting epoch number: 113 Learning rate: 0.1
Train: [113]	Time 258.666	Data 0.503	Loss 0.529	Prec@1 83.0080	Prec@5 98.2440	
Val: [113]	Time 16.023	Data 0.137	Loss 1.379	Prec@1 66.9400	Prec@5 89.4200	
Best Prec@1: [67.560]	
Starting epoch number: 114 Learning rate: 0.1
Train: [114]	Time 258.765	Data 0.537	Loss 0.532	Prec@1 83.3360	Prec@5 98.0960	
Val: [114]	Time 15.748	Data 0.156	Loss 1.416	Prec@1 65.8600	Prec@5 90.0000	
Best Prec@1: [67.560]	
Starting epoch number: 115 Learning rate: 0.1
Train: [115]	Time 258.494	Data 0.537	Loss 0.526	Prec@1 83.2260	Prec@5 98.2540	
Val: [115]	Time 15.743	Data 0.173	Loss 1.464	Prec@1 66.0400	Prec@5 89.6600	
Best Prec@1: [67.560]	
Starting epoch number: 116 Learning rate: 0.1
Train: [116]	Time 258.053	Data 0.514	Loss 0.522	Prec@1 83.6500	Prec@5 98.1400	
Val: [116]	Time 15.706	Data 0.153	Loss 1.615	Prec@1 64.2900	Prec@5 88.4000	
Best Prec@1: [67.560]	
Starting epoch number: 117 Learning rate: 0.1
Train: [117]	Time 258.163	Data 0.426	Loss 0.521	Prec@1 83.6180	Prec@5 98.0800	
Val: [117]	Time 15.752	Data 0.144	Loss 1.436	Prec@1 65.8200	Prec@5 89.3100	
Best Prec@1: [67.560]	
Starting epoch number: 118 Learning rate: 0.1
Train: [118]	Time 258.471	Data 0.483	Loss 0.534	Prec@1 83.2340	Prec@5 98.1260	
Val: [118]	Time 15.801	Data 0.150	Loss 1.517	Prec@1 65.1800	Prec@5 89.3500	
Best Prec@1: [67.560]	
Starting epoch number: 119 Learning rate: 0.1
Train: [119]	Time 258.807	Data 0.490	Loss 0.526	Prec@1 83.4120	Prec@5 98.1380	
Val: [119]	Time 15.753	Data 0.140	Loss 1.466	Prec@1 65.7000	Prec@5 89.4300	
Best Prec@1: [67.560]	
Starting epoch number: 120 Learning rate: 0.1
Train: [120]	Time 258.725	Data 0.407	Loss 0.533	Prec@1 83.0100	Prec@5 98.1320	
Val: [120]	Time 15.790	Data 0.162	Loss 1.438	Prec@1 66.5700	Prec@5 89.8800	
Best Prec@1: [67.560]	
Starting epoch number: 121 Learning rate: 0.1
Train: [121]	Time 258.463	Data 0.486	Loss 0.528	Prec@1 83.4440	Prec@5 98.1340	
Val: [121]	Time 15.736	Data 0.135	Loss 1.499	Prec@1 65.9500	Prec@5 89.4300	
Best Prec@1: [67.560]	
Starting epoch number: 122 Learning rate: 0.1
Train: [122]	Time 257.961	Data 0.513	Loss 0.528	Prec@1 83.2900	Prec@5 98.1880	
Val: [122]	Time 15.711	Data 0.147	Loss 1.379	Prec@1 67.5100	Prec@5 90.6000	
Best Prec@1: [67.560]	
Starting epoch number: 123 Learning rate: 0.1
Train: [123]	Time 258.354	Data 0.484	Loss 0.517	Prec@1 83.7440	Prec@5 98.1980	
Val: [123]	Time 15.840	Data 0.154	Loss 1.650	Prec@1 64.6100	Prec@5 88.3700	
Best Prec@1: [67.560]	
Starting epoch number: 124 Learning rate: 0.1
Train: [124]	Time 258.643	Data 0.630	Loss 0.527	Prec@1 83.3380	Prec@5 98.2160	
Val: [124]	Time 15.842	Data 0.150	Loss 1.465	Prec@1 66.2600	Prec@5 90.2300	
Best Prec@1: [67.560]	
Starting epoch number: 125 Learning rate: 0.1
Train: [125]	Time 258.526	Data 0.493	Loss 0.519	Prec@1 83.6200	Prec@5 98.1940	
Val: [125]	Time 15.767	Data 0.158	Loss 1.583	Prec@1 65.5500	Prec@5 89.5100	
Best Prec@1: [67.560]	
Starting epoch number: 126 Learning rate: 0.1
Train: [126]	Time 258.745	Data 0.432	Loss 0.520	Prec@1 83.6300	Prec@5 98.2460	
Val: [126]	Time 15.716	Data 0.148	Loss 1.506	Prec@1 66.1800	Prec@5 89.2400	
Best Prec@1: [67.560]	
Starting epoch number: 127 Learning rate: 0.1
Train: [127]	Time 258.424	Data 0.432	Loss 0.523	Prec@1 83.6160	Prec@5 98.1800	
Val: [127]	Time 15.711	Data 0.125	Loss 1.432	Prec@1 66.5100	Prec@5 89.6100	
Best Prec@1: [67.560]	
Starting epoch number: 128 Learning rate: 0.1
Train: [128]	Time 257.780	Data 0.400	Loss 0.520	Prec@1 83.7800	Prec@5 98.1840	
Val: [128]	Time 15.693	Data 0.131	Loss 1.507	Prec@1 65.7200	Prec@5 89.6200	
Best Prec@1: [67.560]	
Starting epoch number: 129 Learning rate: 0.1
Train: [129]	Time 258.302	Data 0.537	Loss 0.514	Prec@1 83.6860	Prec@5 98.2060	
Val: [129]	Time 15.708	Data 0.126	Loss 1.445	Prec@1 66.3900	Prec@5 89.6600	
Best Prec@1: [67.560]	
Starting epoch number: 130 Learning rate: 0.1
Train: [130]	Time 258.345	Data 0.469	Loss 0.512	Prec@1 83.6880	Prec@5 98.3100	
Val: [130]	Time 15.752	Data 0.162	Loss 1.345	Prec@1 67.5800	Prec@5 90.7200	
Best Prec@1: [67.580]	
Starting epoch number: 131 Learning rate: 0.1
Train: [131]	Time 258.312	Data 0.504	Loss 0.523	Prec@1 83.5760	Prec@5 98.1400	
Val: [131]	Time 15.890	Data 0.154	Loss 1.443	Prec@1 66.8000	Prec@5 89.9500	
Best Prec@1: [67.580]	
Starting epoch number: 132 Learning rate: 0.1
Train: [132]	Time 258.643	Data 0.514	Loss 0.516	Prec@1 83.5200	Prec@5 98.3000	
Val: [132]	Time 15.742	Data 0.134	Loss 1.373	Prec@1 66.6000	Prec@5 90.5300	
Best Prec@1: [67.580]	
Starting epoch number: 133 Learning rate: 0.1
Train: [133]	Time 258.557	Data 0.603	Loss 0.523	Prec@1 83.5740	Prec@5 98.1560	
Val: [133]	Time 15.842	Data 0.170	Loss 1.462	Prec@1 65.8500	Prec@5 89.4500	
Best Prec@1: [67.580]	
Starting epoch number: 134 Learning rate: 0.1
Train: [134]	Time 258.664	Data 0.511	Loss 0.515	Prec@1 83.6880	Prec@5 98.2220	
Val: [134]	Time 15.787	Data 0.161	Loss 1.452	Prec@1 66.2100	Prec@5 89.6100	
Best Prec@1: [67.580]	
Starting epoch number: 135 Learning rate: 0.1
Train: [135]	Time 257.900	Data 0.525	Loss 0.510	Prec@1 83.8760	Prec@5 98.3200	
Val: [135]	Time 15.693	Data 0.163	Loss 1.549	Prec@1 64.5800	Prec@5 88.4800	
Best Prec@1: [67.580]	
Starting epoch number: 136 Learning rate: 0.1
Train: [136]	Time 257.989	Data 0.477	Loss 0.518	Prec@1 83.5500	Prec@5 98.2680	
Val: [136]	Time 15.719	Data 0.154	Loss 1.445	Prec@1 65.5800	Prec@5 89.6300	
Best Prec@1: [67.580]	
Starting epoch number: 137 Learning rate: 0.1
Train: [137]	Time 258.334	Data 0.489	Loss 0.518	Prec@1 83.4980	Prec@5 98.1600	
Val: [137]	Time 15.750	Data 0.187	Loss 1.441	Prec@1 65.3800	Prec@5 89.6000	
Best Prec@1: [67.580]	
Starting epoch number: 138 Learning rate: 0.1
Train: [138]	Time 258.291	Data 0.452	Loss 0.514	Prec@1 83.5440	Prec@5 98.3220	
Val: [138]	Time 15.785	Data 0.151	Loss 1.569	Prec@1 64.9100	Prec@5 88.5600	
Best Prec@1: [67.580]	
Starting epoch number: 139 Learning rate: 0.1
Train: [139]	Time 258.442	Data 0.475	Loss 0.506	Prec@1 84.0680	Prec@5 98.3060	
Val: [139]	Time 15.782	Data 0.213	Loss 1.469	Prec@1 66.1400	Prec@5 89.5600	
Best Prec@1: [67.580]	
Starting epoch number: 140 Learning rate: 0.1
Train: [140]	Time 258.547	Data 0.478	Loss 0.505	Prec@1 84.0480	Prec@5 98.2400	
Val: [140]	Time 15.828	Data 0.185	Loss 1.584	Prec@1 64.6600	Prec@5 88.4800	
Best Prec@1: [67.580]	
Starting epoch number: 141 Learning rate: 0.1
Train: [141]	Time 258.700	Data 0.455	Loss 0.517	Prec@1 83.7000	Prec@5 98.2720	
Val: [141]	Time 15.768	Data 0.141	Loss 1.417	Prec@1 66.9200	Prec@5 89.8500	
Best Prec@1: [67.580]	
Starting epoch number: 142 Learning rate: 0.1
Train: [142]	Time 258.501	Data 0.462	Loss 0.511	Prec@1 83.8460	Prec@5 98.3000	
Val: [142]	Time 15.779	Data 0.144	Loss 1.550	Prec@1 65.5200	Prec@5 89.3800	
Best Prec@1: [67.580]	
Starting epoch number: 143 Learning rate: 0.1
Train: [143]	Time 257.522	Data 0.425	Loss 0.509	Prec@1 84.0000	Prec@5 98.2880	
Val: [143]	Time 15.697	Data 0.174	Loss 1.397	Prec@1 66.3200	Prec@5 90.3500	
Best Prec@1: [67.580]	
Starting epoch number: 144 Learning rate: 0.1
Train: [144]	Time 257.556	Data 0.467	Loss 0.509	Prec@1 84.0340	Prec@5 98.1700	
Val: [144]	Time 15.697	Data 0.148	Loss 1.455	Prec@1 66.5500	Prec@5 90.0100	
Best Prec@1: [67.580]	
Starting epoch number: 145 Learning rate: 0.1
Train: [145]	Time 258.185	Data 0.458	Loss 0.504	Prec@1 84.2800	Prec@5 98.3240	
Val: [145]	Time 15.765	Data 0.181	Loss 1.495	Prec@1 65.8600	Prec@5 89.8200	
Best Prec@1: [67.580]	
Starting epoch number: 146 Learning rate: 0.1
Train: [146]	Time 258.436	Data 0.546	Loss 0.517	Prec@1 83.5860	Prec@5 98.3200	
Val: [146]	Time 15.790	Data 0.175	Loss 1.593	Prec@1 64.1800	Prec@5 89.2900	
Best Prec@1: [67.580]	
Starting epoch number: 147 Learning rate: 0.1
Train: [147]	Time 258.558	Data 0.464	Loss 0.516	Prec@1 83.7060	Prec@5 98.2440	
Val: [147]	Time 15.812	Data 0.173	Loss 1.431	Prec@1 67.1200	Prec@5 90.1900	
Best Prec@1: [67.580]	
Starting epoch number: 148 Learning rate: 0.1
Train: [148]	Time 258.562	Data 0.505	Loss 0.503	Prec@1 84.1220	Prec@5 98.3120	
Val: [148]	Time 15.752	Data 0.177	Loss 1.578	Prec@1 64.7400	Prec@5 89.1300	
Best Prec@1: [67.580]	
Starting epoch number: 149 Learning rate: 0.1
Train: [149]	Time 257.666	Data 0.458	Loss 0.503	Prec@1 84.1680	Prec@5 98.2620	
Val: [149]	Time 15.697	Data 0.161	Loss 1.454	Prec@1 66.0300	Prec@5 89.8400	
Best Prec@1: [67.580]	
Starting epoch number: 150 Learning rate: 0.010000000000000002
Train: [150]	Time 257.756	Data 0.460	Loss 0.234	Prec@1 93.1520	Prec@5 99.6100	
Val: [150]	Time 15.754	Data 0.145	Loss 1.000	Prec@1 74.7000	Prec@5 93.8300	
Best Prec@1: [74.700]	
Starting epoch number: 151 Learning rate: 0.010000000000000002
Train: [151]	Time 258.048	Data 0.438	Loss 0.144	Prec@1 96.2840	Prec@5 99.8820	
Val: [151]	Time 15.711	Data 0.134	Loss 0.990	Prec@1 75.8000	Prec@5 94.0000	
Best Prec@1: [75.800]	
Starting epoch number: 152 Learning rate: 0.010000000000000002
Train: [152]	Time 258.441	Data 0.442	Loss 0.114	Prec@1 97.2440	Prec@5 99.9360	
Val: [152]	Time 15.726	Data 0.137	Loss 1.002	Prec@1 75.7100	Prec@5 93.9100	
Best Prec@1: [75.800]	
Starting epoch number: 153 Learning rate: 0.010000000000000002
Train: [153]	Time 258.452	Data 0.447	Loss 0.095	Prec@1 97.9560	Prec@5 99.9620	
Val: [153]	Time 15.694	Data 0.141	Loss 0.990	Prec@1 76.3100	Prec@5 94.2400	
Best Prec@1: [76.310]	
Starting epoch number: 154 Learning rate: 0.010000000000000002
Train: [154]	Time 257.615	Data 0.429	Loss 0.085	Prec@1 98.2320	Prec@5 99.9780	
Val: [154]	Time 15.705	Data 0.146	Loss 1.002	Prec@1 75.9600	Prec@5 94.0000	
Best Prec@1: [76.310]	
Starting epoch number: 155 Learning rate: 0.010000000000000002
Train: [155]	Time 257.828	Data 0.451	Loss 0.074	Prec@1 98.5980	Prec@5 99.9720	
Val: [155]	Time 15.785	Data 0.141	Loss 1.017	Prec@1 76.0100	Prec@5 93.9700	
Best Prec@1: [76.310]	
Starting epoch number: 156 Learning rate: 0.010000000000000002
Train: [156]	Time 258.162	Data 0.455	Loss 0.068	Prec@1 98.8020	Prec@5 99.9880	
Val: [156]	Time 15.820	Data 0.171	Loss 1.016	Prec@1 76.2200	Prec@5 94.0100	
Best Prec@1: [76.310]	
Starting epoch number: 157 Learning rate: 0.010000000000000002
Train: [157]	Time 258.270	Data 0.461	Loss 0.063	Prec@1 98.8800	Prec@5 99.9920	
Val: [157]	Time 15.750	Data 0.139	Loss 1.025	Prec@1 76.3100	Prec@5 93.9400	
Best Prec@1: [76.310]	
Starting epoch number: 158 Learning rate: 0.010000000000000002
Train: [158]	Time 258.011	Data 0.462	Loss 0.056	Prec@1 99.0920	Prec@5 99.9940	
Val: [158]	Time 15.819	Data 0.169	Loss 1.031	Prec@1 76.2400	Prec@5 93.9000	
Best Prec@1: [76.310]	
Starting epoch number: 159 Learning rate: 0.010000000000000002
Train: [159]	Time 257.472	Data 0.448	Loss 0.053	Prec@1 99.1260	Prec@5 99.9980	
Val: [159]	Time 15.698	Data 0.135	Loss 1.028	Prec@1 76.1700	Prec@5 93.9500	
Best Prec@1: [76.310]	
Starting epoch number: 160 Learning rate: 0.010000000000000002
Train: [160]	Time 258.106	Data 0.479	Loss 0.049	Prec@1 99.2360	Prec@5 99.9980	
Val: [160]	Time 15.754	Data 0.156	Loss 1.030	Prec@1 76.1800	Prec@5 93.8900	
Best Prec@1: [76.310]	
Starting epoch number: 161 Learning rate: 0.010000000000000002
Train: [161]	Time 258.269	Data 0.444	Loss 0.046	Prec@1 99.3300	Prec@5 99.9920	
Val: [161]	Time 15.764	Data 0.138	Loss 1.030	Prec@1 76.2700	Prec@5 94.0400	
Best Prec@1: [76.310]	
Starting epoch number: 162 Learning rate: 0.010000000000000002
Train: [162]	Time 258.179	Data 0.449	Loss 0.044	Prec@1 99.3980	Prec@5 99.9940	
Val: [162]	Time 15.718	Data 0.144	Loss 1.028	Prec@1 76.4200	Prec@5 94.0400	
Best Prec@1: [76.420]	
Starting epoch number: 163 Learning rate: 0.010000000000000002
Train: [163]	Time 257.463	Data 0.465	Loss 0.042	Prec@1 99.4580	Prec@5 99.9980	
Val: [163]	Time 15.766	Data 0.166	Loss 1.026	Prec@1 76.3200	Prec@5 94.1300	
Best Prec@1: [76.420]	
Starting epoch number: 164 Learning rate: 0.010000000000000002
Train: [164]	Time 257.640	Data 0.489	Loss 0.041	Prec@1 99.4620	Prec@5 99.9980	
Val: [164]	Time 15.768	Data 0.177	Loss 1.050	Prec@1 76.4200	Prec@5 94.0500	
Best Prec@1: [76.420]	
Starting epoch number: 165 Learning rate: 0.010000000000000002
Train: [165]	Time 257.992	Data 0.436	Loss 0.038	Prec@1 99.5420	Prec@5 99.9960	
Val: [165]	Time 15.759	Data 0.169	Loss 1.037	Prec@1 76.2000	Prec@5 94.1500	
Best Prec@1: [76.420]	
Starting epoch number: 166 Learning rate: 0.010000000000000002
Train: [166]	Time 258.248	Data 0.461	Loss 0.038	Prec@1 99.5000	Prec@5 100.0000	
Val: [166]	Time 15.764	Data 0.139	Loss 1.036	Prec@1 76.4200	Prec@5 94.2100	
Best Prec@1: [76.420]	
Starting epoch number: 167 Learning rate: 0.010000000000000002
Train: [167]	Time 258.110	Data 0.451	Loss 0.035	Prec@1 99.6040	Prec@5 99.9980	
Val: [167]	Time 15.729	Data 0.140	Loss 1.025	Prec@1 76.3500	Prec@5 94.1900	
Best Prec@1: [76.420]	
Starting epoch number: 168 Learning rate: 0.010000000000000002
Train: [168]	Time 257.403	Data 0.494	Loss 0.034	Prec@1 99.6440	Prec@5 100.0000	
Val: [168]	Time 15.804	Data 0.164	Loss 1.039	Prec@1 76.1600	Prec@5 94.1100	
Best Prec@1: [76.420]	
Starting epoch number: 169 Learning rate: 0.010000000000000002
Train: [169]	Time 257.886	Data 0.470	Loss 0.033	Prec@1 99.6920	Prec@5 99.9980	
Val: [169]	Time 15.748	Data 0.141	Loss 1.034	Prec@1 76.2600	Prec@5 94.0500	
Best Prec@1: [76.420]	
Starting epoch number: 170 Learning rate: 0.010000000000000002
Train: [170]	Time 258.289	Data 0.473	Loss 0.033	Prec@1 99.6360	Prec@5 100.0000	
Val: [170]	Time 15.726	Data 0.140	Loss 1.044	Prec@1 76.0200	Prec@5 94.0300	
Best Prec@1: [76.420]	
Starting epoch number: 171 Learning rate: 0.010000000000000002
Train: [171]	Time 258.375	Data 0.452	Loss 0.032	Prec@1 99.6540	Prec@5 100.0000	
Val: [171]	Time 15.785	Data 0.191	Loss 1.035	Prec@1 76.5600	Prec@5 93.9700	
Best Prec@1: [76.560]	
Starting epoch number: 172 Learning rate: 0.010000000000000002
Train: [172]	Time 257.806	Data 0.453	Loss 0.031	Prec@1 99.6740	Prec@5 99.9980	
Val: [172]	Time 15.677	Data 0.133	Loss 1.036	Prec@1 76.5800	Prec@5 94.0700	
Best Prec@1: [76.580]	
Starting epoch number: 173 Learning rate: 0.010000000000000002
Train: [173]	Time 257.395	Data 0.509	Loss 0.029	Prec@1 99.7420	Prec@5 100.0000	
Val: [173]	Time 15.744	Data 0.135	Loss 1.043	Prec@1 76.4000	Prec@5 94.0100	
Best Prec@1: [76.580]	
Starting epoch number: 174 Learning rate: 0.010000000000000002
Train: [174]	Time 257.980	Data 0.461	Loss 0.029	Prec@1 99.7100	Prec@5 100.0000	
Val: [174]	Time 15.742	Data 0.162	Loss 1.037	Prec@1 76.6900	Prec@5 94.0800	
Best Prec@1: [76.690]	
Starting epoch number: 175 Learning rate: 0.010000000000000002
Train: [175]	Time 258.271	Data 0.475	Loss 0.028	Prec@1 99.7420	Prec@5 100.0000	
Val: [175]	Time 15.811	Data 0.190	Loss 1.042	Prec@1 76.5500	Prec@5 94.0300	
Best Prec@1: [76.690]	
Starting epoch number: 176 Learning rate: 0.010000000000000002
Train: [176]	Time 258.089	Data 0.427	Loss 0.027	Prec@1 99.7940	Prec@5 100.0000	
Val: [176]	Time 15.741	Data 0.145	Loss 1.028	Prec@1 76.6500	Prec@5 94.2300	
Best Prec@1: [76.690]	
Starting epoch number: 177 Learning rate: 0.010000000000000002
Train: [177]	Time 258.301	Data 0.425	Loss 0.027	Prec@1 99.7800	Prec@5 99.9980	
Val: [177]	Time 15.828	Data 0.159	Loss 1.026	Prec@1 76.6400	Prec@5 94.1300	
Best Prec@1: [76.690]	
Starting epoch number: 178 Learning rate: 0.010000000000000002
Train: [178]	Time 257.761	Data 0.449	Loss 0.027	Prec@1 99.7760	Prec@5 100.0000	
Val: [178]	Time 15.785	Data 0.180	Loss 1.037	Prec@1 76.4800	Prec@5 93.9500	
Best Prec@1: [76.690]	
Starting epoch number: 179 Learning rate: 0.010000000000000002
Train: [179]	Time 257.587	Data 0.438	Loss 0.026	Prec@1 99.8020	Prec@5 100.0000	
Val: [179]	Time 15.744	Data 0.182	Loss 1.041	Prec@1 76.6100	Prec@5 94.0900	
Best Prec@1: [76.690]	
Starting epoch number: 180 Learning rate: 0.010000000000000002
Train: [180]	Time 258.024	Data 0.488	Loss 0.025	Prec@1 99.8120	Prec@5 100.0000	
Val: [180]	Time 15.761	Data 0.164	Loss 1.042	Prec@1 76.5900	Prec@5 94.0500	
Best Prec@1: [76.690]	
Starting epoch number: 181 Learning rate: 0.010000000000000002
Train: [181]	Time 258.043	Data 0.504	Loss 0.025	Prec@1 99.8360	Prec@5 100.0000	
Val: [181]	Time 15.773	Data 0.143	Loss 1.030	Prec@1 76.6100	Prec@5 94.0200	
Best Prec@1: [76.690]	
Starting epoch number: 182 Learning rate: 0.010000000000000002
Train: [182]	Time 258.187	Data 0.462	Loss 0.024	Prec@1 99.8220	Prec@5 100.0000	
Val: [182]	Time 15.724	Data 0.137	Loss 1.034	Prec@1 76.7000	Prec@5 94.1400	
Best Prec@1: [76.700]	
Starting epoch number: 183 Learning rate: 0.010000000000000002
Train: [183]	Time 258.107	Data 0.445	Loss 0.024	Prec@1 99.8300	Prec@5 100.0000	
Val: [183]	Time 15.720	Data 0.173	Loss 1.030	Prec@1 76.5200	Prec@5 94.1500	
Best Prec@1: [76.700]	
Starting epoch number: 184 Learning rate: 0.010000000000000002
Train: [184]	Time 257.246	Data 0.438	Loss 0.023	Prec@1 99.8520	Prec@5 100.0000	
Val: [184]	Time 15.742	Data 0.183	Loss 1.033	Prec@1 76.6300	Prec@5 94.0200	
Best Prec@1: [76.700]	
Starting epoch number: 185 Learning rate: 0.010000000000000002
Train: [185]	Time 257.572	Data 0.463	Loss 0.022	Prec@1 99.8640	Prec@5 100.0000	
Val: [185]	Time 15.779	Data 0.168	Loss 1.017	Prec@1 77.0000	Prec@5 94.1000	
Best Prec@1: [77.000]	
Starting epoch number: 186 Learning rate: 0.010000000000000002
Train: [186]	Time 257.987	Data 0.511	Loss 0.022	Prec@1 99.8620	Prec@5 100.0000	
Val: [186]	Time 15.810	Data 0.146	Loss 1.023	Prec@1 76.8000	Prec@5 94.1600	
Best Prec@1: [77.000]	
Starting epoch number: 187 Learning rate: 0.010000000000000002
Train: [187]	Time 258.051	Data 0.433	Loss 0.022	Prec@1 99.8500	Prec@5 100.0000	
Val: [187]	Time 15.769	Data 0.145	Loss 1.030	Prec@1 76.7600	Prec@5 94.0400	
Best Prec@1: [77.000]	
Starting epoch number: 188 Learning rate: 0.010000000000000002
Train: [188]	Time 258.061	Data 0.451	Loss 0.023	Prec@1 99.8460	Prec@5 100.0000	
Val: [188]	Time 15.677	Data 0.163	Loss 1.034	Prec@1 76.4600	Prec@5 93.9500	
Best Prec@1: [77.000]	
Starting epoch number: 189 Learning rate: 0.010000000000000002
Train: [189]	Time 257.169	Data 0.431	Loss 0.022	Prec@1 99.8600	Prec@5 100.0000	
Val: [189]	Time 15.739	Data 0.148	Loss 1.017	Prec@1 76.9000	Prec@5 94.1400	
Best Prec@1: [77.000]	
Starting epoch number: 190 Learning rate: 0.010000000000000002
Train: [190]	Time 257.654	Data 0.461	Loss 0.022	Prec@1 99.8780	Prec@5 100.0000	
Val: [190]	Time 15.674	Data 0.141	Loss 1.030	Prec@1 76.6800	Prec@5 93.9300	
Best Prec@1: [77.000]	
Starting epoch number: 191 Learning rate: 0.010000000000000002
Train: [191]	Time 257.933	Data 0.423	Loss 0.022	Prec@1 99.8580	Prec@5 100.0000	
Val: [191]	Time 15.770	Data 0.155	Loss 1.024	Prec@1 76.8500	Prec@5 94.0300	
Best Prec@1: [77.000]	
Starting epoch number: 192 Learning rate: 0.010000000000000002
Train: [192]	Time 257.860	Data 0.473	Loss 0.022	Prec@1 99.8700	Prec@5 100.0000	
Val: [192]	Time 15.732	Data 0.160	Loss 1.019	Prec@1 76.6000	Prec@5 94.0100	
Best Prec@1: [77.000]	
Starting epoch number: 193 Learning rate: 0.010000000000000002
Train: [193]	Time 257.146	Data 0.455	Loss 0.021	Prec@1 99.8560	Prec@5 100.0000	
Val: [193]	Time 15.760	Data 0.156	Loss 1.021	Prec@1 76.5200	Prec@5 94.0800	
Best Prec@1: [77.000]	
Starting epoch number: 194 Learning rate: 0.010000000000000002
Train: [194]	Time 257.614	Data 0.481	Loss 0.021	Prec@1 99.8820	Prec@5 100.0000	
Val: [194]	Time 15.751	Data 0.148	Loss 1.022	Prec@1 76.4500	Prec@5 94.0700	
Best Prec@1: [77.000]	
Starting epoch number: 195 Learning rate: 0.010000000000000002
Train: [195]	Time 258.159	Data 0.458	Loss 0.021	Prec@1 99.8820	Prec@5 100.0000	
Val: [195]	Time 15.818	Data 0.174	Loss 1.021	Prec@1 76.8300	Prec@5 94.0100	
Best Prec@1: [77.000]	
Starting epoch number: 196 Learning rate: 0.010000000000000002
Train: [196]	Time 258.316	Data 0.477	Loss 0.021	Prec@1 99.8780	Prec@5 100.0000	
Val: [196]	Time 15.804	Data 0.169	Loss 1.021	Prec@1 76.8600	Prec@5 93.9400	
Best Prec@1: [77.000]	
Starting epoch number: 197 Learning rate: 0.010000000000000002
Train: [197]	Time 258.017	Data 0.463	Loss 0.020	Prec@1 99.8940	Prec@5 100.0000	
Val: [197]	Time 15.655	Data 0.154	Loss 1.020	Prec@1 76.7700	Prec@5 94.1300	
Best Prec@1: [77.000]	
Starting epoch number: 198 Learning rate: 0.010000000000000002
Train: [198]	Time 257.495	Data 0.454	Loss 0.021	Prec@1 99.8980	Prec@5 100.0000	
Val: [198]	Time 15.681	Data 0.141	Loss 1.017	Prec@1 76.8200	Prec@5 93.9500	
Best Prec@1: [77.000]	
Starting epoch number: 199 Learning rate: 0.010000000000000002
Train: [199]	Time 257.786	Data 0.448	Loss 0.021	Prec@1 99.8700	Prec@5 100.0000	
Val: [199]	Time 15.744	Data 0.149	Loss 1.020	Prec@1 76.8100	Prec@5 93.8500	
Best Prec@1: [77.000]	
Starting epoch number: 200 Learning rate: 0.010000000000000002
Train: [200]	Time 258.022	Data 0.487	Loss 0.021	Prec@1 99.8980	Prec@5 100.0000	
Val: [200]	Time 15.690	Data 0.142	Loss 1.013	Prec@1 76.6000	Prec@5 94.0500	
Best Prec@1: [77.000]	
Starting epoch number: 201 Learning rate: 0.010000000000000002
Train: [201]	Time 258.355	Data 0.433	Loss 0.020	Prec@1 99.8860	Prec@5 100.0000	
Val: [201]	Time 15.735	Data 0.151	Loss 1.019	Prec@1 76.7800	Prec@5 94.0100	
Best Prec@1: [77.000]	
Starting epoch number: 202 Learning rate: 0.010000000000000002
Train: [202]	Time 258.346	Data 0.442	Loss 0.019	Prec@1 99.8920	Prec@5 100.0000	
Val: [202]	Time 15.699	Data 0.137	Loss 1.020	Prec@1 76.6000	Prec@5 93.8800	
Best Prec@1: [77.000]	
Starting epoch number: 203 Learning rate: 0.010000000000000002
Train: [203]	Time 257.201	Data 0.433	Loss 0.019	Prec@1 99.9020	Prec@5 100.0000	
Val: [203]	Time 15.695	Data 0.147	Loss 1.013	Prec@1 76.6500	Prec@5 93.9500	
Best Prec@1: [77.000]	
Starting epoch number: 204 Learning rate: 0.010000000000000002
Train: [204]	Time 257.443	Data 0.454	Loss 0.020	Prec@1 99.8920	Prec@5 100.0000	
Val: [204]	Time 15.734	Data 0.147	Loss 1.018	Prec@1 76.5300	Prec@5 93.9200	
Best Prec@1: [77.000]	
Starting epoch number: 205 Learning rate: 0.010000000000000002
Train: [205]	Time 257.249	Data 0.464	Loss 0.019	Prec@1 99.8960	Prec@5 100.0000	
Val: [205]	Time 15.625	Data 0.152	Loss 1.011	Prec@1 76.4800	Prec@5 94.0600	
Best Prec@1: [77.000]	
Starting epoch number: 206 Learning rate: 0.010000000000000002
Train: [206]	Time 255.935	Data 0.446	Loss 0.019	Prec@1 99.9020	Prec@5 100.0000	
Val: [206]	Time 15.681	Data 0.135	Loss 1.014	Prec@1 76.7500	Prec@5 94.0100	
Best Prec@1: [77.000]	
Starting epoch number: 207 Learning rate: 0.010000000000000002
Train: [207]	Time 256.463	Data 0.420	Loss 0.019	Prec@1 99.9200	Prec@5 100.0000	
Val: [207]	Time 15.614	Data 0.155	Loss 1.006	Prec@1 76.4700	Prec@5 93.8500	
Best Prec@1: [77.000]	
Starting epoch number: 208 Learning rate: 0.010000000000000002
Train: [208]	Time 256.894	Data 0.462	Loss 0.019	Prec@1 99.8880	Prec@5 100.0000	
Val: [208]	Time 15.665	Data 0.158	Loss 1.000	Prec@1 76.6800	Prec@5 93.9700	
Best Prec@1: [77.000]	
Starting epoch number: 209 Learning rate: 0.010000000000000002
Train: [209]	Time 256.947	Data 0.426	Loss 0.019	Prec@1 99.9040	Prec@5 100.0000	
Val: [209]	Time 15.630	Data 0.138	Loss 1.016	Prec@1 76.7400	Prec@5 93.8100	
Best Prec@1: [77.000]	
Starting epoch number: 210 Learning rate: 0.010000000000000002
Train: [210]	Time 256.631	Data 0.419	Loss 0.019	Prec@1 99.9200	Prec@5 99.9980	
Val: [210]	Time 15.593	Data 0.134	Loss 1.014	Prec@1 76.8500	Prec@5 93.8900	
Best Prec@1: [77.000]	
Starting epoch number: 211 Learning rate: 0.010000000000000002
Train: [211]	Time 256.599	Data 0.430	Loss 0.019	Prec@1 99.8940	Prec@5 100.0000	
Val: [211]	Time 15.595	Data 0.174	Loss 1.009	Prec@1 76.7300	Prec@5 93.8500	
Best Prec@1: [77.000]	
Starting epoch number: 212 Learning rate: 0.010000000000000002
Train: [212]	Time 256.102	Data 0.450	Loss 0.018	Prec@1 99.9400	Prec@5 100.0000	
Val: [212]	Time 15.663	Data 0.187	Loss 1.014	Prec@1 76.6500	Prec@5 93.8800	
Best Prec@1: [77.000]	
Starting epoch number: 213 Learning rate: 0.010000000000000002
Train: [213]	Time 256.630	Data 0.439	Loss 0.019	Prec@1 99.9200	Prec@5 100.0000	
Val: [213]	Time 15.610	Data 0.149	Loss 1.014	Prec@1 76.6000	Prec@5 93.8300	
Best Prec@1: [77.000]	
Starting epoch number: 214 Learning rate: 0.010000000000000002
Train: [214]	Time 256.842	Data 0.502	Loss 0.018	Prec@1 99.9120	Prec@5 100.0000	
Val: [214]	Time 15.623	Data 0.176	Loss 1.008	Prec@1 76.7200	Prec@5 93.8600	
Best Prec@1: [77.000]	
Starting epoch number: 215 Learning rate: 0.010000000000000002
Train: [215]	Time 256.727	Data 0.450	Loss 0.018	Prec@1 99.9000	Prec@5 100.0000	
Val: [215]	Time 15.640	Data 0.155	Loss 1.010	Prec@1 76.6500	Prec@5 94.0100	
Best Prec@1: [77.000]	
Starting epoch number: 216 Learning rate: 0.010000000000000002
Train: [216]	Time 256.806	Data 0.455	Loss 0.019	Prec@1 99.9000	Prec@5 100.0000	
Val: [216]	Time 15.669	Data 0.185	Loss 1.005	Prec@1 76.7500	Prec@5 93.9100	
Best Prec@1: [77.000]	
Starting epoch number: 217 Learning rate: 0.010000000000000002
Train: [217]	Time 256.949	Data 0.475	Loss 0.018	Prec@1 99.9220	Prec@5 100.0000	
Val: [217]	Time 15.666	Data 0.162	Loss 1.006	Prec@1 76.5900	Prec@5 93.9900	
Best Prec@1: [77.000]	
Starting epoch number: 218 Learning rate: 0.010000000000000002
Train: [218]	Time 256.381	Data 0.453	Loss 0.018	Prec@1 99.9220	Prec@5 99.9980	
Val: [218]	Time 15.563	Data 0.162	Loss 1.006	Prec@1 77.0300	Prec@5 93.9300	
Best Prec@1: [77.030]	
Starting epoch number: 219 Learning rate: 0.010000000000000002
Train: [219]	Time 256.026	Data 0.332	Loss 0.018	Prec@1 99.9040	Prec@5 100.0000	
Val: [219]	Time 15.521	Data 0.117	Loss 1.001	Prec@1 76.6700	Prec@5 93.7200	
Best Prec@1: [77.030]	
Starting epoch number: 220 Learning rate: 0.010000000000000002
Train: [220]	Time 255.674	Data 0.319	Loss 0.017	Prec@1 99.9300	Prec@5 100.0000	
Val: [220]	Time 15.564	Data 0.130	Loss 0.998	Prec@1 76.6400	Prec@5 93.7200	
Best Prec@1: [77.030]	
Starting epoch number: 221 Learning rate: 0.010000000000000002
Train: [221]	Time 255.801	Data 0.290	Loss 0.018	Prec@1 99.9200	Prec@5 100.0000	
Val: [221]	Time 15.546	Data 0.111	Loss 0.997	Prec@1 76.5600	Prec@5 93.8200	
Best Prec@1: [77.030]	
Starting epoch number: 222 Learning rate: 0.010000000000000002
Train: [222]	Time 255.921	Data 0.399	Loss 0.018	Prec@1 99.9360	Prec@5 100.0000	
Val: [222]	Time 15.564	Data 0.132	Loss 0.999	Prec@1 76.5500	Prec@5 93.7600	
Best Prec@1: [77.030]	
Starting epoch number: 223 Learning rate: 0.010000000000000002
Train: [223]	Time 255.981	Data 0.291	Loss 0.018	Prec@1 99.9180	Prec@5 100.0000	
Val: [223]	Time 15.567	Data 0.131	Loss 1.002	Prec@1 76.8600	Prec@5 93.7500	
Best Prec@1: [77.030]	
Starting epoch number: 224 Learning rate: 0.010000000000000002
Train: [224]	Time 256.044	Data 0.330	Loss 0.018	Prec@1 99.9240	Prec@5 100.0000	
Val: [224]	Time 15.641	Data 0.148	Loss 1.010	Prec@1 76.6300	Prec@5 93.9200	
Best Prec@1: [77.030]	
Starting epoch number: 225 Learning rate: 0.0010000000000000002
Train: [225]	Time 255.950	Data 0.384	Loss 0.016	Prec@1 99.9500	Prec@5 100.0000	
Val: [225]	Time 15.534	Data 0.145	Loss 1.006	Prec@1 76.6800	Prec@5 93.7200	
Best Prec@1: [77.030]	
Starting epoch number: 226 Learning rate: 0.0010000000000000002
Train: [226]	Time 255.756	Data 0.294	Loss 0.015	Prec@1 99.9480	Prec@5 100.0000	
Val: [226]	Time 15.526	Data 0.116	Loss 0.996	Prec@1 76.7100	Prec@5 93.9400	
Best Prec@1: [77.030]	
Starting epoch number: 227 Learning rate: 0.0010000000000000002
Train: [227]	Time 255.248	Data 0.341	Loss 0.015	Prec@1 99.9580	Prec@5 100.0000	
Val: [227]	Time 15.494	Data 0.130	Loss 0.998	Prec@1 76.6100	Prec@5 93.8100	
Best Prec@1: [77.030]	
Starting epoch number: 228 Learning rate: 0.0010000000000000002
Train: [228]	Time 255.513	Data 0.281	Loss 0.014	Prec@1 99.9560	Prec@5 100.0000	
Val: [228]	Time 15.533	Data 0.111	Loss 1.001	Prec@1 76.8300	Prec@5 93.9300	
Best Prec@1: [77.030]	
Starting epoch number: 229 Learning rate: 0.0010000000000000002
Train: [229]	Time 255.923	Data 0.324	Loss 0.015	Prec@1 99.9540	Prec@5 100.0000	
Val: [229]	Time 15.557	Data 0.129	Loss 0.995	Prec@1 76.7800	Prec@5 93.8100	
Best Prec@1: [77.030]	
Starting epoch number: 230 Learning rate: 0.0010000000000000002
Train: [230]	Time 256.070	Data 0.296	Loss 0.015	Prec@1 99.9480	Prec@5 100.0000	
Val: [230]	Time 15.570	Data 0.122	Loss 0.993	Prec@1 76.7500	Prec@5 93.8900	
Best Prec@1: [77.030]	
Starting epoch number: 231 Learning rate: 0.0010000000000000002
Train: [231]	Time 256.161	Data 0.307	Loss 0.014	Prec@1 99.9500	Prec@5 100.0000	
Val: [231]	Time 15.549	Data 0.130	Loss 1.002	Prec@1 76.9000	Prec@5 93.7400	
Best Prec@1: [77.030]	
Starting epoch number: 232 Learning rate: 0.0010000000000000002
Train: [232]	Time 255.317	Data 0.303	Loss 0.014	Prec@1 99.9460	Prec@5 100.0000	
Val: [232]	Time 15.533	Data 0.127	Loss 1.001	Prec@1 76.7600	Prec@5 93.8600	
Best Prec@1: [77.030]	
Starting epoch number: 233 Learning rate: 0.0010000000000000002
Train: [233]	Time 255.655	Data 0.340	Loss 0.014	Prec@1 99.9620	Prec@5 100.0000	
Val: [233]	Time 15.522	Data 0.153	Loss 0.996	Prec@1 76.9000	Prec@5 93.8000	
Best Prec@1: [77.030]	
Starting epoch number: 234 Learning rate: 0.0010000000000000002
Train: [234]	Time 255.199	Data 0.301	Loss 0.014	Prec@1 99.9720	Prec@5 100.0000	
Val: [234]	Time 15.493	Data 0.116	Loss 0.995	Prec@1 76.7300	Prec@5 93.7600	
Best Prec@1: [77.030]	
Starting epoch number: 235 Learning rate: 0.0010000000000000002
Train: [235]	Time 255.742	Data 0.285	Loss 0.014	Prec@1 99.9760	Prec@5 100.0000	
Val: [235]	Time 15.521	Data 0.117	Loss 0.997	Prec@1 77.0300	Prec@5 93.7800	
Best Prec@1: [77.030]	
Starting epoch number: 236 Learning rate: 0.0010000000000000002
Train: [236]	Time 256.004	Data 0.305	Loss 0.014	Prec@1 99.9600	Prec@5 100.0000	
Val: [236]	Time 15.549	Data 0.116	Loss 0.997	Prec@1 76.9600	Prec@5 93.8200	
Best Prec@1: [77.030]	
Starting epoch number: 237 Learning rate: 0.0010000000000000002
Train: [237]	Time 256.080	Data 0.341	Loss 0.014	Prec@1 99.9540	Prec@5 100.0000	
Val: [237]	Time 15.493	Data 0.133	Loss 0.998	Prec@1 76.6900	Prec@5 93.7300	
Best Prec@1: [77.030]	
Starting epoch number: 238 Learning rate: 0.0010000000000000002
Train: [238]	Time 255.542	Data 0.305	Loss 0.014	Prec@1 99.9640	Prec@5 100.0000	
Val: [238]	Time 15.561	Data 0.125	Loss 0.995	Prec@1 76.9000	Prec@5 93.8900	
Best Prec@1: [77.030]	
Starting epoch number: 239 Learning rate: 0.0010000000000000002
Train: [239]	Time 255.493	Data 0.293	Loss 0.014	Prec@1 99.9560	Prec@5 100.0000	
Val: [239]	Time 15.474	Data 0.125	Loss 0.994	Prec@1 76.9700	Prec@5 93.7400	
Best Prec@1: [77.030]	
Starting epoch number: 240 Learning rate: 0.0010000000000000002
Train: [240]	Time 255.497	Data 0.402	Loss 0.013	Prec@1 99.9560	Prec@5 100.0000	
Val: [240]	Time 15.540	Data 0.137	Loss 0.994	Prec@1 76.8700	Prec@5 93.9200	
Best Prec@1: [77.030]	
Starting epoch number: 241 Learning rate: 0.0010000000000000002
Train: [241]	Time 255.913	Data 0.294	Loss 0.014	Prec@1 99.9600	Prec@5 100.0000	
Val: [241]	Time 15.537	Data 0.116	Loss 0.995	Prec@1 76.8000	Prec@5 93.8900	
Best Prec@1: [77.030]	
Starting epoch number: 242 Learning rate: 0.0010000000000000002
Train: [242]	Time 256.073	Data 0.302	Loss 0.014	Prec@1 99.9600	Prec@5 100.0000	
Val: [242]	Time 15.585	Data 0.124	Loss 0.988	Prec@1 76.9900	Prec@5 93.8900	
Best Prec@1: [77.030]	
Starting epoch number: 243 Learning rate: 0.0010000000000000002
Train: [243]	Time 255.490	Data 0.315	Loss 0.013	Prec@1 99.9640	Prec@5 100.0000	
Val: [243]	Time 15.519	Data 0.108	Loss 0.993	Prec@1 76.9100	Prec@5 93.8300	
Best Prec@1: [77.030]	
Starting epoch number: 244 Learning rate: 0.0010000000000000002
Train: [244]	Time 255.683	Data 0.409	Loss 0.014	Prec@1 99.9540	Prec@5 100.0000	
Val: [244]	Time 15.529	Data 0.143	Loss 0.990	Prec@1 76.8200	Prec@5 93.9900	
Best Prec@1: [77.030]	
Starting epoch number: 245 Learning rate: 0.0010000000000000002
Train: [245]	Time 255.297	Data 0.412	Loss 0.013	Prec@1 99.9580	Prec@5 100.0000	
Val: [245]	Time 15.518	Data 0.151	Loss 0.985	Prec@1 76.9500	Prec@5 93.8800	
Best Prec@1: [77.030]	
Starting epoch number: 246 Learning rate: 0.0010000000000000002
Train: [246]	Time 255.715	Data 0.318	Loss 0.013	Prec@1 99.9460	Prec@5 100.0000	
Val: [246]	Time 15.588	Data 0.138	Loss 0.994	Prec@1 76.9300	Prec@5 93.9800	
Best Prec@1: [77.030]	
Starting epoch number: 247 Learning rate: 0.0010000000000000002
Train: [247]	Time 256.027	Data 0.316	Loss 0.014	Prec@1 99.9560	Prec@5 100.0000	
Val: [247]	Time 15.567	Data 0.114	Loss 0.989	Prec@1 76.9600	Prec@5 93.8800	
Best Prec@1: [77.030]	
Starting epoch number: 248 Learning rate: 0.0010000000000000002
Train: [248]	Time 255.642	Data 0.298	Loss 0.013	Prec@1 99.9580	Prec@5 100.0000	
Val: [248]	Time 15.492	Data 0.121	Loss 0.992	Prec@1 76.9500	Prec@5 93.8700	
Best Prec@1: [77.030]	
Starting epoch number: 249 Learning rate: 0.0010000000000000002
Train: [249]	Time 255.672	Data 0.312	Loss 0.013	Prec@1 99.9660	Prec@5 100.0000	
Val: [249]	Time 15.509	Data 0.128	Loss 0.989	Prec@1 77.0000	Prec@5 93.8500	
Best Prec@1: [77.030]	
Starting epoch number: 250 Learning rate: 0.0010000000000000002
Train: [250]	Time 255.139	Data 0.299	Loss 0.013	Prec@1 99.9640	Prec@5 100.0000	
Val: [250]	Time 15.509	Data 0.118	Loss 0.988	Prec@1 77.0500	Prec@5 94.0100	
Best Prec@1: [77.050]	
Starting epoch number: 251 Learning rate: 0.0010000000000000002
Train: [251]	Time 255.629	Data 0.314	Loss 0.013	Prec@1 99.9600	Prec@5 100.0000	
Val: [251]	Time 15.532	Data 0.142	Loss 0.988	Prec@1 76.9900	Prec@5 93.9000	
Best Prec@1: [77.050]	
Starting epoch number: 252 Learning rate: 0.0010000000000000002
Train: [252]	Time 256.045	Data 0.312	Loss 0.013	Prec@1 99.9580	Prec@5 100.0000	
Val: [252]	Time 15.582	Data 0.131	Loss 0.990	Prec@1 76.9200	Prec@5 93.9300	
Best Prec@1: [77.050]	
Starting epoch number: 253 Learning rate: 0.0010000000000000002
Train: [253]	Time 255.785	Data 0.323	Loss 0.013	Prec@1 99.9540	Prec@5 100.0000	
Val: [253]	Time 15.497	Data 0.120	Loss 0.989	Prec@1 77.0900	Prec@5 93.9000	
Best Prec@1: [77.090]	
Starting epoch number: 254 Learning rate: 0.0010000000000000002
Train: [254]	Time 255.683	Data 0.306	Loss 0.013	Prec@1 99.9680	Prec@5 100.0000	
Val: [254]	Time 15.509	Data 0.129	Loss 0.993	Prec@1 76.7400	Prec@5 94.0000	
Best Prec@1: [77.090]	
Starting epoch number: 255 Learning rate: 0.0010000000000000002
Train: [255]	Time 255.111	Data 0.307	Loss 0.013	Prec@1 99.9560	Prec@5 100.0000	
Val: [255]	Time 15.519	Data 0.122	Loss 0.997	Prec@1 76.9100	Prec@5 93.8900	
Best Prec@1: [77.090]	
Starting epoch number: 256 Learning rate: 0.0010000000000000002
Train: [256]	Time 255.555	Data 0.284	Loss 0.014	Prec@1 99.9620	Prec@5 100.0000	
Val: [256]	Time 15.559	Data 0.119	Loss 0.989	Prec@1 77.0700	Prec@5 93.9500	
Best Prec@1: [77.090]	
Starting epoch number: 257 Learning rate: 0.0010000000000000002
Train: [257]	Time 255.976	Data 0.311	Loss 0.013	Prec@1 99.9540	Prec@5 100.0000	
Val: [257]	Time 15.574	Data 0.125	Loss 0.987	Prec@1 77.0400	Prec@5 93.9700	
Best Prec@1: [77.090]	
Starting epoch number: 258 Learning rate: 0.0010000000000000002
Train: [258]	Time 256.173	Data 0.290	Loss 0.013	Prec@1 99.9680	Prec@5 100.0000	
Val: [258]	Time 15.511	Data 0.114	Loss 0.990	Prec@1 76.9600	Prec@5 93.8500	
Best Prec@1: [77.090]	
Starting epoch number: 259 Learning rate: 0.0010000000000000002
Train: [259]	Time 255.391	Data 0.299	Loss 0.013	Prec@1 99.9540	Prec@5 100.0000	
Val: [259]	Time 15.534	Data 0.145	Loss 0.990	Prec@1 76.8900	Prec@5 93.8600	
Best Prec@1: [77.090]	
Starting epoch number: 260 Learning rate: 0.0010000000000000002
Train: [260]	Time 255.821	Data 0.392	Loss 0.013	Prec@1 99.9700	Prec@5 100.0000	
Val: [260]	Time 15.490	Data 0.150	Loss 0.992	Prec@1 76.9800	Prec@5 93.9400	
Best Prec@1: [77.090]	
Starting epoch number: 261 Learning rate: 0.0010000000000000002
Train: [261]	Time 255.244	Data 0.295	Loss 0.013	Prec@1 99.9720	Prec@5 100.0000	
Val: [261]	Time 15.497	Data 0.116	Loss 0.983	Prec@1 77.0200	Prec@5 93.8900	
Best Prec@1: [77.090]	
Starting epoch number: 262 Learning rate: 0.0010000000000000002
Train: [262]	Time 255.791	Data 0.291	Loss 0.013	Prec@1 99.9680	Prec@5 100.0000	
Val: [262]	Time 15.540	Data 0.123	Loss 0.995	Prec@1 76.8700	Prec@5 93.8900	
Best Prec@1: [77.090]	
Starting epoch number: 263 Learning rate: 0.0010000000000000002
Train: [263]	Time 256.050	Data 0.330	Loss 0.013	Prec@1 99.9660	Prec@5 100.0000	
Val: [263]	Time 15.581	Data 0.135	Loss 0.991	Prec@1 76.8900	Prec@5 93.8600	
Best Prec@1: [77.090]	
Starting epoch number: 264 Learning rate: 0.0010000000000000002
Train: [264]	Time 255.553	Data 0.301	Loss 0.013	Prec@1 99.9680	Prec@5 100.0000	
Val: [264]	Time 15.550	Data 0.146	Loss 0.991	Prec@1 76.9500	Prec@5 93.9500	
Best Prec@1: [77.090]	
Starting epoch number: 265 Learning rate: 0.0010000000000000002
Train: [265]	Time 255.756	Data 0.334	Loss 0.013	Prec@1 99.9760	Prec@5 100.0000	
Val: [265]	Time 15.539	Data 0.153	Loss 0.991	Prec@1 77.0700	Prec@5 93.8200	
Best Prec@1: [77.090]	
Starting epoch number: 266 Learning rate: 0.0010000000000000002
Train: [266]	Time 255.193	Data 0.281	Loss 0.013	Prec@1 99.9700	Prec@5 100.0000	
Val: [266]	Time 15.539	Data 0.126	Loss 0.988	Prec@1 76.7900	Prec@5 93.8900	
Best Prec@1: [77.090]	
Starting epoch number: 267 Learning rate: 0.0010000000000000002
Train: [267]	Time 255.676	Data 0.291	Loss 0.013	Prec@1 99.9640	Prec@5 100.0000	
Val: [267]	Time 15.516	Data 0.118	Loss 0.990	Prec@1 76.8300	Prec@5 93.8900	
Best Prec@1: [77.090]	
Starting epoch number: 268 Learning rate: 0.0010000000000000002
Train: [268]	Time 256.059	Data 0.296	Loss 0.013	Prec@1 99.9680	Prec@5 100.0000	
Val: [268]	Time 15.669	Data 0.117	Loss 0.989	Prec@1 76.9700	Prec@5 93.8200	
Best Prec@1: [77.090]	
Starting epoch number: 269 Learning rate: 0.0010000000000000002
Train: [269]	Time 255.709	Data 0.329	Loss 0.013	Prec@1 99.9720	Prec@5 100.0000	
Val: [269]	Time 15.555	Data 0.138	Loss 0.991	Prec@1 76.9900	Prec@5 93.8000	
Best Prec@1: [77.090]	
Starting epoch number: 270 Learning rate: 0.0010000000000000002
Train: [270]	Time 255.754	Data 0.292	Loss 0.013	Prec@1 99.9560	Prec@5 100.0000	
Val: [270]	Time 15.515	Data 0.120	Loss 0.986	Prec@1 77.1400	Prec@5 93.9800	
Best Prec@1: [77.140]	
Starting epoch number: 271 Learning rate: 0.0010000000000000002
Train: [271]	Time 255.187	Data 0.305	Loss 0.013	Prec@1 99.9600	Prec@5 100.0000	
Val: [271]	Time 15.520	Data 0.127	Loss 0.989	Prec@1 76.7700	Prec@5 93.8200	
Best Prec@1: [77.140]	
Starting epoch number: 272 Learning rate: 0.0010000000000000002
Train: [272]	Time 255.758	Data 0.382	Loss 0.013	Prec@1 99.9660	Prec@5 100.0000	
Val: [272]	Time 15.540	Data 0.137	Loss 0.985	Prec@1 77.2100	Prec@5 93.9800	
Best Prec@1: [77.210]	
Starting epoch number: 273 Learning rate: 0.0010000000000000002
Train: [273]	Time 256.112	Data 0.325	Loss 0.013	Prec@1 99.9540	Prec@5 100.0000	
Val: [273]	Time 15.609	Data 0.143	Loss 0.991	Prec@1 76.9200	Prec@5 93.8800	
Best Prec@1: [77.210]	
Starting epoch number: 274 Learning rate: 0.0010000000000000002
Train: [274]	Time 255.782	Data 0.294	Loss 0.013	Prec@1 99.9640	Prec@5 100.0000	
Val: [274]	Time 15.503	Data 0.129	Loss 0.985	Prec@1 76.8500	Prec@5 93.8400	
Best Prec@1: [77.210]	
Starting epoch number: 275 Learning rate: 0.0010000000000000002
Train: [275]	Time 255.610	Data 0.387	Loss 0.013	Prec@1 99.9580	Prec@5 100.0000	
Val: [275]	Time 15.571	Data 0.145	Loss 0.986	Prec@1 76.9200	Prec@5 93.9100	
Best Prec@1: [77.210]	
Starting epoch number: 276 Learning rate: 0.0010000000000000002
Train: [276]	Time 255.452	Data 0.390	Loss 0.013	Prec@1 99.9640	Prec@5 100.0000	
Val: [276]	Time 15.502	Data 0.142	Loss 0.979	Prec@1 76.9400	Prec@5 93.9800	
Best Prec@1: [77.210]	
Starting epoch number: 277 Learning rate: 0.0010000000000000002
Train: [277]	Time 255.445	Data 0.400	Loss 0.013	Prec@1 99.9620	Prec@5 100.0000	
Val: [277]	Time 15.581	Data 0.140	Loss 0.986	Prec@1 76.8500	Prec@5 93.9700	
Best Prec@1: [77.210]	
Starting epoch number: 278 Learning rate: 0.0010000000000000002
Train: [278]	Time 255.893	Data 0.396	Loss 0.013	Prec@1 99.9520	Prec@5 100.0000	
Val: [278]	Time 15.575	Data 0.141	Loss 0.987	Prec@1 77.0200	Prec@5 93.8600	
Best Prec@1: [77.210]	
Starting epoch number: 279 Learning rate: 0.0010000000000000002
Train: [279]	Time 255.962	Data 0.312	Loss 0.013	Prec@1 99.9560	Prec@5 100.0000	
Val: [279]	Time 15.570	Data 0.144	Loss 0.986	Prec@1 77.1500	Prec@5 93.8800	
Best Prec@1: [77.210]	
Starting epoch number: 280 Learning rate: 0.0010000000000000002
Train: [280]	Time 255.467	Data 0.313	Loss 0.013	Prec@1 99.9640	Prec@5 100.0000	
Val: [280]	Time 15.564	Data 0.140	Loss 0.990	Prec@1 76.8700	Prec@5 93.8600	
Best Prec@1: [77.210]	
Starting epoch number: 281 Learning rate: 0.0010000000000000002
Train: [281]	Time 255.614	Data 0.367	Loss 0.012	Prec@1 99.9780	Prec@5 100.0000	
Val: [281]	Time 15.521	Data 0.146	Loss 0.984	Prec@1 77.0000	Prec@5 93.9400	
Best Prec@1: [77.210]	
Starting epoch number: 282 Learning rate: 0.0010000000000000002
Train: [282]	Time 255.339	Data 0.306	Loss 0.013	Prec@1 99.9700	Prec@5 100.0000	
Val: [282]	Time 15.545	Data 0.126	Loss 0.989	Prec@1 76.8700	Prec@5 93.9100	
Best Prec@1: [77.210]	
Starting epoch number: 283 Learning rate: 0.0010000000000000002
Train: [283]	Time 255.980	Data 0.420	Loss 0.013	Prec@1 99.9600	Prec@5 100.0000	
Val: [283]	Time 15.585	Data 0.151	Loss 0.985	Prec@1 76.7300	Prec@5 93.8200	
Best Prec@1: [77.210]	
Starting epoch number: 284 Learning rate: 0.0010000000000000002
Train: [284]	Time 255.952	Data 0.285	Loss 0.013	Prec@1 99.9620	Prec@5 100.0000	
Val: [284]	Time 15.514	Data 0.115	Loss 0.992	Prec@1 77.0000	Prec@5 94.0000	
Best Prec@1: [77.210]	
Starting epoch number: 285 Learning rate: 0.0010000000000000002
Train: [285]	Time 255.466	Data 0.289	Loss 0.013	Prec@1 99.9720	Prec@5 100.0000	
Val: [285]	Time 15.554	Data 0.131	Loss 0.989	Prec@1 76.9000	Prec@5 93.8400	
Best Prec@1: [77.210]	
Starting epoch number: 286 Learning rate: 0.0010000000000000002
Train: [286]	Time 255.643	Data 0.320	Loss 0.013	Prec@1 99.9700	Prec@5 100.0000	
Val: [286]	Time 15.491	Data 0.133	Loss 0.978	Prec@1 77.2300	Prec@5 94.0100	
Best Prec@1: [77.230]	
Starting epoch number: 287 Learning rate: 0.0010000000000000002
Train: [287]	Time 255.158	Data 0.285	Loss 0.013	Prec@1 99.9640	Prec@5 100.0000	
Val: [287]	Time 15.546	Data 0.131	Loss 0.988	Prec@1 76.8500	Prec@5 93.9200	
Best Prec@1: [77.230]	
Starting epoch number: 288 Learning rate: 0.0010000000000000002
Train: [288]	Time 255.772	Data 0.400	Loss 0.013	Prec@1 99.9620	Prec@5 100.0000	
Val: [288]	Time 15.580	Data 0.142	Loss 0.980	Prec@1 77.0900	Prec@5 94.0100	
Best Prec@1: [77.230]	
Starting epoch number: 289 Learning rate: 0.0010000000000000002
Train: [289]	Time 256.007	Data 0.286	Loss 0.013	Prec@1 99.9680	Prec@5 100.0000	
Val: [289]	Time 15.594	Data 0.155	Loss 0.985	Prec@1 77.0500	Prec@5 93.9400	
Best Prec@1: [77.230]	
Starting epoch number: 290 Learning rate: 0.0010000000000000002
Train: [290]	Time 255.587	Data 0.290	Loss 0.012	Prec@1 99.9640	Prec@5 100.0000	
Val: [290]	Time 15.518	Data 0.122	Loss 0.988	Prec@1 77.0400	Prec@5 93.9500	
Best Prec@1: [77.230]	
Starting epoch number: 291 Learning rate: 0.0010000000000000002
Train: [291]	Time 255.646	Data 0.316	Loss 0.013	Prec@1 99.9780	Prec@5 100.0000	
Val: [291]	Time 15.549	Data 0.127	Loss 0.987	Prec@1 76.8300	Prec@5 94.0000	
Best Prec@1: [77.230]	
Starting epoch number: 292 Learning rate: 0.0010000000000000002
Train: [292]	Time 255.323	Data 0.314	Loss 0.013	Prec@1 99.9580	Prec@5 100.0000	
Val: [292]	Time 15.528	Data 0.143	Loss 0.981	Prec@1 76.9300	Prec@5 93.8000	
Best Prec@1: [77.230]	
Starting epoch number: 293 Learning rate: 0.0010000000000000002
Train: [293]	Time 255.523	Data 0.297	Loss 0.013	Prec@1 99.9620	Prec@5 100.0000	
Val: [293]	Time 15.539	Data 0.136	Loss 0.987	Prec@1 76.7900	Prec@5 93.9700	
Best Prec@1: [77.230]	
Starting epoch number: 294 Learning rate: 0.0010000000000000002
Train: [294]	Time 256.027	Data 0.321	Loss 0.013	Prec@1 99.9600	Prec@5 100.0000	
Val: [294]	Time 15.586	Data 0.127	Loss 0.991	Prec@1 77.0100	Prec@5 93.8100	
Best Prec@1: [77.230]	
Starting epoch number: 295 Learning rate: 0.0010000000000000002
Train: [295]	Time 256.240	Data 0.392	Loss 0.013	Prec@1 99.9700	Prec@5 100.0000	
Val: [295]	Time 15.544	Data 0.131	Loss 0.984	Prec@1 76.9300	Prec@5 93.9100	
Best Prec@1: [77.230]	
Starting epoch number: 296 Learning rate: 0.0010000000000000002
Train: [296]	Time 255.474	Data 0.305	Loss 0.013	Prec@1 99.9680	Prec@5 100.0000	
Val: [296]	Time 15.540	Data 0.122	Loss 0.982	Prec@1 76.9500	Prec@5 93.8800	
Best Prec@1: [77.230]	
Starting epoch number: 297 Learning rate: 0.0010000000000000002
Train: [297]	Time 255.688	Data 0.319	Loss 0.013	Prec@1 99.9540	Prec@5 100.0000	
Val: [297]	Time 15.509	Data 0.120	Loss 0.985	Prec@1 76.7400	Prec@5 93.9500	
Best Prec@1: [77.230]	
Starting epoch number: 298 Learning rate: 0.0010000000000000002
Train: [298]	Time 255.369	Data 0.368	Loss 0.013	Prec@1 99.9660	Prec@5 100.0000	
Val: [298]	Time 15.532	Data 0.149	Loss 0.987	Prec@1 76.9000	Prec@5 93.8200	
Best Prec@1: [77.230]	
Starting epoch number: 299 Learning rate: 0.0010000000000000002
Train: [299]	Time 255.820	Data 0.328	Loss 0.013	Prec@1 99.9600	Prec@5 100.0000	
Val: [299]	Time 15.556	Data 0.133	Loss 0.983	Prec@1 76.7900	Prec@5 93.8400	
Best Prec@1: [77.230]	
