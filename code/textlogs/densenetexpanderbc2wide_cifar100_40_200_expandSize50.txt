Namespace(acc_type='class', augment=True, batch_size=64, bottleneck=True, cachemode=True, cardinality=8, criterion='crossentropy', cuda=True, data_dir='../data', dataset='cifar100', decayinterval=None, decaylevel=None, droprate=0, epochs=300, evaluate=False, expandConfig=None, expandSize=50, from_modelzoo=False, growth=200, layers=40, learningratescheduler='cifarschedular', logdir='../logs/densenetexpanderbc2wide_cifar100_40_200_expandSize50', lr=0.1, manualSeed=123, maxlr=0.1, minlr=0.0005, model_def='densenetexpander2_cifar', momentum=0.9, name='densenetexpanderbc2wide_cifar100_40_200_expandSize50', nclasses=100, nesterov=True, ngpus=1, optimType='sgd', pretrained=False, pretrained_file='', printfreq=200, reduce=0.5, resume='', start_epoch=0, store='', tenCrop=False, tensorboard=True, testOnly=False, verbose=False, weightDecay=0.0001, weight_init=False, widen_factor=4, workers=2)
DenseNet3 (
  (conv1): Conv2d(3, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): DenseBlock (
    (layer): Sequential (
      (0): BottleneckBlock (
        (bn1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (1): BottleneckBlock (
        (bn1): BatchNorm2d(600, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (2): BottleneckBlock (
        (bn1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (3): BottleneckBlock (
        (bn1): BatchNorm2d(1000, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (4): BottleneckBlock (
        (bn1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (5): BottleneckBlock (
        (bn1): BatchNorm2d(1400, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
    )
  )
  (trans1): TransitionBlock (
    (bn1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True)
    (relu): ReLU (inplace)
    (conv1): ExpanderConv2d (
    )
  )
  (block2): DenseBlock (
    (layer): Sequential (
      (0): BottleneckBlock (
        (bn1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (1): BottleneckBlock (
        (bn1): BatchNorm2d(1000, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (2): BottleneckBlock (
        (bn1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (3): BottleneckBlock (
        (bn1): BatchNorm2d(1400, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (4): BottleneckBlock (
        (bn1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (5): BottleneckBlock (
        (bn1): BatchNorm2d(1800, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
    )
  )
  (trans2): TransitionBlock (
    (bn1): BatchNorm2d(2000, eps=1e-05, momentum=0.1, affine=True)
    (relu): ReLU (inplace)
    (conv1): ExpanderConv2d (
    )
  )
  (block3): DenseBlock (
    (layer): Sequential (
      (0): BottleneckBlock (
        (bn1): BatchNorm2d(1000, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (1): BottleneckBlock (
        (bn1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (2): BottleneckBlock (
        (bn1): BatchNorm2d(1400, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (3): BottleneckBlock (
        (bn1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (4): BottleneckBlock (
        (bn1): BatchNorm2d(1800, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
      (5): BottleneckBlock (
        (bn1): BatchNorm2d(2000, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): ExpanderConv2d (
        )
        (bn2): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)
        (conv2): ExpanderConv2d (
        )
      )
    )
  )
  (bn1): BatchNorm2d(2200, eps=1e-05, momentum=0.1, affine=True)
  (relu): ReLU (inplace)
  (fc): Linear (2200 -> 100)
)
Files already downloaded and verified
Starting epoch number: 0 Learning rate: 0.1
Train: [0]	Time 673.906	Data 0.459	Loss 3.945	Prec@1 10.7100	Prec@5 31.5180	
Val: [0]	Time 40.049	Data 0.141	Loss 7.425	Prec@1 13.6000	Prec@5 35.8500	
Best Prec@1: [13.600]	
Starting epoch number: 1 Learning rate: 0.1
Train: [1]	Time 655.085	Data 0.453	Loss 3.087	Prec@1 23.4020	Prec@5 53.2180	
Val: [1]	Time 40.266	Data 0.166	Loss 4.247	Prec@1 25.6800	Prec@5 56.4100	
Best Prec@1: [25.680]	
Starting epoch number: 2 Learning rate: 0.1
Train: [2]	Time 655.021	Data 0.416	Loss 2.461	Prec@1 35.6560	Prec@5 68.5220	
Val: [2]	Time 40.312	Data 0.173	Loss 3.621	Prec@1 34.2300	Prec@5 64.9600	
Best Prec@1: [34.230]	
Starting epoch number: 3 Learning rate: 0.1
Train: [3]	Time 655.094	Data 0.397	Loss 2.075	Prec@1 44.1200	Prec@5 76.6760	
Val: [3]	Time 40.330	Data 0.162	Loss 2.108	Prec@1 44.9100	Prec@5 77.1500	
Best Prec@1: [44.910]	
Starting epoch number: 4 Learning rate: 0.1
Train: [4]	Time 655.061	Data 0.416	Loss 1.830	Prec@1 49.6280	Prec@5 80.8320	
Val: [4]	Time 40.381	Data 0.179	Loss 2.119	Prec@1 47.0000	Prec@5 78.0400	
Best Prec@1: [47.000]	
Starting epoch number: 5 Learning rate: 0.1
Train: [5]	Time 655.111	Data 0.423	Loss 1.662	Prec@1 53.8400	Prec@5 83.8760	
Val: [5]	Time 40.473	Data 0.205	Loss 1.758	Prec@1 52.8300	Prec@5 82.6100	
Best Prec@1: [52.830]	
Starting epoch number: 6 Learning rate: 0.1
Train: [6]	Time 655.222	Data 0.411	Loss 1.534	Prec@1 56.7540	Prec@5 85.9680	
Val: [6]	Time 40.404	Data 0.154	Loss 1.738	Prec@1 53.6100	Prec@5 83.3200	
Best Prec@1: [53.610]	
Starting epoch number: 7 Learning rate: 0.1
Train: [7]	Time 655.306	Data 0.383	Loss 1.432	Prec@1 59.4520	Prec@5 87.3420	
Val: [7]	Time 40.442	Data 0.153	Loss 1.771	Prec@1 52.8000	Prec@5 82.3900	
Best Prec@1: [53.610]	
Starting epoch number: 8 Learning rate: 0.1
Train: [8]	Time 655.519	Data 0.382	Loss 1.365	Prec@1 61.0840	Prec@5 88.4820	
Val: [8]	Time 40.545	Data 0.186	Loss 1.686	Prec@1 55.0800	Prec@5 84.4800	
Best Prec@1: [55.080]	
Starting epoch number: 9 Learning rate: 0.1
Train: [9]	Time 655.483	Data 0.423	Loss 1.301	Prec@1 62.4660	Prec@5 89.3740	
Val: [9]	Time 40.478	Data 0.152	Loss 1.633	Prec@1 56.2200	Prec@5 85.1100	
Best Prec@1: [56.220]	
Starting epoch number: 10 Learning rate: 0.1
Train: [10]	Time 655.575	Data 0.388	Loss 1.244	Prec@1 64.3140	Prec@5 90.0660	
Val: [10]	Time 40.506	Data 0.163	Loss 1.592	Prec@1 57.9300	Prec@5 85.7600	
Best Prec@1: [57.930]	
Starting epoch number: 11 Learning rate: 0.1
Train: [11]	Time 655.819	Data 0.405	Loss 1.218	Prec@1 64.6400	Prec@5 90.4820	
Val: [11]	Time 40.582	Data 0.209	Loss 1.526	Prec@1 59.1200	Prec@5 86.3800	
Best Prec@1: [59.120]	
Starting epoch number: 12 Learning rate: 0.1
Train: [12]	Time 655.930	Data 0.412	Loss 1.175	Prec@1 66.0700	Prec@5 91.0140	
Val: [12]	Time 40.588	Data 0.174	Loss 1.540	Prec@1 58.9500	Prec@5 86.3100	
Best Prec@1: [59.120]	
Starting epoch number: 13 Learning rate: 0.1
Train: [13]	Time 655.801	Data 0.365	Loss 1.141	Prec@1 66.7580	Prec@5 91.5780	
Val: [13]	Time 40.552	Data 0.197	Loss 1.573	Prec@1 59.4600	Prec@5 86.5900	
Best Prec@1: [59.460]	
Starting epoch number: 14 Learning rate: 0.1
Train: [14]	Time 655.546	Data 0.424	Loss 1.118	Prec@1 67.5300	Prec@5 91.8540	
Val: [14]	Time 40.537	Data 0.193	Loss 1.528	Prec@1 59.1600	Prec@5 87.0400	
Best Prec@1: [59.460]	
Starting epoch number: 15 Learning rate: 0.1
Train: [15]	Time 655.375	Data 0.415	Loss 1.101	Prec@1 67.7560	Prec@5 92.0820	
Val: [15]	Time 40.549	Data 0.153	Loss 1.559	Prec@1 58.9100	Prec@5 86.2600	
Best Prec@1: [59.460]	
Starting epoch number: 16 Learning rate: 0.1
Train: [16]	Time 655.075	Data 0.375	Loss 1.093	Prec@1 67.8960	Prec@5 92.2580	
Val: [16]	Time 40.517	Data 0.189	Loss 1.581	Prec@1 58.9600	Prec@5 86.3700	
Best Prec@1: [59.460]	
Starting epoch number: 17 Learning rate: 0.1
Train: [17]	Time 655.003	Data 0.409	Loss 1.073	Prec@1 68.3040	Prec@5 92.5160	
Val: [17]	Time 40.443	Data 0.175	Loss 1.478	Prec@1 60.6500	Prec@5 87.4600	
Best Prec@1: [60.650]	
Starting epoch number: 18 Learning rate: 0.1
Train: [18]	Time 654.889	Data 0.368	Loss 1.053	Prec@1 68.8380	Prec@5 92.7240	
Val: [18]	Time 40.460	Data 0.187	Loss 1.452	Prec@1 61.5500	Prec@5 87.5800	
Best Prec@1: [61.550]	
Starting epoch number: 19 Learning rate: 0.1
Train: [19]	Time 655.073	Data 0.404	Loss 1.038	Prec@1 69.4700	Prec@5 92.9360	
Val: [19]	Time 40.483	Data 0.190	Loss 1.532	Prec@1 60.4500	Prec@5 86.9100	
Best Prec@1: [61.550]	
Starting epoch number: 20 Learning rate: 0.1
Train: [20]	Time 654.976	Data 0.406	Loss 1.040	Prec@1 69.4960	Prec@5 92.8860	
Val: [20]	Time 40.438	Data 0.146	Loss 1.473	Prec@1 61.2100	Prec@5 88.2800	
Best Prec@1: [61.550]	
Starting epoch number: 21 Learning rate: 0.1
Train: [21]	Time 655.005	Data 0.405	Loss 1.020	Prec@1 69.7700	Prec@5 93.0080	
Val: [21]	Time 40.402	Data 0.169	Loss 1.458	Prec@1 62.0100	Prec@5 87.8700	
Best Prec@1: [62.010]	
Starting epoch number: 22 Learning rate: 0.1
Train: [22]	Time 654.872	Data 0.351	Loss 1.014	Prec@1 69.8300	Prec@5 93.2020	
Val: [22]	Time 40.382	Data 0.149	Loss 1.515	Prec@1 59.9800	Prec@5 86.9500	
Best Prec@1: [62.010]	
Starting epoch number: 23 Learning rate: 0.1
Train: [23]	Time 654.889	Data 0.457	Loss 1.002	Prec@1 70.4300	Prec@5 93.5120	
Val: [23]	Time 40.394	Data 0.174	Loss 1.448	Prec@1 61.4400	Prec@5 88.4500	
Best Prec@1: [62.010]	
Starting epoch number: 24 Learning rate: 0.1
Train: [24]	Time 654.919	Data 0.415	Loss 0.990	Prec@1 70.7040	Prec@5 93.4140	
Val: [24]	Time 40.396	Data 0.181	Loss 1.519	Prec@1 60.7800	Prec@5 87.5300	
Best Prec@1: [62.010]	
Starting epoch number: 25 Learning rate: 0.1
Train: [25]	Time 654.863	Data 0.378	Loss 0.987	Prec@1 70.5820	Prec@5 93.5380	
Val: [25]	Time 40.388	Data 0.161	Loss 1.519	Prec@1 60.3000	Prec@5 87.6900	
Best Prec@1: [62.010]	
Starting epoch number: 26 Learning rate: 0.1
Train: [26]	Time 654.862	Data 0.425	Loss 0.974	Prec@1 70.9960	Prec@5 93.7880	
Val: [26]	Time 40.378	Data 0.173	Loss 1.476	Prec@1 61.8900	Prec@5 88.3300	
Best Prec@1: [62.010]	
Starting epoch number: 27 Learning rate: 0.1
Train: [27]	Time 654.875	Data 0.364	Loss 0.964	Prec@1 71.3740	Prec@5 93.8320	
Val: [27]	Time 40.376	Data 0.175	Loss 1.506	Prec@1 60.7600	Prec@5 87.8300	
Best Prec@1: [62.010]	
Starting epoch number: 28 Learning rate: 0.1
Train: [28]	Time 654.866	Data 0.410	Loss 0.950	Prec@1 71.7280	Prec@5 94.0260	
Val: [28]	Time 40.364	Data 0.154	Loss 1.436	Prec@1 62.3500	Prec@5 88.8400	
Best Prec@1: [62.350]	
Starting epoch number: 29 Learning rate: 0.1
Train: [29]	Time 654.867	Data 0.395	Loss 0.950	Prec@1 71.7620	Prec@5 93.9740	
Val: [29]	Time 40.369	Data 0.194	Loss 1.457	Prec@1 62.4700	Prec@5 88.3200	
Best Prec@1: [62.470]	
Starting epoch number: 30 Learning rate: 0.1
Train: [30]	Time 654.882	Data 0.378	Loss 0.945	Prec@1 71.7720	Prec@5 93.9480	
Val: [30]	Time 40.314	Data 0.142	Loss 1.542	Prec@1 60.7500	Prec@5 87.9400	
Best Prec@1: [62.470]	
Starting epoch number: 31 Learning rate: 0.1
Train: [31]	Time 654.878	Data 0.420	Loss 0.937	Prec@1 71.9880	Prec@5 94.1120	
Val: [31]	Time 40.331	Data 0.168	Loss 1.380	Prec@1 63.8200	Prec@5 88.8400	
Best Prec@1: [63.820]	
Starting epoch number: 32 Learning rate: 0.1
Train: [32]	Time 654.494	Data 0.412	Loss 0.930	Prec@1 72.4000	Prec@5 94.2860	
Val: [32]	Time 40.363	Data 0.169	Loss 1.476	Prec@1 61.6200	Prec@5 87.7000	
Best Prec@1: [63.820]	
Starting epoch number: 33 Learning rate: 0.1
Train: [33]	Time 654.782	Data 0.415	Loss 0.920	Prec@1 72.5000	Prec@5 94.4380	
Val: [33]	Time 40.304	Data 0.169	Loss 1.538	Prec@1 61.3100	Prec@5 87.3600	
Best Prec@1: [63.820]	
Starting epoch number: 34 Learning rate: 0.1
Train: [34]	Time 654.466	Data 0.441	Loss 0.920	Prec@1 72.1320	Prec@5 94.4580	
Val: [34]	Time 40.342	Data 0.146	Loss 1.454	Prec@1 62.1200	Prec@5 88.6300	
Best Prec@1: [63.820]	
Starting epoch number: 35 Learning rate: 0.1
Train: [35]	Time 654.353	Data 0.377	Loss 0.906	Prec@1 72.9260	Prec@5 94.4540	
Val: [35]	Time 40.330	Data 0.175	Loss 1.536	Prec@1 60.5300	Prec@5 87.9600	
Best Prec@1: [63.820]	
Starting epoch number: 36 Learning rate: 0.1
Train: [36]	Time 654.315	Data 0.405	Loss 0.911	Prec@1 72.7680	Prec@5 94.5960	
Val: [36]	Time 40.281	Data 0.149	Loss 1.474	Prec@1 62.4600	Prec@5 88.1600	
Best Prec@1: [63.820]	
Starting epoch number: 37 Learning rate: 0.1
Train: [37]	Time 654.036	Data 0.395	Loss 0.905	Prec@1 72.6900	Prec@5 94.5660	
Val: [37]	Time 40.330	Data 0.166	Loss 1.373	Prec@1 63.2800	Prec@5 89.2900	
Best Prec@1: [63.820]	
Starting epoch number: 38 Learning rate: 0.1
Train: [38]	Time 653.969	Data 0.383	Loss 0.893	Prec@1 73.4580	Prec@5 94.6740	
Val: [38]	Time 40.292	Data 0.157	Loss 1.480	Prec@1 62.2800	Prec@5 88.3400	
Best Prec@1: [63.820]	
Starting epoch number: 39 Learning rate: 0.1
Train: [39]	Time 653.857	Data 0.379	Loss 0.889	Prec@1 73.3040	Prec@5 94.7000	
Val: [39]	Time 40.371	Data 0.235	Loss 1.469	Prec@1 61.7900	Prec@5 88.4900	
Best Prec@1: [63.820]	
Starting epoch number: 40 Learning rate: 0.1
Train: [40]	Time 653.782	Data 0.392	Loss 0.879	Prec@1 73.6600	Prec@5 94.7360	
Val: [40]	Time 40.290	Data 0.155	Loss 1.388	Prec@1 64.1500	Prec@5 89.0200	
Best Prec@1: [64.150]	
Starting epoch number: 41 Learning rate: 0.1
Train: [41]	Time 653.778	Data 0.388	Loss 0.880	Prec@1 73.6680	Prec@5 94.7960	
Val: [41]	Time 40.254	Data 0.145	Loss 1.362	Prec@1 64.0200	Prec@5 89.4100	
Best Prec@1: [64.150]	
Starting epoch number: 42 Learning rate: 0.1
Train: [42]	Time 653.754	Data 0.384	Loss 0.870	Prec@1 73.8660	Prec@5 94.9920	
Val: [42]	Time 40.283	Data 0.172	Loss 1.500	Prec@1 61.5600	Prec@5 87.7800	
Best Prec@1: [64.150]	
Starting epoch number: 43 Learning rate: 0.1
Train: [43]	Time 653.676	Data 0.376	Loss 0.874	Prec@1 73.6740	Prec@5 94.8440	
Val: [43]	Time 40.289	Data 0.188	Loss 1.405	Prec@1 63.2000	Prec@5 89.2800	
Best Prec@1: [64.150]	
Starting epoch number: 44 Learning rate: 0.1
Train: [44]	Time 653.513	Data 0.374	Loss 0.871	Prec@1 73.6480	Prec@5 94.8880	
Val: [44]	Time 40.240	Data 0.143	Loss 1.466	Prec@1 62.2500	Prec@5 88.2800	
Best Prec@1: [64.150]	
Starting epoch number: 45 Learning rate: 0.1
Train: [45]	Time 653.465	Data 0.389	Loss 0.865	Prec@1 73.9700	Prec@5 94.9440	
Val: [45]	Time 40.262	Data 0.163	Loss 1.587	Prec@1 60.5700	Prec@5 87.1300	
Best Prec@1: [64.150]	
Starting epoch number: 46 Learning rate: 0.1
Train: [46]	Time 653.277	Data 0.361	Loss 0.859	Prec@1 74.3040	Prec@5 95.0280	
Val: [46]	Time 40.240	Data 0.154	Loss 1.435	Prec@1 63.1100	Prec@5 88.5800	
Best Prec@1: [64.150]	
Starting epoch number: 47 Learning rate: 0.1
Train: [47]	Time 653.352	Data 0.413	Loss 0.854	Prec@1 74.3360	Prec@5 94.9680	
Val: [47]	Time 40.295	Data 0.201	Loss 1.499	Prec@1 62.3400	Prec@5 88.1900	
Best Prec@1: [64.150]	
Starting epoch number: 48 Learning rate: 0.1
Train: [48]	Time 653.284	Data 0.489	Loss 0.859	Prec@1 74.0760	Prec@5 95.0400	
Val: [48]	Time 40.271	Data 0.172	Loss 1.475	Prec@1 62.1600	Prec@5 88.8200	
Best Prec@1: [64.150]	
Starting epoch number: 49 Learning rate: 0.1
Train: [49]	Time 653.150	Data 0.421	Loss 0.852	Prec@1 74.4280	Prec@5 95.1080	
Val: [49]	Time 40.274	Data 0.173	Loss 1.421	Prec@1 63.5300	Prec@5 88.5900	
Best Prec@1: [64.150]	
Starting epoch number: 50 Learning rate: 0.1
Train: [50]	Time 653.039	Data 0.404	Loss 0.851	Prec@1 74.3340	Prec@5 95.0320	
Val: [50]	Time 40.280	Data 0.158	Loss 1.375	Prec@1 65.0100	Prec@5 89.4600	
Best Prec@1: [65.010]	
Starting epoch number: 51 Learning rate: 0.1
Train: [51]	Time 652.997	Data 0.393	Loss 0.840	Prec@1 74.3860	Prec@5 95.2260	
Val: [51]	Time 40.262	Data 0.145	Loss 1.525	Prec@1 61.7500	Prec@5 87.6200	
Best Prec@1: [65.010]	
Starting epoch number: 52 Learning rate: 0.1
Train: [52]	Time 652.946	Data 0.387	Loss 0.846	Prec@1 74.2000	Prec@5 95.2480	
Val: [52]	Time 40.274	Data 0.161	Loss 1.422	Prec@1 63.3200	Prec@5 89.1000	
Best Prec@1: [65.010]	
Starting epoch number: 53 Learning rate: 0.1
Train: [53]	Time 652.774	Data 0.368	Loss 0.839	Prec@1 74.7060	Prec@5 95.4180	
Val: [53]	Time 40.241	Data 0.147	Loss 1.462	Prec@1 63.3400	Prec@5 88.5700	
Best Prec@1: [65.010]	
Starting epoch number: 54 Learning rate: 0.1
Train: [54]	Time 652.719	Data 0.397	Loss 0.832	Prec@1 74.7960	Prec@5 95.3300	
Val: [54]	Time 40.304	Data 0.198	Loss 1.366	Prec@1 64.9900	Prec@5 89.6600	
Best Prec@1: [65.010]	
Starting epoch number: 55 Learning rate: 0.1
Train: [55]	Time 652.622	Data 0.399	Loss 0.833	Prec@1 74.6900	Prec@5 95.3620	
Val: [55]	Time 40.231	Data 0.180	Loss 1.352	Prec@1 64.5700	Prec@5 89.3200	
Best Prec@1: [65.010]	
Starting epoch number: 56 Learning rate: 0.1
Train: [56]	Time 652.414	Data 0.386	Loss 0.823	Prec@1 74.9280	Prec@5 95.4920	
Val: [56]	Time 40.229	Data 0.198	Loss 1.469	Prec@1 63.3700	Prec@5 88.5200	
Best Prec@1: [65.010]	
Starting epoch number: 57 Learning rate: 0.1
Train: [57]	Time 652.409	Data 0.407	Loss 0.827	Prec@1 74.9080	Prec@5 95.4720	
Val: [57]	Time 40.343	Data 0.199	Loss 1.482	Prec@1 63.4100	Prec@5 88.5700	
Best Prec@1: [65.010]	
Starting epoch number: 58 Learning rate: 0.1
Train: [58]	Time 652.439	Data 0.392	Loss 0.821	Prec@1 75.1980	Prec@5 95.5820	
Val: [58]	Time 40.227	Data 0.181	Loss 1.426	Prec@1 63.8100	Prec@5 88.9700	
Best Prec@1: [65.010]	
Starting epoch number: 59 Learning rate: 0.1
Train: [59]	Time 652.252	Data 0.393	Loss 0.818	Prec@1 75.5280	Prec@5 95.5240	
Val: [59]	Time 40.189	Data 0.156	Loss 1.427	Prec@1 63.7300	Prec@5 89.1300	
Best Prec@1: [65.010]	
Starting epoch number: 60 Learning rate: 0.1
Train: [60]	Time 652.061	Data 0.410	Loss 0.821	Prec@1 75.1000	Prec@5 95.4060	
Val: [60]	Time 40.178	Data 0.141	Loss 1.508	Prec@1 62.3900	Prec@5 88.6000	
Best Prec@1: [65.010]	
Starting epoch number: 61 Learning rate: 0.1
Train: [61]	Time 651.976	Data 0.393	Loss 0.812	Prec@1 75.3720	Prec@5 95.4980	
Val: [61]	Time 40.190	Data 0.165	Loss 1.424	Prec@1 64.4300	Prec@5 88.9300	
Best Prec@1: [65.010]	
Starting epoch number: 62 Learning rate: 0.1
Train: [62]	Time 651.858	Data 0.435	Loss 0.813	Prec@1 75.3920	Prec@5 95.5040	
Val: [62]	Time 40.190	Data 0.173	Loss 1.457	Prec@1 63.8100	Prec@5 89.3000	
Best Prec@1: [65.010]	
Starting epoch number: 63 Learning rate: 0.1
Train: [63]	Time 651.765	Data 0.386	Loss 0.813	Prec@1 75.3760	Prec@5 95.5980	
Val: [63]	Time 40.187	Data 0.171	Loss 1.444	Prec@1 64.1300	Prec@5 89.0100	
Best Prec@1: [65.010]	
Starting epoch number: 64 Learning rate: 0.1
Train: [64]	Time 651.668	Data 0.411	Loss 0.807	Prec@1 75.3920	Prec@5 95.6340	
Val: [64]	Time 40.179	Data 0.171	Loss 1.495	Prec@1 63.0000	Prec@5 88.8700	
Best Prec@1: [65.010]	
Starting epoch number: 65 Learning rate: 0.1
Train: [65]	Time 651.542	Data 0.348	Loss 0.804	Prec@1 75.5000	Prec@5 95.6200	
Val: [65]	Time 40.207	Data 0.204	Loss 1.385	Prec@1 64.5800	Prec@5 89.3200	
Best Prec@1: [65.010]	
Starting epoch number: 66 Learning rate: 0.1
Train: [66]	Time 651.568	Data 0.387	Loss 0.806	Prec@1 75.6540	Prec@5 95.5720	
Val: [66]	Time 40.157	Data 0.155	Loss 1.489	Prec@1 62.6600	Prec@5 88.3000	
Best Prec@1: [65.010]	
Starting epoch number: 67 Learning rate: 0.1
Train: [67]	Time 651.580	Data 0.416	Loss 0.803	Prec@1 75.6460	Prec@5 95.6820	
Val: [67]	Time 40.191	Data 0.195	Loss 1.496	Prec@1 62.9700	Prec@5 88.0000	
Best Prec@1: [65.010]	
Starting epoch number: 68 Learning rate: 0.1
Train: [68]	Time 651.446	Data 0.394	Loss 0.807	Prec@1 75.4960	Prec@5 95.6120	
Val: [68]	Time 40.193	Data 0.205	Loss 1.448	Prec@1 63.6100	Prec@5 88.4000	
Best Prec@1: [65.010]	
Starting epoch number: 69 Learning rate: 0.1
Train: [69]	Time 651.572	Data 0.391	Loss 0.799	Prec@1 75.7440	Prec@5 95.7040	
Val: [69]	Time 40.237	Data 0.186	Loss 1.446	Prec@1 63.3100	Prec@5 88.9400	
Best Prec@1: [65.010]	
Starting epoch number: 70 Learning rate: 0.1
Train: [70]	Time 651.472	Data 0.354	Loss 0.795	Prec@1 75.7800	Prec@5 95.7180	
Val: [70]	Time 40.159	Data 0.164	Loss 1.354	Prec@1 65.4000	Prec@5 89.3900	
Best Prec@1: [65.400]	
Starting epoch number: 71 Learning rate: 0.1
Train: [71]	Time 651.301	Data 0.388	Loss 0.797	Prec@1 75.6920	Prec@5 95.5880	
Val: [71]	Time 40.160	Data 0.153	Loss 1.422	Prec@1 63.5700	Prec@5 89.1200	
Best Prec@1: [65.400]	
Starting epoch number: 72 Learning rate: 0.1
Train: [72]	Time 651.286	Data 0.400	Loss 0.792	Prec@1 75.8680	Prec@5 95.7680	
Val: [72]	Time 40.171	Data 0.178	Loss 1.465	Prec@1 63.4500	Prec@5 88.3400	
Best Prec@1: [65.400]	
Starting epoch number: 73 Learning rate: 0.1
Train: [73]	Time 651.188	Data 0.361	Loss 0.789	Prec@1 75.9400	Prec@5 95.7780	
Val: [73]	Time 40.165	Data 0.153	Loss 1.362	Prec@1 65.1200	Prec@5 89.6700	
Best Prec@1: [65.400]	
Starting epoch number: 74 Learning rate: 0.1
Train: [74]	Time 651.132	Data 0.385	Loss 0.794	Prec@1 75.8800	Prec@5 95.7320	
Val: [74]	Time 40.190	Data 0.190	Loss 1.401	Prec@1 64.9200	Prec@5 89.3900	
Best Prec@1: [65.400]	
Starting epoch number: 75 Learning rate: 0.1
Train: [75]	Time 651.437	Data 0.374	Loss 0.791	Prec@1 75.9800	Prec@5 95.7360	
Val: [75]	Time 40.200	Data 0.191	Loss 1.455	Prec@1 63.4000	Prec@5 88.7700	
Best Prec@1: [65.400]	
Starting epoch number: 76 Learning rate: 0.1
Train: [76]	Time 651.286	Data 0.414	Loss 0.783	Prec@1 76.2120	Prec@5 95.8340	
Val: [76]	Time 40.164	Data 0.156	Loss 1.320	Prec@1 66.2900	Prec@5 90.1100	
Best Prec@1: [66.290]	
Starting epoch number: 77 Learning rate: 0.1
Train: [77]	Time 651.257	Data 0.406	Loss 0.792	Prec@1 75.9860	Prec@5 95.8360	
Val: [77]	Time 40.174	Data 0.166	Loss 1.444	Prec@1 63.7900	Prec@5 88.8200	
Best Prec@1: [66.290]	
Starting epoch number: 78 Learning rate: 0.1
Train: [78]	Time 651.178	Data 0.381	Loss 0.789	Prec@1 76.1500	Prec@5 95.8420	
Val: [78]	Time 40.165	Data 0.154	Loss 1.465	Prec@1 63.6000	Prec@5 88.9400	
Best Prec@1: [66.290]	
Starting epoch number: 79 Learning rate: 0.1
Train: [79]	Time 651.087	Data 0.411	Loss 0.783	Prec@1 76.0820	Prec@5 95.8740	
Val: [79]	Time 40.202	Data 0.185	Loss 1.449	Prec@1 63.5100	Prec@5 89.0500	
Best Prec@1: [66.290]	
Starting epoch number: 80 Learning rate: 0.1
Train: [80]	Time 651.022	Data 0.382	Loss 0.777	Prec@1 76.5680	Prec@5 96.0260	
Val: [80]	Time 40.187	Data 0.174	Loss 1.408	Prec@1 64.3200	Prec@5 89.6100	
Best Prec@1: [66.290]	
Starting epoch number: 81 Learning rate: 0.1
Train: [81]	Time 650.949	Data 0.399	Loss 0.784	Prec@1 76.1700	Prec@5 95.9000	
Val: [81]	Time 40.138	Data 0.154	Loss 1.404	Prec@1 64.0500	Prec@5 89.0900	
Best Prec@1: [66.290]	
Starting epoch number: 82 Learning rate: 0.1
Train: [82]	Time 650.841	Data 0.392	Loss 0.788	Prec@1 76.1960	Prec@5 95.8660	
Val: [82]	Time 40.167	Data 0.165	Loss 1.499	Prec@1 63.0400	Prec@5 87.2400	
Best Prec@1: [66.290]	
Starting epoch number: 83 Learning rate: 0.1
Train: [83]	Time 650.845	Data 0.406	Loss 0.776	Prec@1 76.3520	Prec@5 95.9480	
Val: [83]	Time 40.191	Data 0.186	Loss 1.441	Prec@1 63.8000	Prec@5 88.4200	
Best Prec@1: [66.290]	
Starting epoch number: 84 Learning rate: 0.1
Train: [84]	Time 650.727	Data 0.379	Loss 0.774	Prec@1 76.1220	Prec@5 95.9120	
Val: [84]	Time 40.154	Data 0.151	Loss 1.516	Prec@1 61.7700	Prec@5 87.6100	
Best Prec@1: [66.290]	
Starting epoch number: 85 Learning rate: 0.1
Train: [85]	Time 650.685	Data 0.416	Loss 0.777	Prec@1 76.3440	Prec@5 95.9520	
Val: [85]	Time 40.164	Data 0.168	Loss 1.349	Prec@1 65.7900	Prec@5 89.9500	
Best Prec@1: [66.290]	
Starting epoch number: 86 Learning rate: 0.1
Train: [86]	Time 650.686	Data 0.420	Loss 0.773	Prec@1 76.5400	Prec@5 96.0040	
Val: [86]	Time 40.152	Data 0.153	Loss 1.379	Prec@1 64.9500	Prec@5 89.7000	
Best Prec@1: [66.290]	
Starting epoch number: 87 Learning rate: 0.1
Train: [87]	Time 650.569	Data 0.367	Loss 0.775	Prec@1 76.2360	Prec@5 95.9480	
Val: [87]	Time 40.169	Data 0.171	Loss 1.373	Prec@1 64.8100	Prec@5 89.5700	
Best Prec@1: [66.290]	
Starting epoch number: 88 Learning rate: 0.1
Train: [88]	Time 650.537	Data 0.400	Loss 0.766	Prec@1 76.7340	Prec@5 95.8940	
Val: [88]	Time 40.172	Data 0.167	Loss 1.510	Prec@1 62.6400	Prec@5 88.5200	
Best Prec@1: [66.290]	
Starting epoch number: 89 Learning rate: 0.1
Train: [89]	Time 650.505	Data 0.404	Loss 0.771	Prec@1 76.4740	Prec@5 96.0180	
Val: [89]	Time 40.149	Data 0.151	Loss 1.598	Prec@1 61.7500	Prec@5 87.9800	
Best Prec@1: [66.290]	
Starting epoch number: 90 Learning rate: 0.1
Train: [90]	Time 650.480	Data 0.420	Loss 0.772	Prec@1 76.5200	Prec@5 95.9580	
Val: [90]	Time 40.138	Data 0.161	Loss 1.404	Prec@1 64.1800	Prec@5 89.1400	
Best Prec@1: [66.290]	
Starting epoch number: 91 Learning rate: 0.1
Train: [91]	Time 650.380	Data 0.406	Loss 0.768	Prec@1 76.5040	Prec@5 96.0700	
Val: [91]	Time 40.169	Data 0.170	Loss 1.426	Prec@1 63.8800	Prec@5 89.1300	
Best Prec@1: [66.290]	
Starting epoch number: 92 Learning rate: 0.1
Train: [92]	Time 650.387	Data 0.454	Loss 0.768	Prec@1 76.6120	Prec@5 96.1540	
Val: [92]	Time 40.139	Data 0.166	Loss 1.365	Prec@1 65.2400	Prec@5 89.6500	
Best Prec@1: [66.290]	
Starting epoch number: 93 Learning rate: 0.1
Train: [93]	Time 650.343	Data 0.430	Loss 0.761	Prec@1 76.7460	Prec@5 96.0340	
Val: [93]	Time 40.168	Data 0.191	Loss 1.467	Prec@1 63.6000	Prec@5 88.7200	
Best Prec@1: [66.290]	
Starting epoch number: 94 Learning rate: 0.1
Train: [94]	Time 650.242	Data 0.387	Loss 0.763	Prec@1 76.6420	Prec@5 95.9880	
Val: [94]	Time 40.154	Data 0.165	Loss 1.465	Prec@1 62.9400	Prec@5 88.9000	
Best Prec@1: [66.290]	
Starting epoch number: 95 Learning rate: 0.1
Train: [95]	Time 650.161	Data 0.432	Loss 0.768	Prec@1 76.7580	Prec@5 95.9420	
Val: [95]	Time 40.148	Data 0.153	Loss 1.398	Prec@1 64.0900	Prec@5 88.9700	
Best Prec@1: [66.290]	
Starting epoch number: 96 Learning rate: 0.1
Train: [96]	Time 650.044	Data 0.376	Loss 0.761	Prec@1 76.7900	Prec@5 96.0380	
Val: [96]	Time 40.151	Data 0.171	Loss 1.464	Prec@1 63.2800	Prec@5 88.9100	
Best Prec@1: [66.290]	
Starting epoch number: 97 Learning rate: 0.1
Train: [97]	Time 650.014	Data 0.415	Loss 0.765	Prec@1 76.6620	Prec@5 96.0540	
Val: [97]	Time 40.153	Data 0.181	Loss 1.480	Prec@1 63.8100	Prec@5 88.4900	
Best Prec@1: [66.290]	
Starting epoch number: 98 Learning rate: 0.1
Train: [98]	Time 649.842	Data 0.378	Loss 0.756	Prec@1 77.0180	Prec@5 95.9400	
Val: [98]	Time 40.173	Data 0.206	Loss 1.494	Prec@1 63.4700	Prec@5 88.3500	
Best Prec@1: [66.290]	
Starting epoch number: 99 Learning rate: 0.1
Train: [99]	Time 649.691	Data 0.414	Loss 0.763	Prec@1 76.6260	Prec@5 96.1740	
Val: [99]	Time 40.142	Data 0.177	Loss 1.511	Prec@1 62.9100	Prec@5 88.5700	
Best Prec@1: [66.290]	
Starting epoch number: 100 Learning rate: 0.1
Train: [100]	Time 649.676	Data 0.399	Loss 0.763	Prec@1 76.7280	Prec@5 96.0680	
Val: [100]	Time 40.134	Data 0.175	Loss 1.598	Prec@1 61.5100	Prec@5 87.6700	
Best Prec@1: [66.290]	
Starting epoch number: 101 Learning rate: 0.1
Train: [101]	Time 649.539	Data 0.402	Loss 0.759	Prec@1 76.9520	Prec@5 96.1420	
Val: [101]	Time 40.141	Data 0.178	Loss 1.478	Prec@1 62.6200	Prec@5 88.5900	
Best Prec@1: [66.290]	
Starting epoch number: 102 Learning rate: 0.1
Train: [102]	Time 649.390	Data 0.369	Loss 0.758	Prec@1 76.7920	Prec@5 96.1440	
Val: [102]	Time 40.123	Data 0.162	Loss 1.449	Prec@1 63.4000	Prec@5 89.1000	
Best Prec@1: [66.290]	
Starting epoch number: 103 Learning rate: 0.1
Train: [103]	Time 649.387	Data 0.422	Loss 0.752	Prec@1 76.9140	Prec@5 96.2840	
Val: [103]	Time 40.164	Data 0.209	Loss 1.353	Prec@1 64.9600	Prec@5 89.7500	
Best Prec@1: [66.290]	
Starting epoch number: 104 Learning rate: 0.1
Train: [104]	Time 649.405	Data 0.381	Loss 0.753	Prec@1 77.1140	Prec@5 96.1160	
Val: [104]	Time 40.120	Data 0.184	Loss 1.435	Prec@1 63.6700	Prec@5 89.1100	
Best Prec@1: [66.290]	
Starting epoch number: 105 Learning rate: 0.1
Train: [105]	Time 649.440	Data 0.415	Loss 0.751	Prec@1 77.0740	Prec@5 96.1800	
Val: [105]	Time 40.125	Data 0.173	Loss 1.394	Prec@1 65.1100	Prec@5 89.7500	
Best Prec@1: [66.290]	
Starting epoch number: 106 Learning rate: 0.1
Train: [106]	Time 649.317	Data 0.448	Loss 0.756	Prec@1 76.9380	Prec@5 96.1140	
Val: [106]	Time 40.083	Data 0.150	Loss 1.395	Prec@1 64.8000	Prec@5 89.2700	
Best Prec@1: [66.290]	
Starting epoch number: 107 Learning rate: 0.1
Train: [107]	Time 649.286	Data 0.385	Loss 0.748	Prec@1 77.3080	Prec@5 96.1840	
Val: [107]	Time 40.106	Data 0.185	Loss 1.414	Prec@1 64.6600	Prec@5 89.8400	
Best Prec@1: [66.290]	
Starting epoch number: 108 Learning rate: 0.1
Train: [108]	Time 649.142	Data 0.369	Loss 0.743	Prec@1 77.1380	Prec@5 96.1080	
Val: [108]	Time 40.144	Data 0.202	Loss 1.465	Prec@1 64.4400	Prec@5 88.8700	
Best Prec@1: [66.290]	
Starting epoch number: 109 Learning rate: 0.1
Train: [109]	Time 649.167	Data 0.394	Loss 0.753	Prec@1 76.9460	Prec@5 96.1840	
Val: [109]	Time 40.099	Data 0.159	Loss 1.451	Prec@1 63.7200	Prec@5 88.5900	
Best Prec@1: [66.290]	
Starting epoch number: 110 Learning rate: 0.1
Train: [110]	Time 649.092	Data 0.395	Loss 0.745	Prec@1 77.2260	Prec@5 96.1720	
Val: [110]	Time 40.114	Data 0.187	Loss 1.458	Prec@1 63.8300	Prec@5 88.8000	
Best Prec@1: [66.290]	
Starting epoch number: 111 Learning rate: 0.1
Train: [111]	Time 649.020	Data 0.409	Loss 0.746	Prec@1 77.1820	Prec@5 96.2680	
Val: [111]	Time 40.088	Data 0.168	Loss 1.383	Prec@1 65.3300	Prec@5 89.5400	
Best Prec@1: [66.290]	
Starting epoch number: 112 Learning rate: 0.1
Train: [112]	Time 649.094	Data 0.438	Loss 0.746	Prec@1 77.2680	Prec@5 96.3540	
Val: [112]	Time 40.258	Data 0.191	Loss 1.463	Prec@1 64.1100	Prec@5 89.1600	
Best Prec@1: [66.290]	
Starting epoch number: 113 Learning rate: 0.1
Train: [113]	Time 649.362	Data 0.388	Loss 0.743	Prec@1 77.4340	Prec@5 96.2520	
Val: [113]	Time 40.128	Data 0.176	Loss 1.352	Prec@1 65.8800	Prec@5 89.4000	
Best Prec@1: [66.290]	
Starting epoch number: 114 Learning rate: 0.1
Train: [114]	Time 648.945	Data 0.405	Loss 0.746	Prec@1 77.2740	Prec@5 96.1560	
Val: [114]	Time 40.081	Data 0.157	Loss 1.464	Prec@1 63.4100	Prec@5 88.6900	
Best Prec@1: [66.290]	
Starting epoch number: 115 Learning rate: 0.1
Train: [115]	Time 648.989	Data 0.438	Loss 0.745	Prec@1 77.3480	Prec@5 96.1000	
Val: [115]	Time 40.079	Data 0.159	Loss 1.352	Prec@1 65.8200	Prec@5 89.4700	
Best Prec@1: [66.290]	
Starting epoch number: 116 Learning rate: 0.1
Train: [116]	Time 648.779	Data 0.373	Loss 0.748	Prec@1 77.0740	Prec@5 96.2220	
Val: [116]	Time 40.087	Data 0.163	Loss 1.402	Prec@1 64.8700	Prec@5 89.7300	
Best Prec@1: [66.290]	
Starting epoch number: 117 Learning rate: 0.1
Train: [117]	Time 648.826	Data 0.355	Loss 0.746	Prec@1 77.2800	Prec@5 96.1700	
Val: [117]	Time 40.092	Data 0.171	Loss 1.392	Prec@1 65.4900	Prec@5 89.5100	
Best Prec@1: [66.290]	
Starting epoch number: 118 Learning rate: 0.1
Train: [118]	Time 648.807	Data 0.420	Loss 0.744	Prec@1 77.3200	Prec@5 96.2180	
Val: [118]	Time 40.104	Data 0.178	Loss 1.385	Prec@1 65.7400	Prec@5 89.8100	
Best Prec@1: [66.290]	
Starting epoch number: 119 Learning rate: 0.1
Train: [119]	Time 648.868	Data 0.420	Loss 0.744	Prec@1 77.1300	Prec@5 96.2820	
Val: [119]	Time 40.134	Data 0.210	Loss 1.490	Prec@1 63.5000	Prec@5 88.6200	
Best Prec@1: [66.290]	
Starting epoch number: 120 Learning rate: 0.1
Train: [120]	Time 648.827	Data 0.358	Loss 0.747	Prec@1 77.1120	Prec@5 96.1880	
Val: [120]	Time 40.082	Data 0.145	Loss 1.420	Prec@1 65.0700	Prec@5 89.2400	
Best Prec@1: [66.290]	
Starting epoch number: 121 Learning rate: 0.1
Train: [121]	Time 648.847	Data 0.407	Loss 0.741	Prec@1 77.4020	Prec@5 96.2660	
Val: [121]	Time 40.106	Data 0.148	Loss 1.466	Prec@1 63.8600	Prec@5 88.8000	
Best Prec@1: [66.290]	
Starting epoch number: 122 Learning rate: 0.1
Train: [122]	Time 648.793	Data 0.370	Loss 0.740	Prec@1 77.4300	Prec@5 96.2900	
Val: [122]	Time 40.097	Data 0.141	Loss 1.331	Prec@1 65.9700	Prec@5 90.0700	
Best Prec@1: [66.290]	
Starting epoch number: 123 Learning rate: 0.1
Train: [123]	Time 648.763	Data 0.365	Loss 0.734	Prec@1 77.5560	Prec@5 96.2480	
Val: [123]	Time 40.179	Data 0.194	Loss 1.461	Prec@1 64.1100	Prec@5 88.7500	
Best Prec@1: [66.290]	
Starting epoch number: 124 Learning rate: 0.1
Train: [124]	Time 648.770	Data 0.368	Loss 0.740	Prec@1 77.2880	Prec@5 96.3460	
Val: [124]	Time 40.142	Data 0.157	Loss 1.369	Prec@1 65.4000	Prec@5 89.6900	
Best Prec@1: [66.290]	
Starting epoch number: 125 Learning rate: 0.1
Train: [125]	Time 648.798	Data 0.393	Loss 0.737	Prec@1 77.3720	Prec@5 96.3220	
Val: [125]	Time 40.175	Data 0.172	Loss 1.442	Prec@1 63.9300	Prec@5 88.9400	
Best Prec@1: [66.290]	
Starting epoch number: 126 Learning rate: 0.1
Train: [126]	Time 648.764	Data 0.375	Loss 0.740	Prec@1 77.2780	Prec@5 96.3200	
Val: [126]	Time 40.133	Data 0.146	Loss 1.575	Prec@1 62.3000	Prec@5 87.1100	
Best Prec@1: [66.290]	
Starting epoch number: 127 Learning rate: 0.1
Train: [127]	Time 648.738	Data 0.360	Loss 0.736	Prec@1 77.3940	Prec@5 96.3480	
Val: [127]	Time 40.164	Data 0.191	Loss 1.424	Prec@1 64.3300	Prec@5 88.8300	
Best Prec@1: [66.290]	
Starting epoch number: 128 Learning rate: 0.1
Train: [128]	Time 648.753	Data 0.370	Loss 0.739	Prec@1 77.2360	Prec@5 96.2880	
Val: [128]	Time 40.231	Data 0.141	Loss 1.371	Prec@1 65.2000	Prec@5 89.4800	
Best Prec@1: [66.290]	
Starting epoch number: 129 Learning rate: 0.1
Train: [129]	Time 648.727	Data 0.370	Loss 0.737	Prec@1 77.4440	Prec@5 96.2480	
Val: [129]	Time 40.203	Data 0.150	Loss 1.427	Prec@1 65.0000	Prec@5 89.7500	
Best Prec@1: [66.290]	
Starting epoch number: 130 Learning rate: 0.1
Train: [130]	Time 648.798	Data 0.420	Loss 0.749	Prec@1 77.1180	Prec@5 96.1980	
Val: [130]	Time 40.336	Data 0.213	Loss 1.402	Prec@1 63.8400	Prec@5 89.0000	
Best Prec@1: [66.290]	
Starting epoch number: 131 Learning rate: 0.1
Train: [131]	Time 648.922	Data 0.394	Loss 0.747	Prec@1 77.0300	Prec@5 96.2880	
Val: [131]	Time 40.213	Data 0.153	Loss 1.351	Prec@1 65.2200	Prec@5 89.2700	
Best Prec@1: [66.290]	
Starting epoch number: 132 Learning rate: 0.1
Train: [132]	Time 648.932	Data 0.412	Loss 0.743	Prec@1 77.1700	Prec@5 96.2360	
Val: [132]	Time 40.246	Data 0.172	Loss 1.440	Prec@1 64.0700	Prec@5 89.1000	
Best Prec@1: [66.290]	
Starting epoch number: 133 Learning rate: 0.1
Train: [133]	Time 649.058	Data 0.417	Loss 0.740	Prec@1 77.3620	Prec@5 96.2320	
Val: [133]	Time 40.294	Data 0.183	Loss 1.425	Prec@1 64.7700	Prec@5 89.1100	
Best Prec@1: [66.290]	
Starting epoch number: 134 Learning rate: 0.1
Train: [134]	Time 649.088	Data 0.412	Loss 0.729	Prec@1 77.6660	Prec@5 96.3920	
Val: [134]	Time 40.311	Data 0.176	Loss 1.346	Prec@1 65.4500	Prec@5 89.7100	
Best Prec@1: [66.290]	
Starting epoch number: 135 Learning rate: 0.1
Train: [135]	Time 649.170	Data 0.392	Loss 0.739	Prec@1 77.5500	Prec@5 96.3520	
Val: [135]	Time 40.247	Data 0.174	Loss 1.417	Prec@1 64.1500	Prec@5 89.4400	
Best Prec@1: [66.290]	
Starting epoch number: 136 Learning rate: 0.1
Train: [136]	Time 649.051	Data 0.357	Loss 0.734	Prec@1 77.3720	Prec@5 96.4240	
Val: [136]	Time 40.302	Data 0.169	Loss 1.392	Prec@1 65.8500	Prec@5 89.4700	
Best Prec@1: [66.290]	
Starting epoch number: 137 Learning rate: 0.1
Train: [137]	Time 649.311	Data 0.392	Loss 0.735	Prec@1 77.5080	Prec@5 96.3660	
Val: [137]	Time 40.290	Data 0.169	Loss 1.354	Prec@1 65.9900	Prec@5 89.6900	
Best Prec@1: [66.290]	
Starting epoch number: 138 Learning rate: 0.1
Train: [138]	Time 649.255	Data 0.402	Loss 0.739	Prec@1 77.5300	Prec@5 96.1880	
Val: [138]	Time 40.235	Data 0.157	Loss 1.394	Prec@1 64.8700	Prec@5 89.5700	
Best Prec@1: [66.290]	
Starting epoch number: 139 Learning rate: 0.1
Train: [139]	Time 648.865	Data 0.393	Loss 0.733	Prec@1 77.5720	Prec@5 96.2420	
Val: [139]	Time 40.236	Data 0.167	Loss 1.391	Prec@1 65.5600	Prec@5 89.6900	
Best Prec@1: [66.290]	
Starting epoch number: 140 Learning rate: 0.1
Train: [140]	Time 648.784	Data 0.448	Loss 0.729	Prec@1 77.6520	Prec@5 96.4600	
Val: [140]	Time 40.221	Data 0.141	Loss 1.402	Prec@1 65.7700	Prec@5 89.3800	
Best Prec@1: [66.290]	
Starting epoch number: 141 Learning rate: 0.1
Train: [141]	Time 648.427	Data 0.388	Loss 0.742	Prec@1 77.1060	Prec@5 96.3240	
Val: [141]	Time 40.238	Data 0.168	Loss 1.455	Prec@1 64.3900	Prec@5 89.5900	
Best Prec@1: [66.290]	
Starting epoch number: 142 Learning rate: 0.1
Train: [142]	Time 648.311	Data 0.353	Loss 0.737	Prec@1 77.4400	Prec@5 96.3540	
Val: [142]	Time 40.906	Data 0.936	Loss 1.431	Prec@1 65.0000	Prec@5 89.2700	
Best Prec@1: [66.290]	
Starting epoch number: 143 Learning rate: 0.1
Train: [143]	Time 648.367	Data 0.393	Loss 0.730	Prec@1 77.8260	Prec@5 96.3760	
Val: [143]	Time 40.171	Data 0.138	Loss 1.393	Prec@1 65.1500	Prec@5 89.3600	
Best Prec@1: [66.290]	
Starting epoch number: 144 Learning rate: 0.1
Train: [144]	Time 648.294	Data 0.383	Loss 0.732	Prec@1 77.5140	Prec@5 96.4220	
Val: [144]	Time 40.194	Data 0.167	Loss 1.401	Prec@1 64.3800	Prec@5 89.1100	
Best Prec@1: [66.290]	
Starting epoch number: 145 Learning rate: 0.1
Train: [145]	Time 648.275	Data 0.388	Loss 0.731	Prec@1 77.7020	Prec@5 96.3080	
Val: [145]	Time 40.169	Data 0.151	Loss 1.413	Prec@1 65.0700	Prec@5 90.0400	
Best Prec@1: [66.290]	
Starting epoch number: 146 Learning rate: 0.1
Train: [146]	Time 648.262	Data 0.426	Loss 0.723	Prec@1 77.7460	Prec@5 96.4420	
Val: [146]	Time 40.176	Data 0.170	Loss 1.407	Prec@1 64.7200	Prec@5 89.1700	
Best Prec@1: [66.290]	
Starting epoch number: 147 Learning rate: 0.1
Train: [147]	Time 648.094	Data 0.383	Loss 0.743	Prec@1 77.4820	Prec@5 96.1860	
Val: [147]	Time 40.123	Data 0.160	Loss 1.370	Prec@1 65.3700	Prec@5 89.3200	
Best Prec@1: [66.290]	
Starting epoch number: 148 Learning rate: 0.1
Train: [148]	Time 648.037	Data 0.401	Loss 0.730	Prec@1 77.5800	Prec@5 96.2780	
Val: [148]	Time 40.141	Data 0.141	Loss 1.414	Prec@1 63.8300	Prec@5 89.0800	
Best Prec@1: [66.290]	
Starting epoch number: 149 Learning rate: 0.1
Train: [149]	Time 647.936	Data 0.400	Loss 0.722	Prec@1 77.9200	Prec@5 96.4940	
Val: [149]	Time 40.198	Data 0.200	Loss 1.379	Prec@1 65.5600	Prec@5 89.6000	
Best Prec@1: [66.290]	
Starting epoch number: 150 Learning rate: 0.010000000000000002
Train: [150]	Time 647.946	Data 0.407	Loss 0.435	Prec@1 87.1100	Prec@5 98.6120	
Val: [150]	Time 40.116	Data 0.165	Loss 0.988	Prec@1 73.7200	Prec@5 93.6700	
Best Prec@1: [73.720]	
Starting epoch number: 151 Learning rate: 0.010000000000000002
Train: [151]	Time 647.890	Data 0.395	Loss 0.327	Prec@1 90.5560	Prec@5 99.2320	
Val: [151]	Time 40.154	Data 0.189	Loss 0.987	Prec@1 74.0800	Prec@5 93.7000	
Best Prec@1: [74.080]	
Starting epoch number: 152 Learning rate: 0.010000000000000002
Train: [152]	Time 647.913	Data 0.409	Loss 0.281	Prec@1 91.9600	Prec@5 99.3960	
Val: [152]	Time 40.150	Data 0.177	Loss 0.980	Prec@1 74.5500	Prec@5 93.8200	
Best Prec@1: [74.550]	
Starting epoch number: 153 Learning rate: 0.010000000000000002
Train: [153]	Time 647.872	Data 0.399	Loss 0.251	Prec@1 93.0300	Prec@5 99.5400	
Val: [153]	Time 40.149	Data 0.164	Loss 0.996	Prec@1 74.5500	Prec@5 93.8000	
Best Prec@1: [74.550]	
Starting epoch number: 154 Learning rate: 0.010000000000000002
Train: [154]	Time 647.909	Data 0.406	Loss 0.229	Prec@1 93.7320	Prec@5 99.6020	
Val: [154]	Time 40.143	Data 0.187	Loss 1.002	Prec@1 74.7800	Prec@5 93.8700	
Best Prec@1: [74.780]	
Starting epoch number: 155 Learning rate: 0.010000000000000002
Train: [155]	Time 647.920	Data 0.412	Loss 0.210	Prec@1 94.2600	Prec@5 99.6840	
Val: [155]	Time 40.148	Data 0.145	Loss 1.011	Prec@1 74.8400	Prec@5 94.0700	
Best Prec@1: [74.840]	
Starting epoch number: 156 Learning rate: 0.010000000000000002
Train: [156]	Time 647.959	Data 0.377	Loss 0.197	Prec@1 94.5940	Prec@5 99.7520	
Val: [156]	Time 40.223	Data 0.198	Loss 1.023	Prec@1 75.2300	Prec@5 93.8600	
Best Prec@1: [75.230]	
Starting epoch number: 157 Learning rate: 0.010000000000000002
Train: [157]	Time 648.039	Data 0.393	Loss 0.180	Prec@1 95.2500	Prec@5 99.7940	
Val: [157]	Time 40.189	Data 0.156	Loss 1.031	Prec@1 75.0400	Prec@5 93.8000	
Best Prec@1: [75.230]	
Starting epoch number: 158 Learning rate: 0.010000000000000002
Train: [158]	Time 648.239	Data 0.422	Loss 0.167	Prec@1 95.6780	Prec@5 99.8360	
Val: [158]	Time 40.188	Data 0.146	Loss 1.043	Prec@1 74.8300	Prec@5 93.8200	
Best Prec@1: [75.230]	
Starting epoch number: 159 Learning rate: 0.010000000000000002
Train: [159]	Time 648.291	Data 0.417	Loss 0.158	Prec@1 96.0000	Prec@5 99.8600	
Val: [159]	Time 40.263	Data 0.169	Loss 1.062	Prec@1 74.8200	Prec@5 93.7600	
Best Prec@1: [75.230]	
Starting epoch number: 160 Learning rate: 0.010000000000000002
Train: [160]	Time 648.786	Data 0.413	Loss 0.150	Prec@1 96.0640	Prec@5 99.8780	
Val: [160]	Time 40.319	Data 0.176	Loss 1.060	Prec@1 75.0000	Prec@5 93.7300	
Best Prec@1: [75.230]	
Starting epoch number: 161 Learning rate: 0.010000000000000002
Train: [161]	Time 649.379	Data 0.409	Loss 0.139	Prec@1 96.5220	Prec@5 99.8880	
Val: [161]	Time 40.327	Data 0.160	Loss 1.074	Prec@1 75.0300	Prec@5 93.8100	
Best Prec@1: [75.230]	
Starting epoch number: 162 Learning rate: 0.010000000000000002
Train: [162]	Time 649.825	Data 0.379	Loss 0.131	Prec@1 96.8000	Prec@5 99.9020	
Val: [162]	Time 40.369	Data 0.165	Loss 1.073	Prec@1 75.0800	Prec@5 93.6100	
Best Prec@1: [75.230]	
Starting epoch number: 163 Learning rate: 0.010000000000000002
Train: [163]	Time 649.886	Data 0.450	Loss 0.127	Prec@1 96.8860	Prec@5 99.9300	
Val: [163]	Time 40.361	Data 0.149	Loss 1.087	Prec@1 74.8200	Prec@5 93.6300	
Best Prec@1: [75.230]	
Starting epoch number: 164 Learning rate: 0.010000000000000002
Train: [164]	Time 649.984	Data 0.373	Loss 0.121	Prec@1 97.1340	Prec@5 99.9200	
Val: [164]	Time 40.404	Data 0.152	Loss 1.107	Prec@1 74.9200	Prec@5 93.6300	
Best Prec@1: [75.230]	
Starting epoch number: 165 Learning rate: 0.010000000000000002
Train: [165]	Time 650.097	Data 0.398	Loss 0.114	Prec@1 97.3200	Prec@5 99.9540	
Val: [165]	Time 40.404	Data 0.185	Loss 1.106	Prec@1 74.9400	Prec@5 93.6800	
Best Prec@1: [75.230]	
Starting epoch number: 166 Learning rate: 0.010000000000000002
Train: [166]	Time 650.280	Data 0.381	Loss 0.111	Prec@1 97.3680	Prec@5 99.9460	
Val: [166]	Time 40.437	Data 0.166	Loss 1.112	Prec@1 75.1000	Prec@5 93.4500	
Best Prec@1: [75.230]	
Starting epoch number: 167 Learning rate: 0.010000000000000002
Train: [167]	Time 650.731	Data 0.387	Loss 0.105	Prec@1 97.6140	Prec@5 99.9780	
Val: [167]	Time 40.384	Data 0.153	Loss 1.128	Prec@1 74.8200	Prec@5 93.5200	
Best Prec@1: [75.230]	
Starting epoch number: 168 Learning rate: 0.010000000000000002
Train: [168]	Time 651.100	Data 0.385	Loss 0.100	Prec@1 97.7860	Prec@5 99.9820	
Val: [168]	Time 40.552	Data 0.159	Loss 1.135	Prec@1 74.9000	Prec@5 93.3500	
Best Prec@1: [75.230]	
Starting epoch number: 169 Learning rate: 0.010000000000000002
Train: [169]	Time 654.104	Data 0.370	Loss 0.097	Prec@1 97.8100	Prec@5 99.9720	
Val: [169]	Time 40.704	Data 0.205	Loss 1.130	Prec@1 74.6200	Prec@5 93.3500	
Best Prec@1: [75.230]	
Starting epoch number: 170 Learning rate: 0.010000000000000002
Train: [170]	Time 652.962	Data 0.385	Loss 0.093	Prec@1 97.9900	Prec@5 99.9720	
Val: [170]	Time 40.628	Data 0.207	Loss 1.136	Prec@1 74.9500	Prec@5 93.4600	
Best Prec@1: [75.230]	
Starting epoch number: 171 Learning rate: 0.010000000000000002
Train: [171]	Time 652.488	Data 0.390	Loss 0.090	Prec@1 98.0280	Prec@5 99.9860	
Val: [171]	Time 40.493	Data 0.143	Loss 1.150	Prec@1 74.7300	Prec@5 93.4300	
Best Prec@1: [75.230]	
Starting epoch number: 172 Learning rate: 0.010000000000000002
Train: [172]	Time 652.083	Data 0.413	Loss 0.085	Prec@1 98.3040	Prec@5 99.9860	
Val: [172]	Time 40.554	Data 0.153	Loss 1.158	Prec@1 74.8300	Prec@5 93.4100	
Best Prec@1: [75.230]	
Starting epoch number: 173 Learning rate: 0.010000000000000002
Train: [173]	Time 651.696	Data 0.417	Loss 0.085	Prec@1 98.1320	Prec@5 99.9820	
Val: [173]	Time 40.521	Data 0.156	Loss 1.156	Prec@1 74.7200	Prec@5 93.6400	
Best Prec@1: [75.230]	
Starting epoch number: 174 Learning rate: 0.010000000000000002
Train: [174]	Time 651.298	Data 0.364	Loss 0.082	Prec@1 98.2600	Prec@5 99.9980	
Val: [174]	Time 40.494	Data 0.162	Loss 1.155	Prec@1 74.8700	Prec@5 93.3100	
Best Prec@1: [75.230]	
Starting epoch number: 175 Learning rate: 0.010000000000000002
Train: [175]	Time 651.489	Data 0.459	Loss 0.078	Prec@1 98.4440	Prec@5 99.9820	
Val: [175]	Time 40.520	Data 0.173	Loss 1.163	Prec@1 75.0100	Prec@5 93.2400	
Best Prec@1: [75.230]	
Starting epoch number: 176 Learning rate: 0.010000000000000002
Train: [176]	Time 651.046	Data 0.389	Loss 0.076	Prec@1 98.5140	Prec@5 99.9900	
Val: [176]	Time 40.426	Data 0.182	Loss 1.170	Prec@1 74.8700	Prec@5 93.3100	
Best Prec@1: [75.230]	
Starting epoch number: 177 Learning rate: 0.010000000000000002
Train: [177]	Time 651.032	Data 0.407	Loss 0.073	Prec@1 98.5920	Prec@5 99.9940	
Val: [177]	Time 40.521	Data 0.189	Loss 1.155	Prec@1 75.0900	Prec@5 93.5600	
Best Prec@1: [75.230]	
Starting epoch number: 178 Learning rate: 0.010000000000000002
Train: [178]	Time 650.712	Data 0.394	Loss 0.070	Prec@1 98.7000	Prec@5 99.9940	
Val: [178]	Time 40.485	Data 0.194	Loss 1.156	Prec@1 75.1900	Prec@5 93.5900	
Best Prec@1: [75.230]	
Starting epoch number: 179 Learning rate: 0.010000000000000002
Train: [179]	Time 650.902	Data 0.369	Loss 0.071	Prec@1 98.5820	Prec@5 99.9920	
Val: [179]	Time 40.448	Data 0.179	Loss 1.173	Prec@1 74.7600	Prec@5 93.3700	
Best Prec@1: [75.230]	
Starting epoch number: 180 Learning rate: 0.010000000000000002
Train: [180]	Time 650.787	Data 0.362	Loss 0.068	Prec@1 98.6780	Prec@5 99.9900	
Val: [180]	Time 40.390	Data 0.152	Loss 1.177	Prec@1 74.8200	Prec@5 93.2600	
Best Prec@1: [75.230]	
Starting epoch number: 181 Learning rate: 0.010000000000000002
Train: [181]	Time 650.490	Data 0.406	Loss 0.067	Prec@1 98.6960	Prec@5 99.9960	
Val: [181]	Time 40.435	Data 0.219	Loss 1.164	Prec@1 74.6100	Prec@5 93.4200	
Best Prec@1: [75.230]	
Starting epoch number: 182 Learning rate: 0.010000000000000002
Train: [182]	Time 649.929	Data 0.379	Loss 0.065	Prec@1 98.8460	Prec@5 99.9960	
Val: [182]	Time 40.429	Data 0.166	Loss 1.188	Prec@1 74.5000	Prec@5 93.4000	
Best Prec@1: [75.230]	
Starting epoch number: 183 Learning rate: 0.010000000000000002
Train: [183]	Time 649.787	Data 0.418	Loss 0.063	Prec@1 98.8500	Prec@5 99.9980	
Val: [183]	Time 40.335	Data 0.163	Loss 1.184	Prec@1 74.6200	Prec@5 93.1400	
Best Prec@1: [75.230]	
Starting epoch number: 184 Learning rate: 0.010000000000000002
Train: [184]	Time 649.425	Data 0.390	Loss 0.063	Prec@1 98.8360	Prec@5 99.9940	
Val: [184]	Time 40.337	Data 0.187	Loss 1.171	Prec@1 74.6000	Prec@5 93.3900	
Best Prec@1: [75.230]	
Starting epoch number: 185 Learning rate: 0.010000000000000002
Train: [185]	Time 649.217	Data 0.376	Loss 0.061	Prec@1 98.9160	Prec@5 99.9940	
Val: [185]	Time 40.292	Data 0.191	Loss 1.171	Prec@1 75.1300	Prec@5 93.3200	
Best Prec@1: [75.230]	
Starting epoch number: 186 Learning rate: 0.010000000000000002
Train: [186]	Time 648.943	Data 0.458	Loss 0.061	Prec@1 98.9660	Prec@5 99.9940	
Val: [186]	Time 40.270	Data 0.163	Loss 1.180	Prec@1 74.7900	Prec@5 93.4600	
Best Prec@1: [75.230]	
Starting epoch number: 187 Learning rate: 0.010000000000000002
Train: [187]	Time 648.676	Data 0.376	Loss 0.060	Prec@1 98.9420	Prec@5 99.9940	
Val: [187]	Time 40.254	Data 0.203	Loss 1.197	Prec@1 74.6500	Prec@5 93.3900	
Best Prec@1: [75.230]	
Starting epoch number: 188 Learning rate: 0.010000000000000002
Train: [188]	Time 648.409	Data 0.389	Loss 0.058	Prec@1 98.9600	Prec@5 100.0000	
Val: [188]	Time 40.270	Data 0.159	Loss 1.174	Prec@1 74.5800	Prec@5 93.2800	
Best Prec@1: [75.230]	
Starting epoch number: 189 Learning rate: 0.010000000000000002
Train: [189]	Time 648.277	Data 0.409	Loss 0.058	Prec@1 98.9980	Prec@5 99.9980	
Val: [189]	Time 40.253	Data 0.163	Loss 1.189	Prec@1 74.5600	Prec@5 93.3800	
Best Prec@1: [75.230]	
Starting epoch number: 190 Learning rate: 0.010000000000000002
Train: [190]	Time 648.188	Data 0.399	Loss 0.057	Prec@1 98.9820	Prec@5 99.9940	
Val: [190]	Time 40.190	Data 0.168	Loss 1.189	Prec@1 74.7000	Prec@5 93.2900	
Best Prec@1: [75.230]	
Starting epoch number: 191 Learning rate: 0.010000000000000002
Train: [191]	Time 648.018	Data 0.360	Loss 0.057	Prec@1 98.9280	Prec@5 99.9960	
Val: [191]	Time 40.069	Data 0.116	Loss 1.197	Prec@1 74.3000	Prec@5 93.3600	
Best Prec@1: [75.230]	
Starting epoch number: 192 Learning rate: 0.010000000000000002
Train: [192]	Time 647.621	Data 0.301	Loss 0.056	Prec@1 99.0200	Prec@5 99.9900	
Val: [192]	Time 40.058	Data 0.113	Loss 1.199	Prec@1 74.9100	Prec@5 93.2600	
Best Prec@1: [75.230]	
Starting epoch number: 193 Learning rate: 0.010000000000000002
Train: [193]	Time 647.672	Data 0.277	Loss 0.056	Prec@1 98.9720	Prec@5 99.9940	
Val: [193]	Time 40.068	Data 0.119	Loss 1.197	Prec@1 74.3400	Prec@5 93.1200	
Best Prec@1: [75.230]	
Starting epoch number: 194 Learning rate: 0.010000000000000002
Train: [194]	Time 647.628	Data 0.292	Loss 0.053	Prec@1 99.1520	Prec@5 99.9980	
Val: [194]	Time 40.046	Data 0.108	Loss 1.186	Prec@1 74.4800	Prec@5 93.2200	
Best Prec@1: [75.230]	
Starting epoch number: 195 Learning rate: 0.010000000000000002
Train: [195]	Time 647.581	Data 0.294	Loss 0.055	Prec@1 99.0220	Prec@5 99.9960	
Val: [195]	Time 40.052	Data 0.112	Loss 1.188	Prec@1 74.5900	Prec@5 93.2300	
Best Prec@1: [75.230]	
Starting epoch number: 196 Learning rate: 0.010000000000000002
Train: [196]	Time 647.614	Data 0.307	Loss 0.055	Prec@1 99.0560	Prec@5 99.9980	
Val: [196]	Time 40.086	Data 0.117	Loss 1.179	Prec@1 74.6600	Prec@5 93.2900	
Best Prec@1: [75.230]	
Starting epoch number: 197 Learning rate: 0.010000000000000002
Train: [197]	Time 647.550	Data 0.297	Loss 0.054	Prec@1 99.0900	Prec@5 100.0000	
Val: [197]	Time 40.098	Data 0.138	Loss 1.188	Prec@1 74.5000	Prec@5 93.4800	
Best Prec@1: [75.230]	
Starting epoch number: 198 Learning rate: 0.010000000000000002
Train: [198]	Time 647.534	Data 0.289	Loss 0.056	Prec@1 99.0180	Prec@5 99.9960	
Val: [198]	Time 40.019	Data 0.110	Loss 1.194	Prec@1 74.3800	Prec@5 93.2600	
Best Prec@1: [75.230]	
Starting epoch number: 199 Learning rate: 0.010000000000000002
Train: [199]	Time 647.478	Data 0.287	Loss 0.052	Prec@1 99.1420	Prec@5 100.0000	
Val: [199]	Time 40.008	Data 0.119	Loss 1.188	Prec@1 74.6300	Prec@5 93.3500	
Best Prec@1: [75.230]	
Starting epoch number: 200 Learning rate: 0.010000000000000002
Train: [200]	Time 647.544	Data 0.289	Loss 0.052	Prec@1 99.1600	Prec@5 99.9940	
Val: [200]	Time 40.004	Data 0.123	Loss 1.182	Prec@1 74.7300	Prec@5 93.2400	
Best Prec@1: [75.230]	
Starting epoch number: 201 Learning rate: 0.010000000000000002
Train: [201]	Time 647.423	Data 0.299	Loss 0.052	Prec@1 99.1780	Prec@5 99.9980	
Val: [201]	Time 39.996	Data 0.106	Loss 1.199	Prec@1 74.5800	Prec@5 93.0800	
Best Prec@1: [75.230]	
Starting epoch number: 202 Learning rate: 0.010000000000000002
Train: [202]	Time 647.451	Data 0.306	Loss 0.052	Prec@1 99.1420	Prec@5 100.0000	
Val: [202]	Time 40.018	Data 0.125	Loss 1.198	Prec@1 74.4100	Prec@5 93.3000	
Best Prec@1: [75.230]	
Starting epoch number: 203 Learning rate: 0.010000000000000002
Train: [203]	Time 647.512	Data 0.302	Loss 0.052	Prec@1 99.1280	Prec@5 99.9980	
Val: [203]	Time 40.014	Data 0.113	Loss 1.201	Prec@1 74.5600	Prec@5 93.2700	
Best Prec@1: [75.230]	
Starting epoch number: 204 Learning rate: 0.010000000000000002
Train: [204]	Time 647.599	Data 0.297	Loss 0.052	Prec@1 99.1180	Prec@5 99.9940	
Val: [204]	Time 40.043	Data 0.113	Loss 1.195	Prec@1 74.0100	Prec@5 93.1400	
Best Prec@1: [75.230]	
Starting epoch number: 205 Learning rate: 0.010000000000000002
Train: [205]	Time 647.656	Data 0.306	Loss 0.054	Prec@1 99.0700	Prec@5 100.0000	
Val: [205]	Time 40.070	Data 0.113	Loss 1.197	Prec@1 74.5700	Prec@5 93.2300	
Best Prec@1: [75.230]	
Starting epoch number: 206 Learning rate: 0.010000000000000002
Train: [206]	Time 647.663	Data 0.297	Loss 0.054	Prec@1 99.0660	Prec@5 99.9980	
Val: [206]	Time 40.078	Data 0.119	Loss 1.203	Prec@1 74.5900	Prec@5 93.3000	
Best Prec@1: [75.230]	
Starting epoch number: 207 Learning rate: 0.010000000000000002
Train: [207]	Time 647.618	Data 0.303	Loss 0.052	Prec@1 99.0940	Prec@5 99.9980	
Val: [207]	Time 40.108	Data 0.111	Loss 1.184	Prec@1 74.5600	Prec@5 93.1500	
Best Prec@1: [75.230]	
Starting epoch number: 208 Learning rate: 0.010000000000000002
Train: [208]	Time 647.621	Data 0.291	Loss 0.053	Prec@1 99.1060	Prec@5 99.9980	
Val: [208]	Time 40.066	Data 0.122	Loss 1.187	Prec@1 74.7000	Prec@5 93.3000	
Best Prec@1: [75.230]	
Starting epoch number: 209 Learning rate: 0.010000000000000002
Train: [209]	Time 647.651	Data 0.288	Loss 0.053	Prec@1 99.1160	Prec@5 99.9980	
Val: [209]	Time 40.130	Data 0.115	Loss 1.199	Prec@1 74.5700	Prec@5 93.0700	
Best Prec@1: [75.230]	
Starting epoch number: 210 Learning rate: 0.010000000000000002
Train: [210]	Time 647.758	Data 0.324	Loss 0.054	Prec@1 99.0500	Prec@5 99.9940	
Val: [210]	Time 40.119	Data 0.117	Loss 1.181	Prec@1 74.6500	Prec@5 93.3200	
Best Prec@1: [75.230]	
Starting epoch number: 211 Learning rate: 0.010000000000000002
Train: [211]	Time 647.774	Data 0.295	Loss 0.054	Prec@1 99.1020	Prec@5 99.9920	
Val: [211]	Time 40.091	Data 0.115	Loss 1.216	Prec@1 74.2300	Prec@5 93.1600	
Best Prec@1: [75.230]	
Starting epoch number: 212 Learning rate: 0.010000000000000002
Train: [212]	Time 647.726	Data 0.304	Loss 0.052	Prec@1 99.1100	Prec@5 100.0000	
Val: [212]	Time 40.102	Data 0.124	Loss 1.203	Prec@1 74.0400	Prec@5 93.0200	
Best Prec@1: [75.230]	
Starting epoch number: 213 Learning rate: 0.010000000000000002
Train: [213]	Time 647.688	Data 0.299	Loss 0.054	Prec@1 99.0520	Prec@5 100.0000	
Val: [213]	Time 40.102	Data 0.124	Loss 1.196	Prec@1 74.2300	Prec@5 93.0700	
Best Prec@1: [75.230]	
Starting epoch number: 214 Learning rate: 0.010000000000000002
Train: [214]	Time 647.648	Data 0.291	Loss 0.053	Prec@1 99.0640	Prec@5 100.0000	
Val: [214]	Time 40.105	Data 0.120	Loss 1.191	Prec@1 74.4600	Prec@5 93.2100	
Best Prec@1: [75.230]	
Starting epoch number: 215 Learning rate: 0.010000000000000002
Train: [215]	Time 647.705	Data 0.290	Loss 0.054	Prec@1 99.0640	Prec@5 99.9960	
Val: [215]	Time 40.096	Data 0.119	Loss 1.216	Prec@1 73.7800	Prec@5 92.9400	
Best Prec@1: [75.230]	
Starting epoch number: 216 Learning rate: 0.010000000000000002
Train: [216]	Time 647.693	Data 0.314	Loss 0.052	Prec@1 99.0960	Prec@5 99.9960	
Val: [216]	Time 40.153	Data 0.122	Loss 1.201	Prec@1 74.3000	Prec@5 93.2200	
Best Prec@1: [75.230]	
Starting epoch number: 217 Learning rate: 0.010000000000000002
Train: [217]	Time 647.641	Data 0.291	Loss 0.054	Prec@1 99.0500	Prec@5 99.9940	
Val: [217]	Time 40.138	Data 0.126	Loss 1.202	Prec@1 74.3100	Prec@5 93.0800	
Best Prec@1: [75.230]	
Starting epoch number: 218 Learning rate: 0.010000000000000002
Train: [218]	Time 647.708	Data 0.315	Loss 0.054	Prec@1 99.0780	Prec@5 100.0000	
Val: [218]	Time 40.116	Data 0.111	Loss 1.205	Prec@1 74.0300	Prec@5 93.2100	
Best Prec@1: [75.230]	
Starting epoch number: 219 Learning rate: 0.010000000000000002
Train: [219]	Time 647.654	Data 0.303	Loss 0.056	Prec@1 98.9900	Prec@5 100.0000	
Val: [219]	Time 40.129	Data 0.122	Loss 1.183	Prec@1 74.5600	Prec@5 93.3000	
Best Prec@1: [75.230]	
Starting epoch number: 220 Learning rate: 0.010000000000000002
Train: [220]	Time 647.696	Data 0.296	Loss 0.057	Prec@1 98.9420	Prec@5 99.9900	
Val: [220]	Time 40.103	Data 0.114	Loss 1.187	Prec@1 74.5900	Prec@5 93.1500	
Best Prec@1: [75.230]	
Starting epoch number: 221 Learning rate: 0.010000000000000002
Train: [221]	Time 647.796	Data 0.298	Loss 0.056	Prec@1 98.9420	Prec@5 99.9920	
Val: [221]	Time 40.110	Data 0.117	Loss 1.179	Prec@1 74.6100	Prec@5 93.2300	
Best Prec@1: [75.230]	
Starting epoch number: 222 Learning rate: 0.010000000000000002
Train: [222]	Time 647.748	Data 0.298	Loss 0.055	Prec@1 99.0180	Prec@5 99.9980	
Val: [222]	Time 40.136	Data 0.109	Loss 1.195	Prec@1 74.2100	Prec@5 93.1800	
Best Prec@1: [75.230]	
Starting epoch number: 223 Learning rate: 0.010000000000000002
Train: [223]	Time 647.743	Data 0.301	Loss 0.054	Prec@1 98.9960	Prec@5 99.9960	
Val: [223]	Time 40.094	Data 0.116	Loss 1.214	Prec@1 74.1500	Prec@5 92.9900	
Best Prec@1: [75.230]	
Starting epoch number: 224 Learning rate: 0.010000000000000002
Train: [224]	Time 647.681	Data 0.296	Loss 0.061	Prec@1 98.7820	Prec@5 99.9980	
Val: [224]	Time 40.062	Data 0.107	Loss 1.229	Prec@1 74.2000	Prec@5 92.9000	
Best Prec@1: [75.230]	
Starting epoch number: 225 Learning rate: 0.0010000000000000002
Train: [225]	Time 647.562	Data 0.293	Loss 0.044	Prec@1 99.3640	Prec@5 100.0000	
Val: [225]	Time 40.023	Data 0.108	Loss 1.158	Prec@1 75.1800	Prec@5 93.1600	
Best Prec@1: [75.230]	
Starting epoch number: 226 Learning rate: 0.0010000000000000002
Train: [226]	Time 647.554	Data 0.294	Loss 0.037	Prec@1 99.5200	Prec@5 100.0000	
Val: [226]	Time 40.024	Data 0.110	Loss 1.145	Prec@1 75.4200	Prec@5 93.3600	
Best Prec@1: [75.420]	
Starting epoch number: 227 Learning rate: 0.0010000000000000002
Train: [227]	Time 647.606	Data 0.315	Loss 0.034	Prec@1 99.5920	Prec@5 99.9980	
Val: [227]	Time 40.006	Data 0.114	Loss 1.152	Prec@1 75.5300	Prec@5 93.4000	
Best Prec@1: [75.530]	
Starting epoch number: 228 Learning rate: 0.0010000000000000002
Train: [228]	Time 647.467	Data 0.309	Loss 0.032	Prec@1 99.6620	Prec@5 100.0000	
Val: [228]	Time 40.015	Data 0.112	Loss 1.146	Prec@1 75.5500	Prec@5 93.3500	
Best Prec@1: [75.550]	
Starting epoch number: 229 Learning rate: 0.0010000000000000002
Train: [229]	Time 647.517	Data 0.312	Loss 0.029	Prec@1 99.7480	Prec@5 100.0000	
Val: [229]	Time 39.995	Data 0.111	Loss 1.144	Prec@1 75.3300	Prec@5 93.4500	
Best Prec@1: [75.550]	
Starting epoch number: 230 Learning rate: 0.0010000000000000002
Train: [230]	Time 647.456	Data 0.281	Loss 0.029	Prec@1 99.7160	Prec@5 100.0000	
Val: [230]	Time 40.003	Data 0.112	Loss 1.137	Prec@1 75.2600	Prec@5 93.6000	
Best Prec@1: [75.550]	
Starting epoch number: 231 Learning rate: 0.0010000000000000002
Train: [231]	Time 647.402	Data 0.302	Loss 0.028	Prec@1 99.7420	Prec@5 100.0000	
Val: [231]	Time 40.008	Data 0.107	Loss 1.145	Prec@1 75.5600	Prec@5 93.5000	
Best Prec@1: [75.560]	
Starting epoch number: 232 Learning rate: 0.0010000000000000002
Train: [232]	Time 647.342	Data 0.297	Loss 0.027	Prec@1 99.7600	Prec@5 100.0000	
Val: [232]	Time 39.999	Data 0.108	Loss 1.161	Prec@1 75.3300	Prec@5 93.3500	
Best Prec@1: [75.560]	
Starting epoch number: 233 Learning rate: 0.0010000000000000002
Train: [233]	Time 647.353	Data 0.291	Loss 0.027	Prec@1 99.7920	Prec@5 100.0000	
Val: [233]	Time 39.999	Data 0.112	Loss 1.145	Prec@1 75.3400	Prec@5 93.4200	
Best Prec@1: [75.560]	
Starting epoch number: 234 Learning rate: 0.0010000000000000002
Train: [234]	Time 647.373	Data 0.313	Loss 0.026	Prec@1 99.8080	Prec@5 100.0000	
Val: [234]	Time 40.011	Data 0.119	Loss 1.148	Prec@1 75.5000	Prec@5 93.5100	
Best Prec@1: [75.560]	
Starting epoch number: 235 Learning rate: 0.0010000000000000002
Train: [235]	Time 647.421	Data 0.299	Loss 0.026	Prec@1 99.7520	Prec@5 100.0000	
Val: [235]	Time 39.998	Data 0.113	Loss 1.134	Prec@1 75.4800	Prec@5 93.4900	
Best Prec@1: [75.560]	
Starting epoch number: 236 Learning rate: 0.0010000000000000002
Train: [236]	Time 647.321	Data 0.301	Loss 0.025	Prec@1 99.7960	Prec@5 100.0000	
Val: [236]	Time 40.012	Data 0.122	Loss 1.147	Prec@1 75.4800	Prec@5 93.4700	
Best Prec@1: [75.560]	
Starting epoch number: 237 Learning rate: 0.0010000000000000002
Train: [237]	Time 647.428	Data 0.300	Loss 0.025	Prec@1 99.8020	Prec@5 100.0000	
Val: [237]	Time 40.002	Data 0.116	Loss 1.145	Prec@1 75.6200	Prec@5 93.5900	
Best Prec@1: [75.620]	
Starting epoch number: 238 Learning rate: 0.0010000000000000002
Train: [238]	Time 647.403	Data 0.305	Loss 0.024	Prec@1 99.8140	Prec@5 99.9980	
Val: [238]	Time 40.000	Data 0.112	Loss 1.138	Prec@1 75.4900	Prec@5 93.5500	
Best Prec@1: [75.620]	
Starting epoch number: 239 Learning rate: 0.0010000000000000002
Train: [239]	Time 647.437	Data 0.297	Loss 0.024	Prec@1 99.8180	Prec@5 100.0000	
Val: [239]	Time 40.023	Data 0.123	Loss 1.147	Prec@1 75.4200	Prec@5 93.4100	
Best Prec@1: [75.620]	
Starting epoch number: 240 Learning rate: 0.0010000000000000002
Train: [240]	Time 647.449	Data 0.293	Loss 0.023	Prec@1 99.8780	Prec@5 100.0000	
Val: [240]	Time 40.006	Data 0.118	Loss 1.142	Prec@1 75.5400	Prec@5 93.4900	
Best Prec@1: [75.620]	
Starting epoch number: 241 Learning rate: 0.0010000000000000002
Train: [241]	Time 647.382	Data 0.298	Loss 0.023	Prec@1 99.8320	Prec@5 100.0000	
Val: [241]	Time 40.000	Data 0.118	Loss 1.142	Prec@1 75.6300	Prec@5 93.5300	
Best Prec@1: [75.630]	
Starting epoch number: 242 Learning rate: 0.0010000000000000002
Train: [242]	Time 647.365	Data 0.302	Loss 0.024	Prec@1 99.8220	Prec@5 100.0000	
Val: [242]	Time 39.985	Data 0.102	Loss 1.138	Prec@1 75.6900	Prec@5 93.5300	
Best Prec@1: [75.690]	
Starting epoch number: 243 Learning rate: 0.0010000000000000002
Train: [243]	Time 647.414	Data 0.294	Loss 0.023	Prec@1 99.8080	Prec@5 100.0000	
Val: [243]	Time 40.012	Data 0.113	Loss 1.143	Prec@1 75.6000	Prec@5 93.5100	
Best Prec@1: [75.690]	
Starting epoch number: 244 Learning rate: 0.0010000000000000002
Train: [244]	Time 647.371	Data 0.298	Loss 0.023	Prec@1 99.8140	Prec@5 99.9980	
Val: [244]	Time 40.014	Data 0.118	Loss 1.154	Prec@1 75.5000	Prec@5 93.4700	
Best Prec@1: [75.690]	
Starting epoch number: 245 Learning rate: 0.0010000000000000002
Train: [245]	Time 647.338	Data 0.286	Loss 0.022	Prec@1 99.8700	Prec@5 100.0000	
Val: [245]	Time 39.998	Data 0.103	Loss 1.140	Prec@1 75.5200	Prec@5 93.5900	
Best Prec@1: [75.690]	
Starting epoch number: 246 Learning rate: 0.0010000000000000002
Train: [246]	Time 647.464	Data 0.288	Loss 0.022	Prec@1 99.8800	Prec@5 100.0000	
Val: [246]	Time 40.080	Data 0.106	Loss 1.146	Prec@1 75.4300	Prec@5 93.6300	
Best Prec@1: [75.690]	
Starting epoch number: 247 Learning rate: 0.0010000000000000002
Train: [247]	Time 647.501	Data 0.304	Loss 0.022	Prec@1 99.8380	Prec@5 100.0000	
Val: [247]	Time 40.040	Data 0.119	Loss 1.140	Prec@1 75.6900	Prec@5 93.6400	
Best Prec@1: [75.690]	
Starting epoch number: 248 Learning rate: 0.0010000000000000002
Train: [248]	Time 647.565	Data 0.288	Loss 0.022	Prec@1 99.8860	Prec@5 100.0000	
Val: [248]	Time 40.120	Data 0.120	Loss 1.138	Prec@1 75.7000	Prec@5 93.4500	
Best Prec@1: [75.700]	
Starting epoch number: 249 Learning rate: 0.0010000000000000002
Train: [249]	Time 647.619	Data 0.290	Loss 0.021	Prec@1 99.8660	Prec@5 100.0000	
Val: [249]	Time 40.163	Data 0.119	Loss 1.130	Prec@1 75.4400	Prec@5 93.6400	
Best Prec@1: [75.700]	
Starting epoch number: 250 Learning rate: 0.0010000000000000002
Train: [250]	Time 647.610	Data 0.300	Loss 0.022	Prec@1 99.8240	Prec@5 100.0000	
Val: [250]	Time 40.124	Data 0.109	Loss 1.139	Prec@1 75.3500	Prec@5 93.5100	
Best Prec@1: [75.700]	
Starting epoch number: 251 Learning rate: 0.0010000000000000002
Train: [251]	Time 647.675	Data 0.284	Loss 0.022	Prec@1 99.8220	Prec@5 100.0000	
Val: [251]	Time 40.105	Data 0.142	Loss 1.143	Prec@1 75.5300	Prec@5 93.5300	
Best Prec@1: [75.700]	
Starting epoch number: 252 Learning rate: 0.0010000000000000002
Train: [252]	Time 647.596	Data 0.307	Loss 0.022	Prec@1 99.8680	Prec@5 100.0000	
Val: [252]	Time 40.161	Data 0.114	Loss 1.136	Prec@1 75.5500	Prec@5 93.4100	
Best Prec@1: [75.700]	
Starting epoch number: 253 Learning rate: 0.0010000000000000002
Train: [253]	Time 647.932	Data 0.303	Loss 0.021	Prec@1 99.8540	Prec@5 100.0000	
Val: [253]	Time 40.159	Data 0.117	Loss 1.143	Prec@1 75.4500	Prec@5 93.5300	
Best Prec@1: [75.700]	
Starting epoch number: 254 Learning rate: 0.0010000000000000002
Train: [254]	Time 647.955	Data 0.304	Loss 0.021	Prec@1 99.8820	Prec@5 100.0000	
Val: [254]	Time 40.196	Data 0.120	Loss 1.142	Prec@1 75.6100	Prec@5 93.4500	
Best Prec@1: [75.700]	
Starting epoch number: 255 Learning rate: 0.0010000000000000002
Train: [255]	Time 648.488	Data 0.307	Loss 0.021	Prec@1 99.8640	Prec@5 100.0000	
Val: [255]	Time 40.262	Data 0.122	Loss 1.144	Prec@1 75.3500	Prec@5 93.5100	
Best Prec@1: [75.700]	
Starting epoch number: 256 Learning rate: 0.0010000000000000002
Train: [256]	Time 648.847	Data 0.303	Loss 0.020	Prec@1 99.8840	Prec@5 100.0000	
Val: [256]	Time 40.252	Data 0.112	Loss 1.130	Prec@1 75.5300	Prec@5 93.5900	
Best Prec@1: [75.700]	
Starting epoch number: 257 Learning rate: 0.0010000000000000002
Train: [257]	Time 649.469	Data 0.300	Loss 0.021	Prec@1 99.8500	Prec@5 100.0000	
Val: [257]	Time 40.370	Data 0.108	Loss 1.139	Prec@1 75.4500	Prec@5 93.4900	
Best Prec@1: [75.700]	
Starting epoch number: 258 Learning rate: 0.0010000000000000002
Train: [258]	Time 650.503	Data 0.283	Loss 0.021	Prec@1 99.8700	Prec@5 100.0000	
Val: [258]	Time 40.421	Data 0.119	Loss 1.145	Prec@1 75.3100	Prec@5 93.6000	
Best Prec@1: [75.700]	
Starting epoch number: 259 Learning rate: 0.0010000000000000002
Train: [259]	Time 650.813	Data 0.308	Loss 0.021	Prec@1 99.9080	Prec@5 100.0000	
Val: [259]	Time 40.426	Data 0.118	Loss 1.142	Prec@1 75.5700	Prec@5 93.5700	
Best Prec@1: [75.700]	
Starting epoch number: 260 Learning rate: 0.0010000000000000002
Train: [260]	Time 651.016	Data 0.298	Loss 0.020	Prec@1 99.8660	Prec@5 100.0000	
Val: [260]	Time 40.407	Data 0.115	Loss 1.144	Prec@1 75.3900	Prec@5 93.4900	
Best Prec@1: [75.700]	
Starting epoch number: 261 Learning rate: 0.0010000000000000002
Train: [261]	Time 650.562	Data 0.296	Loss 0.020	Prec@1 99.8900	Prec@5 100.0000	
Val: [261]	Time 40.383	Data 0.104	Loss 1.132	Prec@1 75.5100	Prec@5 93.6200	
Best Prec@1: [75.700]	
Starting epoch number: 262 Learning rate: 0.0010000000000000002
Train: [262]	Time 650.426	Data 0.291	Loss 0.020	Prec@1 99.8900	Prec@5 100.0000	
Val: [262]	Time 40.347	Data 0.113	Loss 1.136	Prec@1 75.5900	Prec@5 93.4300	
Best Prec@1: [75.700]	
Starting epoch number: 263 Learning rate: 0.0010000000000000002
Train: [263]	Time 650.213	Data 0.298	Loss 0.020	Prec@1 99.8660	Prec@5 100.0000	
Val: [263]	Time 40.355	Data 0.120	Loss 1.146	Prec@1 75.4900	Prec@5 93.5100	
Best Prec@1: [75.700]	
Starting epoch number: 264 Learning rate: 0.0010000000000000002
Train: [264]	Time 649.957	Data 0.298	Loss 0.020	Prec@1 99.9000	Prec@5 100.0000	
Val: [264]	Time 40.370	Data 0.111	Loss 1.143	Prec@1 75.3800	Prec@5 93.6300	
Best Prec@1: [75.700]	
Starting epoch number: 265 Learning rate: 0.0010000000000000002
Train: [265]	Time 649.514	Data 0.288	Loss 0.020	Prec@1 99.8840	Prec@5 100.0000	
Val: [265]	Time 40.296	Data 0.105	Loss 1.151	Prec@1 75.1100	Prec@5 93.4800	
Best Prec@1: [75.700]	
Starting epoch number: 266 Learning rate: 0.0010000000000000002
Train: [266]	Time 649.458	Data 0.300	Loss 0.020	Prec@1 99.8960	Prec@5 100.0000	
Val: [266]	Time 40.324	Data 0.116	Loss 1.140	Prec@1 75.5200	Prec@5 93.5800	
Best Prec@1: [75.700]	
Starting epoch number: 267 Learning rate: 0.0010000000000000002
Train: [267]	Time 649.125	Data 0.319	Loss 0.020	Prec@1 99.9060	Prec@5 100.0000	
Val: [267]	Time 40.253	Data 0.114	Loss 1.147	Prec@1 75.4500	Prec@5 93.5600	
Best Prec@1: [75.700]	
Starting epoch number: 268 Learning rate: 0.0010000000000000002
Train: [268]	Time 648.910	Data 0.299	Loss 0.020	Prec@1 99.8760	Prec@5 100.0000	
Val: [268]	Time 40.232	Data 0.112	Loss 1.143	Prec@1 75.3100	Prec@5 93.5000	
Best Prec@1: [75.700]	
Starting epoch number: 269 Learning rate: 0.0010000000000000002
Train: [269]	Time 648.712	Data 0.291	Loss 0.020	Prec@1 99.8900	Prec@5 100.0000	
Val: [269]	Time 40.288	Data 0.111	Loss 1.133	Prec@1 75.2900	Prec@5 93.4700	
Best Prec@1: [75.700]	
Starting epoch number: 270 Learning rate: 0.0010000000000000002
Train: [270]	Time 648.955	Data 0.295	Loss 0.020	Prec@1 99.8900	Prec@5 100.0000	
Val: [270]	Time 40.219	Data 0.119	Loss 1.146	Prec@1 75.4600	Prec@5 93.2800	
Best Prec@1: [75.700]	
Starting epoch number: 271 Learning rate: 0.0010000000000000002
Train: [271]	Time 649.047	Data 0.311	Loss 0.020	Prec@1 99.8820	Prec@5 100.0000	
Val: [271]	Time 40.271	Data 0.116	Loss 1.138	Prec@1 75.3900	Prec@5 93.5700	
Best Prec@1: [75.700]	
Starting epoch number: 272 Learning rate: 0.0010000000000000002
Train: [272]	Time 649.339	Data 0.290	Loss 0.019	Prec@1 99.9120	Prec@5 100.0000	
Val: [272]	Time 40.365	Data 0.119	Loss 1.141	Prec@1 75.4800	Prec@5 93.4400	
Best Prec@1: [75.700]	
Starting epoch number: 273 Learning rate: 0.0010000000000000002
Train: [273]	Time 649.922	Data 0.309	Loss 0.020	Prec@1 99.8600	Prec@5 100.0000	
Val: [273]	Time 40.379	Data 0.111	Loss 1.142	Prec@1 75.5500	Prec@5 93.4600	
Best Prec@1: [75.700]	
Starting epoch number: 274 Learning rate: 0.0010000000000000002
Train: [274]	Time 649.766	Data 0.273	Loss 0.019	Prec@1 99.8860	Prec@5 100.0000	
Val: [274]	Time 40.311	Data 0.104	Loss 1.141	Prec@1 75.5900	Prec@5 93.4600	
Best Prec@1: [75.700]	
Starting epoch number: 275 Learning rate: 0.0010000000000000002
Train: [275]	Time 650.257	Data 0.287	Loss 0.020	Prec@1 99.8940	Prec@5 100.0000	
Val: [275]	Time 40.292	Data 0.110	Loss 1.134	Prec@1 75.5100	Prec@5 93.6200	
Best Prec@1: [75.700]	
Starting epoch number: 276 Learning rate: 0.0010000000000000002
Train: [276]	Time 650.191	Data 0.289	Loss 0.019	Prec@1 99.8980	Prec@5 100.0000	
Val: [276]	Time 40.380	Data 0.105	Loss 1.145	Prec@1 75.2800	Prec@5 93.6500	
Best Prec@1: [75.700]	
Starting epoch number: 277 Learning rate: 0.0010000000000000002
Train: [277]	Time 649.649	Data 0.300	Loss 0.019	Prec@1 99.9180	Prec@5 100.0000	
Val: [277]	Time 40.345	Data 0.118	Loss 1.141	Prec@1 75.4400	Prec@5 93.5500	
Best Prec@1: [75.700]	
Starting epoch number: 278 Learning rate: 0.0010000000000000002
Train: [278]	Time 649.761	Data 0.305	Loss 0.019	Prec@1 99.9220	Prec@5 100.0000	
Val: [278]	Time 40.302	Data 0.112	Loss 1.138	Prec@1 75.5500	Prec@5 93.4600	
Best Prec@1: [75.700]	
Starting epoch number: 279 Learning rate: 0.0010000000000000002
Train: [279]	Time 649.721	Data 0.286	Loss 0.019	Prec@1 99.8740	Prec@5 100.0000	
Val: [279]	Time 40.364	Data 0.116	Loss 1.145	Prec@1 75.2200	Prec@5 93.5300	
Best Prec@1: [75.700]	
Starting epoch number: 280 Learning rate: 0.0010000000000000002
Train: [280]	Time 649.671	Data 0.293	Loss 0.019	Prec@1 99.9100	Prec@5 100.0000	
Val: [280]	Time 40.348	Data 0.110	Loss 1.143	Prec@1 75.3800	Prec@5 93.4700	
Best Prec@1: [75.700]	
Starting epoch number: 281 Learning rate: 0.0010000000000000002
Train: [281]	Time 649.528	Data 0.302	Loss 0.019	Prec@1 99.9240	Prec@5 100.0000	
Val: [281]	Time 40.257	Data 0.113	Loss 1.141	Prec@1 75.5500	Prec@5 93.6100	
Best Prec@1: [75.700]	
Starting epoch number: 282 Learning rate: 0.0010000000000000002
Train: [282]	Time 649.298	Data 0.300	Loss 0.018	Prec@1 99.9200	Prec@5 100.0000	
Val: [282]	Time 40.346	Data 0.123	Loss 1.134	Prec@1 75.5000	Prec@5 93.5500	
Best Prec@1: [75.700]	
Starting epoch number: 283 Learning rate: 0.0010000000000000002
Train: [283]	Time 649.431	Data 0.312	Loss 0.019	Prec@1 99.8940	Prec@5 100.0000	
Val: [283]	Time 40.293	Data 0.110	Loss 1.133	Prec@1 75.6100	Prec@5 93.5900	
Best Prec@1: [75.700]	
Starting epoch number: 284 Learning rate: 0.0010000000000000002
Train: [284]	Time 649.687	Data 0.310	Loss 0.018	Prec@1 99.9380	Prec@5 100.0000	
Val: [284]	Time 40.305	Data 0.111	Loss 1.134	Prec@1 75.3200	Prec@5 93.5000	
Best Prec@1: [75.700]	
Starting epoch number: 285 Learning rate: 0.0010000000000000002
Train: [285]	Time 649.580	Data 0.291	Loss 0.018	Prec@1 99.8860	Prec@5 100.0000	
Val: [285]	Time 40.260	Data 0.119	Loss 1.142	Prec@1 75.4900	Prec@5 93.5300	
Best Prec@1: [75.700]	
Starting epoch number: 286 Learning rate: 0.0010000000000000002
Train: [286]	Time 649.733	Data 0.293	Loss 0.019	Prec@1 99.8960	Prec@5 100.0000	
Val: [286]	Time 40.287	Data 0.116	Loss 1.137	Prec@1 75.4100	Prec@5 93.4700	
Best Prec@1: [75.700]	
Starting epoch number: 287 Learning rate: 0.0010000000000000002
Train: [287]	Time 649.772	Data 0.289	Loss 0.019	Prec@1 99.9160	Prec@5 100.0000	
Val: [287]	Time 40.355	Data 0.118	Loss 1.140	Prec@1 75.2600	Prec@5 93.4000	
Best Prec@1: [75.700]	
Starting epoch number: 288 Learning rate: 0.0010000000000000002
Train: [288]	Time 649.930	Data 0.293	Loss 0.019	Prec@1 99.9000	Prec@5 100.0000	
Val: [288]	Time 40.336	Data 0.119	Loss 1.136	Prec@1 75.3400	Prec@5 93.4600	
Best Prec@1: [75.700]	
Starting epoch number: 289 Learning rate: 0.0010000000000000002
Train: [289]	Time 650.141	Data 0.303	Loss 0.019	Prec@1 99.8920	Prec@5 100.0000	
Val: [289]	Time 40.391	Data 0.109	Loss 1.137	Prec@1 75.3600	Prec@5 93.4600	
Best Prec@1: [75.700]	
Starting epoch number: 290 Learning rate: 0.0010000000000000002
Train: [290]	Time 650.384	Data 0.301	Loss 0.019	Prec@1 99.8880	Prec@5 100.0000	
Val: [290]	Time 40.386	Data 0.120	Loss 1.141	Prec@1 75.3200	Prec@5 93.5900	
Best Prec@1: [75.700]	
Starting epoch number: 291 Learning rate: 0.0010000000000000002
Train: [291]	Time 650.473	Data 0.312	Loss 0.019	Prec@1 99.9000	Prec@5 100.0000	
Val: [291]	Time 40.440	Data 0.122	Loss 1.130	Prec@1 75.4000	Prec@5 93.5300	
Best Prec@1: [75.700]	
Starting epoch number: 292 Learning rate: 0.0010000000000000002
Train: [292]	Time 650.508	Data 0.292	Loss 0.019	Prec@1 99.9080	Prec@5 100.0000	
Val: [292]	Time 40.401	Data 0.117	Loss 1.134	Prec@1 75.3200	Prec@5 93.6300	
Best Prec@1: [75.700]	
Starting epoch number: 293 Learning rate: 0.0010000000000000002
Train: [293]	Time 650.477	Data 0.288	Loss 0.018	Prec@1 99.9040	Prec@5 100.0000	
Val: [293]	Time 40.428	Data 0.114	Loss 1.140	Prec@1 75.4100	Prec@5 93.4600	
Best Prec@1: [75.700]	
Starting epoch number: 294 Learning rate: 0.0010000000000000002
Train: [294]	Time 650.663	Data 0.292	Loss 0.018	Prec@1 99.9060	Prec@5 100.0000	
Val: [294]	Time 40.290	Data 0.121	Loss 1.136	Prec@1 75.4200	Prec@5 93.5500	
Best Prec@1: [75.700]	
Starting epoch number: 295 Learning rate: 0.0010000000000000002
Train: [295]	Time 651.013	Data 0.301	Loss 0.019	Prec@1 99.8900	Prec@5 100.0000	
Val: [295]	Time 40.455	Data 0.112	Loss 1.134	Prec@1 75.5200	Prec@5 93.6800	
Best Prec@1: [75.700]	
Starting epoch number: 296 Learning rate: 0.0010000000000000002
Train: [296]	Time 650.739	Data 0.292	Loss 0.018	Prec@1 99.9080	Prec@5 100.0000	
Val: [296]	Time 40.384	Data 0.107	Loss 1.141	Prec@1 75.4800	Prec@5 93.5000	
Best Prec@1: [75.700]	
Starting epoch number: 297 Learning rate: 0.0010000000000000002
Train: [297]	Time 650.887	Data 0.310	Loss 0.019	Prec@1 99.9160	Prec@5 100.0000	
Val: [297]	Time 40.340	Data 0.124	Loss 1.141	Prec@1 75.2700	Prec@5 93.5400	
Best Prec@1: [75.700]	
Starting epoch number: 298 Learning rate: 0.0010000000000000002
Train: [298]	Time 650.877	Data 0.293	Loss 0.018	Prec@1 99.9040	Prec@5 100.0000	
Val: [298]	Time 40.467	Data 0.118	Loss 1.141	Prec@1 75.2600	Prec@5 93.6300	
Best Prec@1: [75.700]	
Starting epoch number: 299 Learning rate: 0.0010000000000000002
Train: [299]	Time 650.804	Data 0.275	Loss 0.019	Prec@1 99.8820	Prec@5 100.0000	
Val: [299]	Time 40.464	Data 0.121	Loss 1.145	Prec@1 75.3800	Prec@5 93.5400	
Best Prec@1: [75.700]	
